{
    "docs": [
        {
            "location": "/",
            "text": "Kubernetes Lab Guide\n\n\nWelcome to the Kubernetes Course  by School of Devops\n\n\nThis is a Lab Guide which goes along with the Docker and Kubernetes courses conducted by School of Devops.\n\n\nFor information about the devops trainign courses visit \nschoolofdevops.net\n.\n\n\nTeam\n\n\n\n\nGourav Shah\n\n\nVijayboopathy\n\n\nVenkat\n\n\nAbhijeet Nirmal",
            "title": "Home"
        },
        {
            "location": "/#kubernetes-lab-guide",
            "text": "Welcome to the Kubernetes Course  by School of Devops  This is a Lab Guide which goes along with the Docker and Kubernetes courses conducted by School of Devops.  For information about the devops trainign courses visit  schoolofdevops.net .",
            "title": "Kubernetes Lab Guide"
        },
        {
            "location": "/#team",
            "text": "Gourav Shah  Vijayboopathy  Venkat  Abhijeet Nirmal",
            "title": "Team"
        },
        {
            "location": "/operating-containers/",
            "text": "Lab: Getting Started with Docker Operations\n\n\nIn this chapter, we are going to learn about docker shell, the command line utility and how to use it to\nlaunch containers. We will also learn what it means to run a container, its lifecycle and perform basic\noperations such as creating, starting, stopping, removing, pausing containers and checking the status etc.\n\n\nUsing docker cli\n\n\nWe can use docker cli to interact with docker daemon. Various functions of docker command is given below. Try this yourself by runnig \n$sudo docker\n command  \n\n\ndocker\n\n\n\n\n[Output]  \n\n\nUsage: docker [OPTIONS] COMMAND [arg...]\n       docker [ --help | -v | --version ]\n\nA self-sufficient runtime for containers.\n\nOptions:\n\n  --config=~/.docker              Location of client config files\n  -D, --debug                     Enable debug mode\n  -H, --host=[]                   Daemon socket(s) to connect to\n  -h, --help                      Print usage\n  -l, --log-level=info            Set the logging level\n  --tls                           Use TLS; implied by --tlsverify\n  --tlscacert=~/.docker/ca.pem    Trust certs signed only by this CA\n  --tlscert=~/.docker/cert.pem    Path to TLS certificate file\n  --tlskey=~/.docker/key.pem      Path to TLS key file\n  --tlsverify                     Use TLS and verify the remote\n  -v, --version                   Print version information and quit\n\nCommands:\n    attach    Attach to a running container\n    build     Build an image from a Dockerfile\n    commit    Create a new image from a container's changes\n    cp        Copy files/folders between a container and the local filesystem\n    create    Create a new container\n    diff      Inspect changes on a container's filesystem\n    events    Get real time events from the server\n    exec      Run a command in a running container\n    export    Export a container's filesystem as a tar archive\n    history   Show the history of an image\n    images    List images\n    import    Import the contents from a tarball to create a filesystem image\n    info      Display system-wide information\n    inspect   Return low-level information on a container, image or task\n    kill      Kill one or more running containers\n    load      Load an image from a tar archive or STDIN\n    login     Log in to a Docker registry.\n    logout    Log out from a Docker registry.\n    logs      Fetch the logs of a container\n    network   Manage Docker networks\n    node      Manage Docker Swarm nodes\n    pause     Pause all processes within one or more containers\n    port      List port mappings or a specific mapping for the container\n    ps        List containers\n    pull      Pull an image or a repository from a registry\n    push      Push an image or a repository to a registry\n    rename    Rename a container\n    restart   Restart a container\n    rm        Remove one or more containers\n    rmi       Remove one or more images\n    run       Run a command in a new container\n    save      Save one or more images to a tar archive (streamed to STDOUT by default)\n    search    Search the Docker Hub for images\n    service   Manage Docker services\n    start     Start one or more stopped containers\n    stats     Display a live stream of container(s) resource usage statistics\n    stop      Stop one or more running containers\n    swarm     Manage Docker Swarm\n    tag       Tag an image into a repository\n    top       Display the running processes of a container\n    unpause   Unpause all processes within one or more containers\n    update    Update configuration of one or more containers\n    version   Show the Docker version information\n    volume    Manage Docker volumes\n    wait      Block until a container stops, then print its exit code\n\n\n\n\n\nGetting Information about Docker Setup\n\n\nWe can get the information about our Docker setup in several ways. Namely,  \n\n\ndocker version\n\ndocker -v\n\ndocker system info\n\n\n\n\nThe \ndocker system info\n command gives a lot of useful information like total number of containers and images along with information about host resource utilization  etc.\n\n\nStream events from the docker daemon\n\n\nDocker \nevents\n serves us with the stream of events or interactions that are happening with the docker daemon. This does not stream the log data of application inside the container. That is done by \ndocker logs\n command. Let us see how this command works\n\nOpen an another terminal. Let us call the old terminal as \nTerminal 1\n and the newer one as \nTerminal 2\n.\n\n\nFrom Terminal 1, execute \ndocker events\n. Now you are getting the data stream from docker daemon  \n\n\ndocker system events\n\n\n\n\nLaunching our first container\n\n\nNow we have a basic understanding of docker command and sub commands, let us dive straight into launching our very first \ncontainer\n  \n\n\ndocker run alpine:3.4 uptime\n\n\n\n\nWhere,  \n\n\n\n\nwe are using docker \nclient\n to  \n\n\nrun a application/command \nuptime\n using    \n\n\nan image by name \nalpine:3.4\n  \n\n\n\n\n[Output]  \n\n\nUnable to find image 'alpine:3.4' locally\n3.4: Pulling from library/alpine\n81033e7c1d6a: Pull complete\nDigest: sha256:2532609239f3a96fbc30670716d87c8861b8a1974564211325633ca093b11c0b\nStatus: Downloaded newer image for alpine:3.4\n\n 15:24:34 up  7:36,  load average: 0.00, 0.03, 0.04\n\n\n\n\nWhat happened?\n  \n\n\nThis command will  \n\n\n\n\nPull the \nalpine\n image file from \ndocker hub\n, a cloud registry\n\n\nCreate a runtime environment/ container with the above image   \n\n\nLaunch a program (called \nuptime\n) inside that container  \n\n\nStream that output to the terminal  \n\n\nStop the container once the program is exited\n\n\n\n\nWhere did my container go?\n  \n\n\ndocker container  ps\n\ndocker container  ps -l\n\n\n\n\nThe point here to remember is that, when that executable stops running inside the container, the container itself will stop.\n\n\nLet's see what happens when we run that command again,  \n\n\n[Output]  \n\n\ndocker run alpine uptime\n 07:48:06 up  3:15,  load average: 0.00, 0.00, 0.00\n\n\n\n\nNow docker no longer pulls the image again from registry, because \nit has stored the image locally\n from the previous run. So once an image is pulled, we can make use of that image to create and run as many container as we want without the need of downloading the image again. However it has created a new instance of the iamge/container.\n\n\nChecking Status of the containers\n\n\nWe have understood how docker run commands works. But what if you want to see list of running containers and history of containers that had run and exited? This can be done by executing the following commands  \n\n\ndocker ps\n\n\n\n\n[Output]  \n\n\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n\n\n\n\nThis command doesn't give us any information. Because, \ndocker ps\n command will only show list of container(s) which are \nrunning\n  \n\n\ndocker ps -l\n\n\n\n\n[Output]  \n\n\nCONTAINER ID        IMAGE               COMMAND             CREATED              STATUS                          PORTS               NAMES\n988f4d90d604        alpine              \"uptime\"            About a minute ago   Exited (0) About a minute ago                       fervent_hypatia\n\n\n\n\nthe \n-l\n flag shows the last run container along with other details like image it used, command it executed, return code of that command, etc.,  \n\n\ndocker ps -n 2\n\n\n\n\n[Output]  \n\n\nNAMES\n988f4d90d604        alpine              \"uptime\"            About a minute ago   Exited (0) About a minute ago                       fervent_hypatia\nacea3023dca4        alpine              \"uptime\"            3 minutes ago        Exited (0) 3 minutes ago                            mad_darwin\n\n\n\n\nDocker gives us the flexibility to show the desirable number of last run containers. This can be achieved by using \n-n #no_of_results\n flag  \n\n\ndocker ps -a\n\n\n\n\n[Output]  \n\n\nCONTAINER ID        IMAGE                        COMMAND                  CREATED              STATUS                          PORTS                  NAMES\n988f4d90d604        alpine                       \"uptime\"                 About a minute ago   Exited (0) About a minute ago                          fervent_hypatia\nacea3023dca4        alpine                       \"uptime\"                 4 minutes ago        Exited (0) 4 minutes ago                               mad_darwin\n60ffa94e69ec        ubuntu:14.04.3               \"bash\"                   27 hours ago         Exited (0) 26 hours ago                                infallible_meninsky\ndd75c04e7d2b        schoolofdevops/ghost:0.3.1   \"/entrypoint.sh npm s\"   4 days ago           Exited (0) 3 days ago                                  kickass_bardeen\nc082972f66d6        schoolofdevops/ghost:0.3.1   \"/entrypoint.sh npm s\"   4 days ago           Exited (0) 3 days ago           0.0.0.0:80->2368/tcp   sodcblog\n\n\n\n\n\nThis command will show all the container we have run so far.  \n\n\nRunning Containers in Interactive Mode\n\n\nWe can interact with docker containers by giving -it flags at the run time. These flags stand for\n\n  * i - Interactive\n\n  * t - tty\n\n\ndocker run -it alpine:3.4 sh\n\n\n\n\n[Output]  \n\n\nUnable to find image 'alpine:3.4' locally\nlatest: Pulling from library/alpine\nff3a5c916c92: Already exists\nDigest: sha256:7df6db5aa61ae9480f52f0b3a06a140ab98d427f86d8d5de0bedab9b8df6b1c0\nStatus: Downloaded newer image for alpine:latest\n/ #\n\n\n\n\nAs you see, we have landed straight into \nsh\n shell of that container. This is the result of using \n-it\n flags and mentioning that container to run the \nsh\n shell. Don't try to exit that container yet. We have to execute some other commands in it to understand the next topic  \n\n\nNamespaced:\n\n\nLike a full fledged OS, Docker container has its own namespaces\n\nThis enables Docker container to isolate itself from the host as well as other containers  \n\n\nRun the following commands and see that alpine container has its own namespaces and not inheriting much from \nhost OS\n  \n\n\nNAMESPACED  \n\n\n[Run the following commands inside the container]  \n\n\ncat /etc/issue\n\nps aux\n\nifconfig\n\nhostname\n\n\n\n\nShared(NON NAMESPACED):  \n\n\nWe have understood that containers have their own namespaces. But will they share something to some extent? the answer is \nYES\n. Let's run the following commands on both the container and the host machine  \n\n\n[Run the following commands inside the container]  \n\n\nuptime\n\nuname -a\n\ncat /proc/cpuinfo\n\ndate\n\nfree\n\n\n\n\nAs you can see, the container uses the same Linux Kernel from the host machine. Just like \nuname\n command, the following commands share the same information as well. In order to avoid repetition, we will see the output of container alone.\n\n\nNow exit out of that container by running \nexit\n or by pressing \nctrl+d\n  \n\n\nMaking Containers Persist\n\n\nRunning Containers in Detached Mode\n\n\nSo far, we have run the containers interactively. But this is not always the case. Sometimes you may want to start a container  without interacting with it. This can be achieved by using \n\"detached mode\"\n (\n-d\n) flag. Hence the container will launch the deafault application inside and run in the background. This saves a lot of time, we don't have to wait till the applications launches successfully. It will happen behind the screen. Let us run the following command to see this in action  \n\n\n[Command]  \n\n\ndocker run -idt schoolofdevops/loop program\n\n\n\n\n-d , --detach : detached mode  \n\n\n[Output]  \n\n\n2533adf280ac4838b446e4ee1151f52793e6ac499d2e631b2c752459bb18ad5f\n\n\n\n\nThis will run the container in detached mode. We are only given with full container id as the output  \n\n\nLet us check whether this container is running or not\n\n[Command]  \n\n\ndocker ps\n\n\n\n\n[Output]  \n\n\nCONTAINER ID        IMAGE                 COMMAND             CREATED             STATUS              PORTS               NAMES\n2533adf280ac        schoolofdevops/loop   \"program\"           37 seconds ago      Up 36 seconds                           prickly_bose\n\n\n\n\nAs we can see in the output, the container is running in the background  \n\n\nChecking Logs\n\n\nTo check the logs, find out the container id/name and run the following commands, replacing 08f0242aa61c with your container id\n\n\n[Commands]  \n\n\ndocker container ps\n\ndocker container logs <container_id/name>\n\ndocker container logs -f  <container_id/name>\n\n\n\n\nConnecting to running container to execute commands\n\n\nWe can connect to the containers which are running in detached mode by using these following commands\n\n[Command]  \n\n\ndocker exec -it <container_id/name> sh\n\n\n\n\n[Output]  \n\n\n/ #\n\n\n\n\nYou could try running any commands on the shell\ne.g.\n\n\napk update\napk add vim\nps aux\n\n\n\n\nNow exit the container.  \n\n\nLaunching a container with a pre built app image\n\n\nTo launch vote container run the following command. Don't bother about the new flag \n-P\n now. We will explain about that flag later in this chapter  \n\n\ndocker container run  -idt -P  schoolofdevops/vote\n\n\n\n\n[Output]  \n\n\nUnable to find image 'schoolofdevops/vote:latest' locally\nlatest: Pulling from schoolofdevops/vote\nDigest: sha256:9195942ea654fa8d8aeb37900be5619215c08e7e1bef0b7dfe4c04a9cc20a8c2\nStatus: Downloaded newer image for schoolofdevops/vote:latest\n7d58ecc05754b5fd192c4ecceae334ac22565684c6923ea332bff5c88e3fca2b\n\n\n\n\nLets check the status of the container  \n\n\ndocker ps -l\n\n\n\n\n[Output]  \n\n\nCONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS              PORTS                   NAMES\n7d58ecc05754        schoolofdevops/vote   \"gunicorn app:app -b\u2026\"   27 seconds ago      Up 26 seconds       0.0.0.0:32768->80/tcp   peaceful_varahamihira\n\n\n\n\nRenaming the container\n\n\nWe can rename the container by using following command  \n\n\ndocker rename 7d58ecc05754 vote\n\n\n\n\n[replace 7d58ecc05754 with the actual container id on your system ]\n\n\nWe have changed container's automatically generated name to \nvote\n. This new name can be of your choice. The point to understand is this command takes two arguments. The \nOld_name followed by New_name\n\nRun docker ps command to check the effect of changes  \n\n\ndocker ps\n\n\n\n\n[Output]  \n\n\nCONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS              PORTS                   NAMES\n7d58ecc05754        schoolofdevops/vote   \"gunicorn app:app -b\u2026\"   3 minutes ago       Up 3 minutes        0.0.0.0:32768->80/tcp   vote\n\n\n\n\nAs you can see here, the container is renamed to \nvote\n. This makes referencing container in cli very much easier.  \n\n\nReady to  vote ?\n\n\nLet's see what this \nvote\n application does by connecting to that application. For that we need,  \n\n\n\n\nHost machine's IP  \n\n\nContainer's port which is mapped to a host's port\n\n\n\n\nLet's find out the port mapping of container to host. Docker provides subcommand called \nport\n which does this job  \n\n\ndocker port vote  \n\n\n\n\n[Output]  \n\n\n80/tcp -> 0.0.0.0:32768\n\n\n\n\nSo whatever traffic the host gets in port \n2368\n will be mapped to container's port \n32768\n  \n\n\nLet's connect to http://IP_ADDRESS:PORT to see the actual application  \n\n\nFinding Everything about the running  container\n\n\nThis topic discusses about finding metadata of containers. These metadata include various parameters like,\n\n  * State of the container\n\n  * Mounts\n\n  * Configuration\n\n  * Network, etc.,  \n\n\nInspecting\n\n\nLets try this inspect subcommand in action  \n\n\ndocker inspect vote\n\n\n\n\nData output by above command contains detailed descriptino of the container an its properties. is represented in JSON format which makes filtering these results easier.  \n\n\nCopying files between container and client host\n\n\nWe can copy files/directories form host to container and vice-versa  \n\nLet us create a file on the host  \n\n\ntouch testfile\n\n\n\n\nTo copy the testfile \nfrom host machine to ghsot contanier\n, try  \n\n\ndocker cp testfile vote:/opt  \n\n\n\n\nThis command will copy testfile to vote container's \n/opt\n directory  and will not give any output. To verify the file has been copies or not, let us log into container by running,  \n\n\ndocker exec -it vote bash\n\n\n\n\nChange directory into /opt and list the files  \n\n\ncd /opt  \nls\n\n\n\n\n[Output]  \n\n\ntestfile\n\n\n\n\nThere you can see that file has been successfully copied. Now exit the container  \n\n\nNow you may try to cp some files \nfrom the container to the host machine\n  \n\n\ndocker cp vote:/app  .  \nls  \n\n\n\n\nChecking the Stats\n\n\nDocker \nstats\n command returns a data stream of resource utilization used by containers. The flag \n--no-stream\n disables data stream and displays only first result  \n\n\ndocker stats --no-stream=true vote\n\ndocker stats\n\n\n\n\nControlling Resources\n\n\nDocker provides us the granularity to control each container's \nresource utilization\n. We have several commands in the inventory to achieve this  \n\n\nPutting limits on Running Containers\n\n\nFirst, let us monitor the utilization\n\n\ndocker stats\n\n\n\n\nYou can see that \nMemory\n attribute has \nmax\n as its value, which  means unlimited usage of host's RAM. We can put a cap on that by using \nupdate\n command  \n\n\ndocker update --memory 400M --memory-swap -1 vote   \n\n\n\n\n[Output]  \n\n\nvote\n\n\n\n\nLet us check whether the change has taken effect or not with docker stats terminal   \n\n\ndocker stat\n\n\n\n\nAs you can see, the memory utilization of the container is changed from xxxx (unlimited) to 400 mb  \n\n\nLimiting Resources while launching new containers\n\n\nThe following resources can be limited using the \nupdate\n command\n\n  * CPU\n  * Memory\n  * Disk IO\n  * Capabilities  \n\n\nOpen two terminals, lets call them T1, and T2\n\nIn T1, start monitoring the stats  \n\n\ndocker stats\n\n\n\n\n[Output]  \n\n\nCONTAINER           CPU %               MEM USAGE / LIMIT     MM %               NET I/O             BLOCK I/O             PIDS\nb28efeef41f8        0.16%               190.1 MiB / 400 MiB   47.51%              1.296 kB / 648 B    86.02 kB / 45.06 kB   0\nCONTAINER           CPU %               MEM USAGE / LIMIT     MEM %               NET I/O             BLOCK I/O             PIDS\nb28efeef41f8        0.01%               190.1 MiB / 400 MiB   47.51%              1.296 kB / 648 B    86.02 kB / 45.06 kB   0\n\n\n\n\nFrom T2, launch containers  with different CPU shares as well as \ncpus\n configurations. Default CPU shares are set to 1024. This is a relative weight. Observe docker stats command after every launch to see the effect of your configurations.\n\n\n[CPU Shares]\n\ndocker run -d --cpu-shares 1024 schoolofdevops/stresstest stress --cpu 2\n\ndocker run -d --cpu-shares 1024 schoolofdevops/stresstest stress --cpu 2\n\ndocker run -d --cpu-shares 512 schoolofdevops/stresstest stress --cpu 2\n\ndocker run -d --cpu-shares 512 schoolofdevops/stresstest stress --cpu 2\n\ndocker run -d --cpu-shares 4096 schoolofdevops/stresstest stress --cpu 2\n\n\n[CPUs]\n\ndocker run -d --cpus 0.2 schoolofdevops/stresstest stress --cpu 2\n\n\n\n\n\n\nClose the T2 terminal once you are done with this lab.  \n\n\nChecking disk utilisation by\n\n\n\ndocker system df\n\n\n\n\n\n[output]\n\n\ndocker system df\nTYPE                TOTAL               ACTIVE              SIZE                RECLAIMABLE\nImages              7                   5                   1.031GB             914.5MB (88%)\nContainers          8                   4                   27.97MB             27.73MB (99%)\nLocal Volumes       3                   2                   0B                  0B\nBuild Cache                                                 0B                  0B\n\n\n\n\nTo prune, you could possibly use\n\n\ndocker container prune\n\ndocker system prune\n\n\n\n\ne.g.\n\n\ndocker system prune\nWARNING! This will remove:\n        - all stopped containers\n        - all networks not used by at least one container\n        - all dangling images\n        - all build cache\nAre you sure you want to continue? [y/N]\nN\n\n\n\n\nMake sure you understand what all will be removed before using this command.\n\n\nStopping and Removing Containers\n\n\nWe have learnt about interacting with a container, running a container, pausing and unpausing a container, creating and starting a container. But what if you want to stop the container or remove the container itself  \n\n\nStop a container\n\n\nA container can be stopped using \nstop\n command. This command will stop the application inside that container hence the container itself will be stopped. This command basically sends a \nSIGTERM\n signal to the container (graceful shutdown)  \n\n\n[Command]  \n\n\ndocker stop <container_id/name>\n\n\n\n\nKill a container\n\n\nThis command will send \nSIGKILL\n signal and kills the container ungracefully  \n\n\n[Command]  \n\n\ndocker kill <container_id/name>\n\n\n\n\nIf you want to remove a container, then execute the following command. Before running this command, run docker ps -a to see the list of pre run containers. Choose a container of your wish and then execute docker rm command. Then run docker ps -a again to check the removed container list or not  \n\n\n[Command]  \n\n\ndocker rm <container_id/name>\ndocker rm -f <container_id/name>\n\n\n\n\n\nExercises\n\n\nLaunching Containers with Elevated  Privileges\n\n\nWhen the operator executes docker run --privileged, Docker will enable to access to all devices on the host as well as set some configuration in AppArmor or SELinux to allow the container nearly all the same access to the host as processes running outside containers on the host.",
            "title": "Lab D101 - Operating Containers"
        },
        {
            "location": "/operating-containers/#lab-getting-started-with-docker-operations",
            "text": "In this chapter, we are going to learn about docker shell, the command line utility and how to use it to\nlaunch containers. We will also learn what it means to run a container, its lifecycle and perform basic\noperations such as creating, starting, stopping, removing, pausing containers and checking the status etc.  Using docker cli  We can use docker cli to interact with docker daemon. Various functions of docker command is given below. Try this yourself by runnig  $sudo docker  command    docker  [Output]    Usage: docker [OPTIONS] COMMAND [arg...]\n       docker [ --help | -v | --version ]\n\nA self-sufficient runtime for containers.\n\nOptions:\n\n  --config=~/.docker              Location of client config files\n  -D, --debug                     Enable debug mode\n  -H, --host=[]                   Daemon socket(s) to connect to\n  -h, --help                      Print usage\n  -l, --log-level=info            Set the logging level\n  --tls                           Use TLS; implied by --tlsverify\n  --tlscacert=~/.docker/ca.pem    Trust certs signed only by this CA\n  --tlscert=~/.docker/cert.pem    Path to TLS certificate file\n  --tlskey=~/.docker/key.pem      Path to TLS key file\n  --tlsverify                     Use TLS and verify the remote\n  -v, --version                   Print version information and quit\n\nCommands:\n    attach    Attach to a running container\n    build     Build an image from a Dockerfile\n    commit    Create a new image from a container's changes\n    cp        Copy files/folders between a container and the local filesystem\n    create    Create a new container\n    diff      Inspect changes on a container's filesystem\n    events    Get real time events from the server\n    exec      Run a command in a running container\n    export    Export a container's filesystem as a tar archive\n    history   Show the history of an image\n    images    List images\n    import    Import the contents from a tarball to create a filesystem image\n    info      Display system-wide information\n    inspect   Return low-level information on a container, image or task\n    kill      Kill one or more running containers\n    load      Load an image from a tar archive or STDIN\n    login     Log in to a Docker registry.\n    logout    Log out from a Docker registry.\n    logs      Fetch the logs of a container\n    network   Manage Docker networks\n    node      Manage Docker Swarm nodes\n    pause     Pause all processes within one or more containers\n    port      List port mappings or a specific mapping for the container\n    ps        List containers\n    pull      Pull an image or a repository from a registry\n    push      Push an image or a repository to a registry\n    rename    Rename a container\n    restart   Restart a container\n    rm        Remove one or more containers\n    rmi       Remove one or more images\n    run       Run a command in a new container\n    save      Save one or more images to a tar archive (streamed to STDOUT by default)\n    search    Search the Docker Hub for images\n    service   Manage Docker services\n    start     Start one or more stopped containers\n    stats     Display a live stream of container(s) resource usage statistics\n    stop      Stop one or more running containers\n    swarm     Manage Docker Swarm\n    tag       Tag an image into a repository\n    top       Display the running processes of a container\n    unpause   Unpause all processes within one or more containers\n    update    Update configuration of one or more containers\n    version   Show the Docker version information\n    volume    Manage Docker volumes\n    wait      Block until a container stops, then print its exit code  Getting Information about Docker Setup  We can get the information about our Docker setup in several ways. Namely,    docker version\n\ndocker -v\n\ndocker system info  The  docker system info  command gives a lot of useful information like total number of containers and images along with information about host resource utilization  etc.  Stream events from the docker daemon  Docker  events  serves us with the stream of events or interactions that are happening with the docker daemon. This does not stream the log data of application inside the container. That is done by  docker logs  command. Let us see how this command works \nOpen an another terminal. Let us call the old terminal as  Terminal 1  and the newer one as  Terminal 2 .  From Terminal 1, execute  docker events . Now you are getting the data stream from docker daemon    docker system events  Launching our first container  Now we have a basic understanding of docker command and sub commands, let us dive straight into launching our very first  container     docker run alpine:3.4 uptime  Where,     we are using docker  client  to    run a application/command  uptime  using      an image by name  alpine:3.4      [Output]    Unable to find image 'alpine:3.4' locally\n3.4: Pulling from library/alpine\n81033e7c1d6a: Pull complete\nDigest: sha256:2532609239f3a96fbc30670716d87c8861b8a1974564211325633ca093b11c0b\nStatus: Downloaded newer image for alpine:3.4\n\n 15:24:34 up  7:36,  load average: 0.00, 0.03, 0.04  What happened?     This command will     Pull the  alpine  image file from  docker hub , a cloud registry  Create a runtime environment/ container with the above image     Launch a program (called  uptime ) inside that container    Stream that output to the terminal    Stop the container once the program is exited   Where did my container go?     docker container  ps\n\ndocker container  ps -l  The point here to remember is that, when that executable stops running inside the container, the container itself will stop.  Let's see what happens when we run that command again,    [Output]    docker run alpine uptime\n 07:48:06 up  3:15,  load average: 0.00, 0.00, 0.00  Now docker no longer pulls the image again from registry, because  it has stored the image locally  from the previous run. So once an image is pulled, we can make use of that image to create and run as many container as we want without the need of downloading the image again. However it has created a new instance of the iamge/container.  Checking Status of the containers  We have understood how docker run commands works. But what if you want to see list of running containers and history of containers that had run and exited? This can be done by executing the following commands    docker ps  [Output]    CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES  This command doesn't give us any information. Because,  docker ps  command will only show list of container(s) which are  running     docker ps -l  [Output]    CONTAINER ID        IMAGE               COMMAND             CREATED              STATUS                          PORTS               NAMES\n988f4d90d604        alpine              \"uptime\"            About a minute ago   Exited (0) About a minute ago                       fervent_hypatia  the  -l  flag shows the last run container along with other details like image it used, command it executed, return code of that command, etc.,    docker ps -n 2  [Output]    NAMES\n988f4d90d604        alpine              \"uptime\"            About a minute ago   Exited (0) About a minute ago                       fervent_hypatia\nacea3023dca4        alpine              \"uptime\"            3 minutes ago        Exited (0) 3 minutes ago                            mad_darwin  Docker gives us the flexibility to show the desirable number of last run containers. This can be achieved by using  -n #no_of_results  flag    docker ps -a  [Output]    CONTAINER ID        IMAGE                        COMMAND                  CREATED              STATUS                          PORTS                  NAMES\n988f4d90d604        alpine                       \"uptime\"                 About a minute ago   Exited (0) About a minute ago                          fervent_hypatia\nacea3023dca4        alpine                       \"uptime\"                 4 minutes ago        Exited (0) 4 minutes ago                               mad_darwin\n60ffa94e69ec        ubuntu:14.04.3               \"bash\"                   27 hours ago         Exited (0) 26 hours ago                                infallible_meninsky\ndd75c04e7d2b        schoolofdevops/ghost:0.3.1   \"/entrypoint.sh npm s\"   4 days ago           Exited (0) 3 days ago                                  kickass_bardeen\nc082972f66d6        schoolofdevops/ghost:0.3.1   \"/entrypoint.sh npm s\"   4 days ago           Exited (0) 3 days ago           0.0.0.0:80->2368/tcp   sodcblog  This command will show all the container we have run so far.    Running Containers in Interactive Mode  We can interact with docker containers by giving -it flags at the run time. These flags stand for \n  * i - Interactive \n  * t - tty  docker run -it alpine:3.4 sh  [Output]    Unable to find image 'alpine:3.4' locally\nlatest: Pulling from library/alpine\nff3a5c916c92: Already exists\nDigest: sha256:7df6db5aa61ae9480f52f0b3a06a140ab98d427f86d8d5de0bedab9b8df6b1c0\nStatus: Downloaded newer image for alpine:latest\n/ #  As you see, we have landed straight into  sh  shell of that container. This is the result of using  -it  flags and mentioning that container to run the  sh  shell. Don't try to exit that container yet. We have to execute some other commands in it to understand the next topic    Namespaced:  Like a full fledged OS, Docker container has its own namespaces \nThis enables Docker container to isolate itself from the host as well as other containers    Run the following commands and see that alpine container has its own namespaces and not inheriting much from  host OS     NAMESPACED    [Run the following commands inside the container]    cat /etc/issue\n\nps aux\n\nifconfig\n\nhostname  Shared(NON NAMESPACED):    We have understood that containers have their own namespaces. But will they share something to some extent? the answer is  YES . Let's run the following commands on both the container and the host machine    [Run the following commands inside the container]    uptime\n\nuname -a\n\ncat /proc/cpuinfo\n\ndate\n\nfree  As you can see, the container uses the same Linux Kernel from the host machine. Just like  uname  command, the following commands share the same information as well. In order to avoid repetition, we will see the output of container alone.  Now exit out of that container by running  exit  or by pressing  ctrl+d     Making Containers Persist  Running Containers in Detached Mode  So far, we have run the containers interactively. But this is not always the case. Sometimes you may want to start a container  without interacting with it. This can be achieved by using  \"detached mode\"  ( -d ) flag. Hence the container will launch the deafault application inside and run in the background. This saves a lot of time, we don't have to wait till the applications launches successfully. It will happen behind the screen. Let us run the following command to see this in action    [Command]    docker run -idt schoolofdevops/loop program  -d , --detach : detached mode    [Output]    2533adf280ac4838b446e4ee1151f52793e6ac499d2e631b2c752459bb18ad5f  This will run the container in detached mode. We are only given with full container id as the output    Let us check whether this container is running or not \n[Command]    docker ps  [Output]    CONTAINER ID        IMAGE                 COMMAND             CREATED             STATUS              PORTS               NAMES\n2533adf280ac        schoolofdevops/loop   \"program\"           37 seconds ago      Up 36 seconds                           prickly_bose  As we can see in the output, the container is running in the background    Checking Logs  To check the logs, find out the container id/name and run the following commands, replacing 08f0242aa61c with your container id  [Commands]    docker container ps\n\ndocker container logs <container_id/name>\n\ndocker container logs -f  <container_id/name>  Connecting to running container to execute commands  We can connect to the containers which are running in detached mode by using these following commands \n[Command]    docker exec -it <container_id/name> sh  [Output]    / #  You could try running any commands on the shell\ne.g.  apk update\napk add vim\nps aux  Now exit the container.    Launching a container with a pre built app image  To launch vote container run the following command. Don't bother about the new flag  -P  now. We will explain about that flag later in this chapter    docker container run  -idt -P  schoolofdevops/vote  [Output]    Unable to find image 'schoolofdevops/vote:latest' locally\nlatest: Pulling from schoolofdevops/vote\nDigest: sha256:9195942ea654fa8d8aeb37900be5619215c08e7e1bef0b7dfe4c04a9cc20a8c2\nStatus: Downloaded newer image for schoolofdevops/vote:latest\n7d58ecc05754b5fd192c4ecceae334ac22565684c6923ea332bff5c88e3fca2b  Lets check the status of the container    docker ps -l  [Output]    CONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS              PORTS                   NAMES\n7d58ecc05754        schoolofdevops/vote   \"gunicorn app:app -b\u2026\"   27 seconds ago      Up 26 seconds       0.0.0.0:32768->80/tcp   peaceful_varahamihira  Renaming the container  We can rename the container by using following command    docker rename 7d58ecc05754 vote  [replace 7d58ecc05754 with the actual container id on your system ]  We have changed container's automatically generated name to  vote . This new name can be of your choice. The point to understand is this command takes two arguments. The  Old_name followed by New_name \nRun docker ps command to check the effect of changes    docker ps  [Output]    CONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS              PORTS                   NAMES\n7d58ecc05754        schoolofdevops/vote   \"gunicorn app:app -b\u2026\"   3 minutes ago       Up 3 minutes        0.0.0.0:32768->80/tcp   vote  As you can see here, the container is renamed to  vote . This makes referencing container in cli very much easier.    Ready to  vote ?  Let's see what this  vote  application does by connecting to that application. For that we need,     Host machine's IP    Container's port which is mapped to a host's port   Let's find out the port mapping of container to host. Docker provides subcommand called  port  which does this job    docker port vote    [Output]    80/tcp -> 0.0.0.0:32768  So whatever traffic the host gets in port  2368  will be mapped to container's port  32768     Let's connect to http://IP_ADDRESS:PORT to see the actual application    Finding Everything about the running  container  This topic discusses about finding metadata of containers. These metadata include various parameters like, \n  * State of the container \n  * Mounts \n  * Configuration \n  * Network, etc.,    Inspecting  Lets try this inspect subcommand in action    docker inspect vote  Data output by above command contains detailed descriptino of the container an its properties. is represented in JSON format which makes filtering these results easier.    Copying files between container and client host  We can copy files/directories form host to container and vice-versa   \nLet us create a file on the host    touch testfile  To copy the testfile  from host machine to ghsot contanier , try    docker cp testfile vote:/opt    This command will copy testfile to vote container's  /opt  directory  and will not give any output. To verify the file has been copies or not, let us log into container by running,    docker exec -it vote bash  Change directory into /opt and list the files    cd /opt  \nls  [Output]    testfile  There you can see that file has been successfully copied. Now exit the container    Now you may try to cp some files  from the container to the host machine     docker cp vote:/app  .  \nls    Checking the Stats  Docker  stats  command returns a data stream of resource utilization used by containers. The flag  --no-stream  disables data stream and displays only first result    docker stats --no-stream=true vote\n\ndocker stats  Controlling Resources  Docker provides us the granularity to control each container's  resource utilization . We have several commands in the inventory to achieve this    Putting limits on Running Containers  First, let us monitor the utilization  docker stats  You can see that  Memory  attribute has  max  as its value, which  means unlimited usage of host's RAM. We can put a cap on that by using  update  command    docker update --memory 400M --memory-swap -1 vote     [Output]    vote  Let us check whether the change has taken effect or not with docker stats terminal     docker stat  As you can see, the memory utilization of the container is changed from xxxx (unlimited) to 400 mb    Limiting Resources while launching new containers  The following resources can be limited using the  update  command \n  * CPU\n  * Memory\n  * Disk IO\n  * Capabilities    Open two terminals, lets call them T1, and T2 \nIn T1, start monitoring the stats    docker stats  [Output]    CONTAINER           CPU %               MEM USAGE / LIMIT     MM %               NET I/O             BLOCK I/O             PIDS\nb28efeef41f8        0.16%               190.1 MiB / 400 MiB   47.51%              1.296 kB / 648 B    86.02 kB / 45.06 kB   0\nCONTAINER           CPU %               MEM USAGE / LIMIT     MEM %               NET I/O             BLOCK I/O             PIDS\nb28efeef41f8        0.01%               190.1 MiB / 400 MiB   47.51%              1.296 kB / 648 B    86.02 kB / 45.06 kB   0  From T2, launch containers  with different CPU shares as well as  cpus  configurations. Default CPU shares are set to 1024. This is a relative weight. Observe docker stats command after every launch to see the effect of your configurations.  [CPU Shares]\n\ndocker run -d --cpu-shares 1024 schoolofdevops/stresstest stress --cpu 2\n\ndocker run -d --cpu-shares 1024 schoolofdevops/stresstest stress --cpu 2\n\ndocker run -d --cpu-shares 512 schoolofdevops/stresstest stress --cpu 2\n\ndocker run -d --cpu-shares 512 schoolofdevops/stresstest stress --cpu 2\n\ndocker run -d --cpu-shares 4096 schoolofdevops/stresstest stress --cpu 2\n\n\n[CPUs]\n\ndocker run -d --cpus 0.2 schoolofdevops/stresstest stress --cpu 2  Close the T2 terminal once you are done with this lab.    Checking disk utilisation by  \ndocker system df  [output]  docker system df\nTYPE                TOTAL               ACTIVE              SIZE                RECLAIMABLE\nImages              7                   5                   1.031GB             914.5MB (88%)\nContainers          8                   4                   27.97MB             27.73MB (99%)\nLocal Volumes       3                   2                   0B                  0B\nBuild Cache                                                 0B                  0B  To prune, you could possibly use  docker container prune\n\ndocker system prune  e.g.  docker system prune\nWARNING! This will remove:\n        - all stopped containers\n        - all networks not used by at least one container\n        - all dangling images\n        - all build cache\nAre you sure you want to continue? [y/N]\nN  Make sure you understand what all will be removed before using this command.  Stopping and Removing Containers  We have learnt about interacting with a container, running a container, pausing and unpausing a container, creating and starting a container. But what if you want to stop the container or remove the container itself    Stop a container  A container can be stopped using  stop  command. This command will stop the application inside that container hence the container itself will be stopped. This command basically sends a  SIGTERM  signal to the container (graceful shutdown)    [Command]    docker stop <container_id/name>  Kill a container  This command will send  SIGKILL  signal and kills the container ungracefully    [Command]    docker kill <container_id/name>  If you want to remove a container, then execute the following command. Before running this command, run docker ps -a to see the list of pre run containers. Choose a container of your wish and then execute docker rm command. Then run docker ps -a again to check the removed container list or not    [Command]    docker rm <container_id/name>\ndocker rm -f <container_id/name>  Exercises  Launching Containers with Elevated  Privileges  When the operator executes docker run --privileged, Docker will enable to access to all devices on the host as well as set some configuration in AppArmor or SELinux to allow the container nearly all the same access to the host as processes running outside containers on the host.",
            "title": "Lab: Getting Started with Docker Operations"
        },
        {
            "location": "/dockerizing-voteapp/",
            "text": "Lab : Build a docker image for Instavote frontend vote app\n\n\nVoteapp\n is a app written in python. Its a simple, web based  application which serves as a frontend for Instavote project. As a devops engineer, you have been tasked with building an image for vote app and publish it to docker hub registry.\n\n\nApproach 1: Building docker image for voteapp manually\n\n\non the host\n\n\ngit clone https://github.com/schoolofdevops/vote\ndocker container run -idt --name build -p 8000:80 python:2.7-alpine sh\ncd vote\ndocker cp . build:/app\ndocker exec -it build sh\n\n\n\n\ninside the container\n\n\ncd /app\npip install -r requirements.txt\ngunicorn app:app -b 0.0.0.0:80\n\n\n\n\nValidate by accessing http://IPADDRESS:8000\n\n\non the host\n\n\ndocker diff build\n\ndocker container commit build <docker_id>/vote:v1\n\ndocker login\n\ndocker image push <docker_id>/vote:v1\n\n\n\n\n\nApproach 2: Building image with Dockerfile\n\n\nChange into vote  directory which containts the source code.  This assumes you have already cloned the repo. If not, clone it from https://github.com/schoolofdevops/vote\n\n\ncd vote\nls\n\napp.py  requirements.txt  static  templates\n\n\n\n\nAdd/create  Dockerfile the the same directory (vote) with the following content,\n\n\nFROM python:2.7-alpine\n\nWORKDIR /app\n\nCOPY . .\n\nRUN pip install -r requirements.txt\n\nEXPOSE 80\n\nCMD  gunicorn app:app -b 0.0.0.0:80\n\n\n\n\nBuild image using,\n\n\n docker build -t <docker_id>/vote:v2 .\n\n\n\n\nwhere,\n  \n : your docker registry user/namespace. Replace this with the actual user\n\n\nvalidate\n\n\ndocker image ls\ndocker image history <docker_id>/vote:v2\ndocker image history <docker_id>/vote:v1\n\ndocker container run -idt -P <docker_id>/vote:v2\ndocker ps\n\n\n\n\nCheck by connecting to your host:port to validate if vote web application shows up.\n\n\nOnce validated, tag and push\n\n\ndocker image tag <docker_id>/vote:v2 <docker_id>/vote:latest\ndocker login\ndocker push <docker_id>/vote",
            "title": "Lab D102 - Building and Publishing Docker Images"
        },
        {
            "location": "/dockerizing-voteapp/#lab-build-a-docker-image-for-instavote-frontend-vote-app",
            "text": "Voteapp  is a app written in python. Its a simple, web based  application which serves as a frontend for Instavote project. As a devops engineer, you have been tasked with building an image for vote app and publish it to docker hub registry.  Approach 1: Building docker image for voteapp manually  on the host  git clone https://github.com/schoolofdevops/vote\ndocker container run -idt --name build -p 8000:80 python:2.7-alpine sh\ncd vote\ndocker cp . build:/app\ndocker exec -it build sh  inside the container  cd /app\npip install -r requirements.txt\ngunicorn app:app -b 0.0.0.0:80  Validate by accessing http://IPADDRESS:8000  on the host  docker diff build\n\ndocker container commit build <docker_id>/vote:v1\n\ndocker login\n\ndocker image push <docker_id>/vote:v1  Approach 2: Building image with Dockerfile  Change into vote  directory which containts the source code.  This assumes you have already cloned the repo. If not, clone it from https://github.com/schoolofdevops/vote  cd vote\nls\n\napp.py  requirements.txt  static  templates  Add/create  Dockerfile the the same directory (vote) with the following content,  FROM python:2.7-alpine\n\nWORKDIR /app\n\nCOPY . .\n\nRUN pip install -r requirements.txt\n\nEXPOSE 80\n\nCMD  gunicorn app:app -b 0.0.0.0:80  Build image using,   docker build -t <docker_id>/vote:v2 .  where,\n    : your docker registry user/namespace. Replace this with the actual user  validate  docker image ls\ndocker image history <docker_id>/vote:v2\ndocker image history <docker_id>/vote:v1\n\ndocker container run -idt -P <docker_id>/vote:v2\ndocker ps  Check by connecting to your host:port to validate if vote web application shows up.  Once validated, tag and push  docker image tag <docker_id>/vote:v2 <docker_id>/vote:latest\ndocker login\ndocker push <docker_id>/vote",
            "title": "Lab : Build a docker image for Instavote frontend vote app"
        },
        {
            "location": "/docker-networking/",
            "text": "Lab: Docker Networking\n\n\nHost Networking\n\n\n\n\nbridge\n\n\nhost\n\n\npeer\n\n\nnone\n\n\n\n\nExamine the existing network\n\n\ndocker network ls\n\nNETWORK ID          NAME                DRIVER              SCOPE\nb3d405dd37e4        bridge              bridge              local\n7527c821537c        host                host                local\n773bea4ca095        none                null                local\n\n\n\n\nCreating new network\n\n\ndocker network create -d bridge mynet\n\n\n\n\nvalidate\n\n\ndocker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nb3d405dd37e4        bridge              bridge              local\n7527c821537c        host                host                local\n4e0d9b1a39f8        mynet               bridge              local\n773bea4ca095        none                null                local\n\n\n\n\ndocker network inspect mynet\n\n\n[\n   {\n       \"Name\": \"mynet\",\n       \"Id\": \"4e0d9b1a39f859af4811986534c91527146bc9d2ce178e5de02473c0f8ce62d5\",\n       \"Created\": \"2018-05-03T04:44:19.187296148Z\",\n       \"Scope\": \"local\",\n       \"Driver\": \"bridge\",\n       \"EnableIPv6\": false,\n       \"IPAM\": {\n           \"Driver\": \"default\",\n           \"Options\": {},\n           \"Config\": [\n               {\n                   \"Subnet\": \"172.18.0.0/16\",\n                   \"Gateway\": \"172.18.0.1\"\n               }\n           ]\n       },\n       \"Internal\": false,\n       \"Attachable\": false,\n       \"Ingress\": false,\n       \"ConfigFrom\": {\n           \"Network\": \"\"\n       },\n       \"ConfigOnly\": false,\n       \"Containers\": {},\n       \"Options\": {},\n       \"Labels\": {}\n   }\n]\n\n\n\n\nLaunching containers in different bridges\n\n\nLaunch two containers \nnt01\n and \nnt02\n in \ndefault\n bridge network\n\n\ndocker container run -idt --name nt01 alpine sh\ndocker container run -idt --name nt02 alpine sh\n\n\n\n\nLaunch two containers \nnt03\n and \nnt04\n in \nmynet\n bridge network\n\n\ndocker container run -idt --name nt03 --net mynet alpine sh\ndocker container run -idt --name nt04 --net mynet alpine sh\n\n\n\n\nNow, lets examine if they can interconnect,\n\n\n\ndocker exec nt01 ifconfig eth0\ndocker exec nt02 ifconfig eth0\ndocker exec nt03 ifconfig eth0\ndocker exec nt04 ifconfig eth0\n\n\n\n\n\nThis is what I see\n\n\nnt01 :  172.17.0.18\n\n\nnt02 :  172.17.0.19\n\n\nnt03 :  172.18.0.2\n\n\nnt04 :  172.18.0.3\n\n\nCreate a table with the ips on your host.  Once you do that,\n\n\nTry to,\n\n\n\n\nping from \nnt01\n to \nnt02\n  \n\n\nping from \nnt01\n to \nnt03\n  \n\n\nping from \nnt03\n to \nnt04\n  \n\n\nping from \nnt03\n to \nnt02\n  \n\n\n\n\ne.g.\n\n\n[replace ip addresses as per your setup]\n\n\ndocker exec nt01  ping 172.17.0.19\n\ndocker exec nt01  ping 172.18.0.2\n\ndocker exec nt03  ping 172.17.0.19\n\ndocker exec nt03  ping 172.18.0.2\n\n\n\n\n\n\nClearly, these two are two differnt subnets/networks even though running on the same host. \nnt01\n and \nnt02\n can connect with each other, whereas \nnt03\n  and \nnt04\n can connect. But connection between containers attached to two different subnets is not possible.\n\n\nUsing None Network Driver\n\n\ndocker container run -idt --name nt05 --net none alpine sh\n\ndocker exec -it nt05 sh\n\nifconfig\n\n\n\n\nUsing Host Network Driver\n\n\ndocker container run -idt --name nt05 --net host  alpine sh\n\ndocker exec -it nt05 sh\n\nifconfig\n\n\n\n\nObserve docker bridge, routing and port mapping\n\n\nExercise: Read about \nnetshoot\n utility here\n\n\nLaunch netshoot and connect to the host network\n\n\n\ndocker run -it --net host --privileged  nicolaka/netshoot\n\n\n\n\n\nExamine port mapping,\n\n\niptables -nvL -t nat\n\n\n\n\nTraverse host port to container ip and port.\n\n\nObserve docker bridge and routing with the following command,\n\n\nbrctl show\n\nip route show",
            "title": "Lab D103 - Docker Networking"
        },
        {
            "location": "/docker-networking/#lab-docker-networking",
            "text": "Host Networking   bridge  host  peer  none   Examine the existing network  docker network ls\n\nNETWORK ID          NAME                DRIVER              SCOPE\nb3d405dd37e4        bridge              bridge              local\n7527c821537c        host                host                local\n773bea4ca095        none                null                local  Creating new network  docker network create -d bridge mynet  validate  docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\nb3d405dd37e4        bridge              bridge              local\n7527c821537c        host                host                local\n4e0d9b1a39f8        mynet               bridge              local\n773bea4ca095        none                null                local  docker network inspect mynet\n\n\n[\n   {\n       \"Name\": \"mynet\",\n       \"Id\": \"4e0d9b1a39f859af4811986534c91527146bc9d2ce178e5de02473c0f8ce62d5\",\n       \"Created\": \"2018-05-03T04:44:19.187296148Z\",\n       \"Scope\": \"local\",\n       \"Driver\": \"bridge\",\n       \"EnableIPv6\": false,\n       \"IPAM\": {\n           \"Driver\": \"default\",\n           \"Options\": {},\n           \"Config\": [\n               {\n                   \"Subnet\": \"172.18.0.0/16\",\n                   \"Gateway\": \"172.18.0.1\"\n               }\n           ]\n       },\n       \"Internal\": false,\n       \"Attachable\": false,\n       \"Ingress\": false,\n       \"ConfigFrom\": {\n           \"Network\": \"\"\n       },\n       \"ConfigOnly\": false,\n       \"Containers\": {},\n       \"Options\": {},\n       \"Labels\": {}\n   }\n]  Launching containers in different bridges  Launch two containers  nt01  and  nt02  in  default  bridge network  docker container run -idt --name nt01 alpine sh\ndocker container run -idt --name nt02 alpine sh  Launch two containers  nt03  and  nt04  in  mynet  bridge network  docker container run -idt --name nt03 --net mynet alpine sh\ndocker container run -idt --name nt04 --net mynet alpine sh  Now, lets examine if they can interconnect,  \ndocker exec nt01 ifconfig eth0\ndocker exec nt02 ifconfig eth0\ndocker exec nt03 ifconfig eth0\ndocker exec nt04 ifconfig eth0  This is what I see  nt01 :  172.17.0.18  nt02 :  172.17.0.19  nt03 :  172.18.0.2  nt04 :  172.18.0.3  Create a table with the ips on your host.  Once you do that,  Try to,   ping from  nt01  to  nt02     ping from  nt01  to  nt03     ping from  nt03  to  nt04     ping from  nt03  to  nt02      e.g.  [replace ip addresses as per your setup]  docker exec nt01  ping 172.17.0.19\n\ndocker exec nt01  ping 172.18.0.2\n\ndocker exec nt03  ping 172.17.0.19\n\ndocker exec nt03  ping 172.18.0.2  Clearly, these two are two differnt subnets/networks even though running on the same host.  nt01  and  nt02  can connect with each other, whereas  nt03   and  nt04  can connect. But connection between containers attached to two different subnets is not possible.  Using None Network Driver  docker container run -idt --name nt05 --net none alpine sh\n\ndocker exec -it nt05 sh\n\nifconfig  Using Host Network Driver  docker container run -idt --name nt05 --net host  alpine sh\n\ndocker exec -it nt05 sh\n\nifconfig  Observe docker bridge, routing and port mapping  Exercise: Read about  netshoot  utility here  Launch netshoot and connect to the host network  \ndocker run -it --net host --privileged  nicolaka/netshoot  Examine port mapping,  iptables -nvL -t nat  Traverse host port to container ip and port.  Observe docker bridge and routing with the following command,  brctl show\n\nip route show",
            "title": "Lab: Docker Networking"
        },
        {
            "location": "/docker-volumes/",
            "text": "Lab: Persistent Volumes with Docker\n\n\nTypes of volumes\n\n\n\n\nautomatic volumes\n\n\nnamed volumes\n\n\nvolume binding\n\n\n\n\nAutomatic Volumes\n\n\ndocker container run  -idt --name vt01 -v /var/lib/mysql  alpine sh\ndocker inspect vt01 | grep -i mounts -A 10\n\n\n\n\nNamed volumes\n\n\ndocker container run  -idt --name vt02 -v db-data:/var/lib/mysql  alpine sh\ndocker inspect vt02 | grep -i mounts -A 10\n\n\n\n\nVolume binding\n\n\nmkdir /root/sysfoo\ndocker container run  -idt --name vt03 -v /root/sysfoo:/var/lib/mysql  alpine sh\ndocker inspect vt03 | grep -i mounts -A 10\n\n\n\n\nSharing files between host and the container\n\n\nls /root/sysfoo/\ntouch /root/sysfoo/file1\ndocker exec -it vt03 sh\nls sysfoo/",
            "title": "Lab D104 - Docker Volumes"
        },
        {
            "location": "/docker-volumes/#lab-persistent-volumes-with-docker",
            "text": "Types of volumes   automatic volumes  named volumes  volume binding   Automatic Volumes  docker container run  -idt --name vt01 -v /var/lib/mysql  alpine sh\ndocker inspect vt01 | grep -i mounts -A 10  Named volumes  docker container run  -idt --name vt02 -v db-data:/var/lib/mysql  alpine sh\ndocker inspect vt02 | grep -i mounts -A 10  Volume binding  mkdir /root/sysfoo\ndocker container run  -idt --name vt03 -v /root/sysfoo:/var/lib/mysql  alpine sh\ndocker inspect vt03 | grep -i mounts -A 10  Sharing files between host and the container  ls /root/sysfoo/\ntouch /root/sysfoo/file1\ndocker exec -it vt03 sh\nls sysfoo/",
            "title": "Lab: Persistent Volumes with Docker"
        },
        {
            "location": "/3_install_kubernetes/",
            "text": "LAB - Setting up Kubernetes Cluster with Kubeadm\n\n\nThis documents describes how to setup kubernetes from scratch on your own nodes, without using a managed service. This setup uses \nkubeadm\n to install and configure kubernetes cluster.\n\n\nCompatibility\n\n\nKubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.\n\n\nThe below steps are applicable for the below mentioned OS\n\n\n\n\n\n\n\n\nOS\n\n\nVersion\n\n\nCodename\n\n\n\n\n\n\n\n\n\n\nUbuntu\n\n\n16.04 / 18.04\n\n\nXenial\n\n\n\n\n\n\n\n\nBase Setup\n\n\nSkip this step if using a pre configured lab environment\n\n\nSkip this step and scroll to Initializing Master if you have setup nodes with vagrant\n\n\nOn all nodes which would be part of this cluster, you need to do the base setup as described in the following steps. To simplify this, you could also   \ndownload and run this script\n\n\nCreate Kubernetes Repository\n\n\nSkip this step if using a pre configured lab environment\n\n\nWe need to create a repository to download Kubernetes.\n\n\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n\n\n\n\ncat <<EOF > /etc/apt/sources.list.d/kubernetes.list\ndeb http://apt.kubernetes.io/ kubernetes-xenial main\nEOF\n\n\n\n\nInstallation of the packages\n\n\nSkip this step if using a pre configured lab environment\n\n\nWe should update the machines before installing so that we can update the repository.\n\n\napt-get update -y\n\n\n\n\nInstalling all the packages with dependencies:\n\n\napt-get install -y docker.io kubelet kubeadm kubectl kubernetes-cni\n\n\n\n\nrm -rf /var/lib/kubelet/*\n\n\n\n\nSetup sysctl configs\n\n\nSkip this step if using a pre configured lab environment\n\n\nIn order for many container networks to work, the following needs to be enabled on each node.\n\n\nsysctl net.bridge.bridge-nf-call-iptables=1\n\n\n\n\nThe above steps has to be followed in all the nodes.\n\n\nBegin from the following step if using a pre configured lab environment\n\n\nInitializing Master\n\n\nThis tutorial assumes \nkube-01\n  as the master and used kubeadm as a tool to install and setup the cluster. This section also assumes that you are using vagrant based setup provided along with this tutorial. If not, please update the IP address of the master accordingly.\n\n\nTo initialize master, run this on kube-01 (1st node)\n\n\nreplace 192.168.56.101 with the actual IP of your node\n\n\nkubeadm init --apiserver-advertise-address 192.168.56.101 --pod-network-cidr=192.168.0.0/16\n\n\n\n\n\nInitialization of the Nodes (Previously Minions)\n\n\nAfter master being initialized, it should display the command which could be used on all worker/nodes to join the k8s cluster.\n\n\ne.g.\n\n\nkubeadm join --token c04797.8db60f6b2c0dd078 192.168.12.10:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0\n\n\n\n\ndont copy above command as is, this is just a sample, use actual\n\n\nCopy and paste it on all node.\n\n\nSetup the admin client - Kubectl\n\n\non Master Node\n\n\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\n\n\n\nValidate\n\n\nkubectl get nodes\n\n\n\n\nYou could also put the above command on a watch to observe the nodes getting ready.\n\n\nwatch kubectl get nodes\n\n\n\n\nConfiguring Networking with Calico Plugin\n\n\nInstalling overlay network is necessary for the pods to communicate with each other across the hosts. It is necessary to do this before you try to deploy any applications to your cluster.\n\n\nThere are various overlay networking drivers available for kubernetes. We are going to use \nCalico\n.\n\n\n\nkubectl apply -f https://docs.projectcalico.org/v3.7/manifests/calico.yaml\n\n\n\n\n\nValidating the Setup\n\n\nYou could validate the status of this cluster, health of pods and whether all the components are up or not by using a few or all of the following commands.\n\n\nTo check if nodes are ready\n\n\nkubectl get nodes\nkubectl get cs\n\n\n\n\n\n[ Expected output ]\n\n\nroot@kube-01:~# kubectl get nodes\nNAME      STATUS    ROLES     AGE       VERSION\nkube-01   Ready     master    9m        v1.8.2\nkube-02   Ready     <none>    4m        v1.8.2\nkube-03   Ready     <none>    4m        v1.8.2\n\n\n\n\nAdditional Status Commands\n\n\nkubectl version\n\nkubectl cluster-info\n\nkubectl get pods -n kube-system\n\nkubectl get events\n\n\n\n\n\nIt will take a few minutes to have the cluster up and running with all the services.\n\n\nEnable Kubernetes Dashboard\n\n\nAfter the Pod networks is installled, We can install another add-on service which is Kubernetes Dashboard.\n\n\nInstalling Dashboard:\n\n\nkubectl apply -f https://gist.githubusercontent.com/initcron/32ff89394c881414ea7ef7f4d3a1d499/raw/4863613585d05f9360321c7141cc32b8aa305605/kube-dashboard.yaml\n\n\n\n\n\nThis will create a pod for the Kubernetes Dashboard.\n\n\nDashboard would be setup and available on port 31000. To access it go to the browser, and provide the  following URL\n\n\nuse any of your node's (VM/Server) IP here\n\n\nhttp://NODEIP:31000\n\n\n\n\nThe Dashboard Looks like:\n\n\n\n\nSet up Visualiser\n\n\nFork the repository and deploy the visualizer on kubernetes\n\n\ngit clone  https://github.com/schoolofdevops/kube-ops-view\nkubectl apply -f kube-ops-view/deploy/\n\n\n\n\n\nVisualiser will run on  \n32000\n port. You could access it using a URL such as below and  add /#scale=2.0 or similar option where 2.0 = 200% the scale.\n\n\nreplace <NODE_IP> with actual IP of one of your nodes\n\n\nhttp://<NODE_IP>:32000/#scale=2.0\n\n\n\n\n\n\nKubernetes visualiser is a third party application which provides a operational view of your kubernetes cluster. Its very useful tool for learning kubernetes as it demonstrates the state of the cluster as well as state of the pods as you make changes. You could read further about it \nat this link\n.  \n\n\nDownload the supporting code\n\n\nBefore we proceed further, please checkout the code from the following git repo. This would offer the supporting code for the exercises that follow.\n\n\nrun this on the host where you have configured kubectl\n\n\ngit clone https://github.com/initcron/k8s-code.git",
            "title": "Lab K101 - Setup Kubernetes Cluster"
        },
        {
            "location": "/3_install_kubernetes/#lab-setting-up-kubernetes-cluster-with-kubeadm",
            "text": "This documents describes how to setup kubernetes from scratch on your own nodes, without using a managed service. This setup uses  kubeadm  to install and configure kubernetes cluster.",
            "title": "LAB - Setting up Kubernetes Cluster with Kubeadm"
        },
        {
            "location": "/3_install_kubernetes/#compatibility",
            "text": "Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.  The below steps are applicable for the below mentioned OS     OS  Version  Codename      Ubuntu  16.04 / 18.04  Xenial",
            "title": "Compatibility"
        },
        {
            "location": "/3_install_kubernetes/#base-setup",
            "text": "Skip this step if using a pre configured lab environment  Skip this step and scroll to Initializing Master if you have setup nodes with vagrant  On all nodes which would be part of this cluster, you need to do the base setup as described in the following steps. To simplify this, you could also    download and run this script  Create Kubernetes Repository  Skip this step if using a pre configured lab environment  We need to create a repository to download Kubernetes.  curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -  cat <<EOF > /etc/apt/sources.list.d/kubernetes.list\ndeb http://apt.kubernetes.io/ kubernetes-xenial main\nEOF  Installation of the packages  Skip this step if using a pre configured lab environment  We should update the machines before installing so that we can update the repository.  apt-get update -y  Installing all the packages with dependencies:  apt-get install -y docker.io kubelet kubeadm kubectl kubernetes-cni  rm -rf /var/lib/kubelet/*  Setup sysctl configs  Skip this step if using a pre configured lab environment  In order for many container networks to work, the following needs to be enabled on each node.  sysctl net.bridge.bridge-nf-call-iptables=1  The above steps has to be followed in all the nodes.  Begin from the following step if using a pre configured lab environment",
            "title": "Base Setup"
        },
        {
            "location": "/3_install_kubernetes/#initializing-master",
            "text": "This tutorial assumes  kube-01   as the master and used kubeadm as a tool to install and setup the cluster. This section also assumes that you are using vagrant based setup provided along with this tutorial. If not, please update the IP address of the master accordingly.  To initialize master, run this on kube-01 (1st node)  replace 192.168.56.101 with the actual IP of your node  kubeadm init --apiserver-advertise-address 192.168.56.101 --pod-network-cidr=192.168.0.0/16  Initialization of the Nodes (Previously Minions)  After master being initialized, it should display the command which could be used on all worker/nodes to join the k8s cluster.  e.g.  kubeadm join --token c04797.8db60f6b2c0dd078 192.168.12.10:6443 --discovery-token-ca-cert-hash sha256:88ebb5d5f7fdfcbbc3cde98690b1dea9d0f96de4a7e6bf69198172debca74cd0  dont copy above command as is, this is just a sample, use actual  Copy and paste it on all node.  Setup the admin client - Kubectl  on Master Node  mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config  Validate  kubectl get nodes  You could also put the above command on a watch to observe the nodes getting ready.  watch kubectl get nodes",
            "title": "Initializing Master"
        },
        {
            "location": "/3_install_kubernetes/#configuring-networking-with-calico-plugin",
            "text": "Installing overlay network is necessary for the pods to communicate with each other across the hosts. It is necessary to do this before you try to deploy any applications to your cluster.  There are various overlay networking drivers available for kubernetes. We are going to use  Calico .  \nkubectl apply -f https://docs.projectcalico.org/v3.7/manifests/calico.yaml",
            "title": "Configuring Networking with Calico Plugin"
        },
        {
            "location": "/3_install_kubernetes/#validating-the-setup",
            "text": "You could validate the status of this cluster, health of pods and whether all the components are up or not by using a few or all of the following commands.  To check if nodes are ready  kubectl get nodes\nkubectl get cs  [ Expected output ]  root@kube-01:~# kubectl get nodes\nNAME      STATUS    ROLES     AGE       VERSION\nkube-01   Ready     master    9m        v1.8.2\nkube-02   Ready     <none>    4m        v1.8.2\nkube-03   Ready     <none>    4m        v1.8.2  Additional Status Commands  kubectl version\n\nkubectl cluster-info\n\nkubectl get pods -n kube-system\n\nkubectl get events  It will take a few minutes to have the cluster up and running with all the services.",
            "title": "Validating the Setup"
        },
        {
            "location": "/3_install_kubernetes/#enable-kubernetes-dashboard",
            "text": "After the Pod networks is installled, We can install another add-on service which is Kubernetes Dashboard.  Installing Dashboard:  kubectl apply -f https://gist.githubusercontent.com/initcron/32ff89394c881414ea7ef7f4d3a1d499/raw/4863613585d05f9360321c7141cc32b8aa305605/kube-dashboard.yaml  This will create a pod for the Kubernetes Dashboard.  Dashboard would be setup and available on port 31000. To access it go to the browser, and provide the  following URL  use any of your node's (VM/Server) IP here  http://NODEIP:31000  The Dashboard Looks like:",
            "title": "Enable Kubernetes Dashboard"
        },
        {
            "location": "/3_install_kubernetes/#set-up-visualiser",
            "text": "Fork the repository and deploy the visualizer on kubernetes  git clone  https://github.com/schoolofdevops/kube-ops-view\nkubectl apply -f kube-ops-view/deploy/  Visualiser will run on   32000  port. You could access it using a URL such as below and  add /#scale=2.0 or similar option where 2.0 = 200% the scale.  replace <NODE_IP> with actual IP of one of your nodes  http://<NODE_IP>:32000/#scale=2.0   Kubernetes visualiser is a third party application which provides a operational view of your kubernetes cluster. Its very useful tool for learning kubernetes as it demonstrates the state of the cluster as well as state of the pods as you make changes. You could read further about it  at this link .",
            "title": "Set up Visualiser"
        },
        {
            "location": "/3_install_kubernetes/#download-the-supporting-code",
            "text": "Before we proceed further, please checkout the code from the following git repo. This would offer the supporting code for the exercises that follow.  run this on the host where you have configured kubectl  git clone https://github.com/initcron/k8s-code.git",
            "title": "Download the supporting code"
        },
        {
            "location": "/quickdive/",
            "text": "LAB K102: Kubernetes Quick Dive\n\n\nIn this lab you are going to deploy the \ninstavote\n application stack \nas described here\n in a kubernetes environment using \nkubectl\n commands. Later, you would learn how to do the same by writing declarive \nyaml\n syntax.  \n\n\nPurpose of this lab is to quickly get your app up and running and demonstrate kubernetes key features such as scheduling, high availability, scalability, load balancing, service discovery etc.\n\n\nDeploying app with kubernetes\n\n\nLaunch vote application with kubernetes. (simiar to docker run command)\n\n\nkubectl  run vote --image=schoolofdevops/vote:v1\n\n\n\n\nSince the above command is now deprecated, you could also use a alternate command such as\n\n\nFollowing is an alternate. run it only if you have not used kubectl run above\n\n\nkubectl create deployment vote --image=schoolofdevops/vote:v1\n\n\n\n\nYou could now validate that the instance of vote app is running by using the following commands,\n\n\nkubectl get pods\n\nkubectl get deployments\n\n\n\n\nScalability\n\n\nScale the vote app to run 4 instances.\n\n\nkubectl scale deployment vote --replicas=4\nkubectl get deployments,pods\n\n\n\n\nHigh Availability\n\n\nkubectl get pods\n\n\n\n\nThe above command will list pods. Try to delete a few pods and observe how it affects the availability of your application.\n\n\nkubectl delete pods vote-xxxx vote-yyyy\nkubectl get deploy,rs,pods\n\n\n\n\n\nLoad Balancing\n\n\nPublish the application (similar to using -P for port mapping)\n\n\n\n\nkubectl expose deployment vote --type=NodePort --port 80\n\nkubectl get svc\n\n\n\n\n\nConnect to the app,  refresh the page to see it load balancing.  Also try to vote and observe what happens.  \n\n\nDeploying a new version\n\n\nkubectl scale deployment vote --replicas=12\n\nkubectl set image deployment vote vote=schoolofdevops/vote:v2\n\n\n\n\n\nwatch the rolling update  in action\n\n\nwatch kubectl get deploy,rs,pods\n\n\n\n\nService Discovery\n\n\nPre Test\n: Try to submit a vote from the frontend vote app. Does that work ?\n\n\nNow lets launch rest of the apps.\n\n\nkubectl  create deployment  redis  --image=redis:alpine\n\nkubectl expose deployment redis --port 6379\n\n\n\n\n\noptionally, you could launch rest of the services.\n\n\nkubectl  create deployment  worker --image=schoolofdevops/worker\n\nkubectl  create deployment  db --image=postgres:9.4\n\nkubectl expose deployment db --port 5432\n\nkubectl create deployment  result --image=schoolofdevops/vote-result\n\nkubectl expose deployment result --type=NodePort --port 80\n\n\n\n\n\nPost Tests\n:\n\n\n\n\nTry to submit a vote from the frontend vote app. Does that work ?  If yes, try to deduce how it could connect to the backend applications such as redis.\n\n\nAccess results ui application. When you submit vote, do the results change ?\n\n\n\n\nCleaing up\n\n\nOnce you are done observing, you could delete it with the following commands,\n\n\n\nkubectl delete deploy db redis vote worker result\n\nkubectl delete svc db redis result vote\n\n\n\n\nSummary\n\n\nWhen you deploy an application in kubernetes, you submit it to the api server/ cluster manager. Kubernetes automatically schedules it on a cluster, networks the pods, provides service discovery. In addition as you observed, your application is scalable, high available and is already running behind a  load balancer.",
            "title": "Lab K102 - Kubernetes Quickdive"
        },
        {
            "location": "/quickdive/#lab-k102-kubernetes-quick-dive",
            "text": "In this lab you are going to deploy the  instavote  application stack  as described here  in a kubernetes environment using  kubectl  commands. Later, you would learn how to do the same by writing declarive  yaml  syntax.    Purpose of this lab is to quickly get your app up and running and demonstrate kubernetes key features such as scheduling, high availability, scalability, load balancing, service discovery etc.  Deploying app with kubernetes  Launch vote application with kubernetes. (simiar to docker run command)  kubectl  run vote --image=schoolofdevops/vote:v1  Since the above command is now deprecated, you could also use a alternate command such as  Following is an alternate. run it only if you have not used kubectl run above  kubectl create deployment vote --image=schoolofdevops/vote:v1  You could now validate that the instance of vote app is running by using the following commands,  kubectl get pods\n\nkubectl get deployments  Scalability  Scale the vote app to run 4 instances.  kubectl scale deployment vote --replicas=4\nkubectl get deployments,pods  High Availability  kubectl get pods  The above command will list pods. Try to delete a few pods and observe how it affects the availability of your application.  kubectl delete pods vote-xxxx vote-yyyy\nkubectl get deploy,rs,pods  Load Balancing  Publish the application (similar to using -P for port mapping)  \n\nkubectl expose deployment vote --type=NodePort --port 80\n\nkubectl get svc  Connect to the app,  refresh the page to see it load balancing.  Also try to vote and observe what happens.    Deploying a new version  kubectl scale deployment vote --replicas=12\n\nkubectl set image deployment vote vote=schoolofdevops/vote:v2  watch the rolling update  in action  watch kubectl get deploy,rs,pods  Service Discovery  Pre Test : Try to submit a vote from the frontend vote app. Does that work ?  Now lets launch rest of the apps.  kubectl  create deployment  redis  --image=redis:alpine\n\nkubectl expose deployment redis --port 6379  optionally, you could launch rest of the services.  kubectl  create deployment  worker --image=schoolofdevops/worker\n\nkubectl  create deployment  db --image=postgres:9.4\n\nkubectl expose deployment db --port 5432\n\nkubectl create deployment  result --image=schoolofdevops/vote-result\n\nkubectl expose deployment result --type=NodePort --port 80  Post Tests :   Try to submit a vote from the frontend vote app. Does that work ?  If yes, try to deduce how it could connect to the backend applications such as redis.  Access results ui application. When you submit vote, do the results change ?   Cleaing up  Once you are done observing, you could delete it with the following commands,  \nkubectl delete deploy db redis vote worker result\n\nkubectl delete svc db redis result vote  Summary  When you deploy an application in kubernetes, you submit it to the api server/ cluster manager. Kubernetes automatically schedules it on a cluster, networks the pods, provides service discovery. In addition as you observed, your application is scalable, high available and is already running behind a  load balancer.",
            "title": "LAB K102: Kubernetes Quick Dive"
        },
        {
            "location": "/5-vote-deploying_pods/",
            "text": "Lab K103: Launching Pods with Kubernetes\n\n\nIn this lab, you would learn how to launch applications using  the basic deployment unit of kubernetes i.e. \npods\n. This time, you are going to do it by writing declarative configs with \nyaml\n syntax.\n\n\nResource Configs\n\n\nEach entity created with kubernetes is a resource including pod, service, deployments, replication controller etc. Resources can be defined as YAML or JSON.  Here is the syntax to create a YAML specification.\n\n\nAKMS\n => Resource Configs Specs\n\n\napiVersion: v1\nkind:\nmetadata:\nspec:\n\n\n\n\nUse this \nKubernetes API Reference Document\n while writing the API specs. This is the most important reference  while working with kubernetes, so its highly advisible that you bookmark it for the version of kubernetes that you use.\n\n\nTo find the versino of kubernetes use the following command,\n\n\nkubectl version -o yaml\n\n\n\n\nTo list API objects, use the following commands,\n\n\nkubectl api-resources\n\n\n\n\nWriting Pod Spec\n\n\nLets now create the  Pod config by adding the kind and specs to schma given in the file vote-pod.yaml as follows.\n\n\nFilename: k8s-code/pods/vote-pod.yaml\n\n\napiVersion:\nkind: Pod\nmetadata:\nspec:\n\n\n\n\nFollowing are the specs to launch the vote application,\n\n\n\n\npod:\n\n\nname: vote\n\n\nlabels:\n\n\napp: python\n\n\nrole: vote\n\n\nversion: v1\n\n\n\n\n\n\n\n\n\n\ncontainer\n\n\nname: app\n\n\nimage: schoolofdevops/vote:v1\n\n\n\n\n\n\n\n\nRefer to the \nPod Spec\n find out the relevant properties and add it to the \nvote-pod.yaml\n provided in the supporting code repo.\n\n\nFilename: k8s-code/pods/vote-pod.yaml\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: vote\n  labels:\n    app: python\n    role: vote\n    version: v1\nspec:\n  containers:\n    - name: app\n      image: schoolofdevops/vote:v1\n\n\n\n\nUse this example link to refer to pod spec\n\n\nLaunching and operating Pods\n\n\nTo launch a monitoring screen to see whats being launched, use the following command in a new terminal window where kubectl is configured.\n\n\nwatch -n 1  kubectl get pods,deploy,rs,svc\n\n\n\n\n\nkubectl Syntax:\n\n\nkubectl\nkubectl apply --help\nkubectl apply -f FILE\n\n\n\n\nTo \nlaunch\n pod using configs above,\n\n\nkubectl apply -f vote-pod.yaml\n\n\n\n\n\nTo \nview\n pods\n\n\nkubectl get pods\n\nkubectl get po -o wide\n\nkubectl get pods vote\n\n\n\n\nTo get detailed info\n\n\nkubectl describe pods vote\n\n\n\n\nCommands to  \noperate\n the pod\n\n\n\nkubectl logs vote\n\nkubectl exec -it vote  sh\n\n\n\n\n\n\nRun the following commands inside the container in a pod after running exec command as above\n\n\nifconfig\ncat /etc/issue\nhostname\ncat /proc/cpuinfo\nps aux\n\n\n\n\nuse ^d or exit to log out\n\n\nAdding a Volume for data persistence\n\n\nLets create a pod for database and attach a volume to it. To achieve this we will need to\n\n\n\n\ncreate a \nvolumes\n definition\n\n\nattach volume to container using \nVolumeMounts\n property\n\n\n\n\nLocal host volumes are of two types:  \n\n\n\n\nemptyDir  \n\n\nhostPath  \n\n\n\n\nWe will pick hostPath. \nRefer to this doc to read more about hostPath.\n\n\nFile: db-pod.yaml\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: db\n  labels:\n    app: postgres\n    role: database\n    tier: back\nspec:\n  containers:\n    - name: db\n      image: postgres:9.4\n      ports:\n        - containerPort: 5432\n      volumeMounts:\n      - name: db-data\n        mountPath: /var/lib/postgresql/data\n  volumes:\n  - name: db-data\n    hostPath:\n      path: /var/lib/pgdata\n      type: DirectoryOrCreate\n\n\n\n\nTo create this pod,\n\n\nkubectl apply -f db-pod.yaml\n\nkubectl describe pod db\n\nkubectl get events\n\n\n\n\nExercise\n : Examine \n/var/lib/pgdata\n on the systems to check if the directory is been created and if the data is present.\n\n\nCreating Multi Container Pods\n\n\nfile: multi_container_pod.yml\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    tier: front\n    app: nginx\n    role: ui\nspec:\n  containers:\n    - name: nginx\n      image: nginx:stable-alpine\n      ports:\n        - containerPort: 80\n          protocol: TCP\n      volumeMounts:\n        - name: data\n          mountPath: /var/www/html-sample-app\n\n    - name: sync\n      image: schoolofdevops/sync:v2\n      volumeMounts:\n        - name: data\n          mountPath: /var/www/app\n\n  volumes:\n    - name: data\n      emptyDir: {}\n\n\n\n\nTo create this pod\n\n\nkubectl apply -f multi_container_pod.yml\n\n\n\n\nCheck Status\n\n\nroot@kube-01:~# kubectl get pods\nNAME      READY     STATUS              RESTARTS   AGE\nnginx     0/2       ContainerCreating   0          7s\nvote      1/1       Running             0          3m\n\n\n\n\nChecking logs, logging in\n\n\nkubectl logs  web  -c sync\nkubectl logs  web  -c nginx\n\nkubectl exec -it web  sh  -c nginx\nkubectl exec -it web  sh  -c sync\n\n\n\n\n\nObserve whats common and whats isolated in two containers running inside  the same pod using the following commands,\n\n\nshared\n\n\nhostname\nifconfig\n\n\n\n\nisolated\n\n\ncat /etc/issue\nps aux\ndf -h\n\n\n\n\n\nAdding Resource requests and limits\n\n\nWe can control the amount of resource requested and also put a limit on the maximum a container in a pod could take up.  This can be done by adding to the existing pod spec as below. Refer to \nofficial document on resource management\n here.\n\n\nFilename: vote-pod.yaml\n\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: vote\n  labels:\n    app: python\n    role: vote\n    version: v1\nspec:\n  containers:\n    - name: app\n      image: schoolofdevops/vote:v1\n      resources:\n        requests:\n          memory: \"64Mi\"\n          cpu: \"50m\"\n        limits:\n          memory: \"128Mi\"\n          cpu: \"250m\"\n\n\n\n\nLets apply the changes now\n\n\nkubectl apply -f vote-pod.yaml\n\n\n\n\nIf you already have \nvote\n pod running, you may see an output similar to below,\n\n\n[sample output]\n\n\nThe Pod \"vote\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n{\"Volumes\":[{\"Name\":\"default-token-snbj4\",\"HostPath\":null,\"EmptyDir\":null,\"GCEPersistentDisk\":null,\"AWSElasticBlockStore\":null,\"GitRepo\":null,\"Secret\":{\"SecretName\":\"default-token-snbj4\",\"Items\":null,\"DefaultMode\":420,\"Optional\":null},\"NFS\":null,\"ISCSI\":null,\"Glusterfs\":null,\"PersistentVolumeClaim\":null,\"RBD\":null,\"Quobyte\":null,\"FlexVolume\":null,\"Cinder\":null,\"CephFS\":null,\"Flocker\":null,\"DownwardAPI\":null,\"FC\":null,\"AzureFile\":null,\"ConfigMap\":null,\"VsphereVolume\":null,\"AzureDisk\":null,\"PhotonPersistentDisk\":null,\"Projected\":null,\"PortworxVolume\":null,\"ScaleIO\":null,\"StorageOS\":null}],\"InitContainers\":null,\"Containers\":[{\"Name\":\"app\",\"Image\":\"schoolofdevops/vote:v1\",\"Command\":null,\"Args\":null,\"WorkingDir\":\"\",\"Ports\":null,\"EnvFrom\":null,\"Env\":null,\"Resources\":{\"Limits\":\n....\n...\n\n\n\n\nFrom the above output, its clear that not all the fields are mutable(except for a few e.g labels). Container based deployments primarily follow concept of \nimmutable deployments\n. So to bring your change into effect, you need to re create the pod as,\n\n\nkubectl delete pod vote\n\nkubectl apply -f vote-pod.yaml\n\nkubectl describe pod vote\n\n\n\n\nFrom the output of the describe command above, you could confirm the resource constraints you added are in place.\n\n\nExercise\n\n\n* Define the value of **cpu.request** > **cpu.limit** Try to apply and observe.\n* Define the values for **memory.request** and **memory.limit** higher than the total system memory. Apply and observe the deployment and pods.\n\n\n\nDeleting Pods\n\n\nNow that you  are done experimenting with pod, \ndelete\n it with the following command,\n\n\nkubectl delete pod vote web db\n\nkubectl get pods\n\n\n\n\nReading List\n\n\n\n\nPodSpec:\n\n\nManaging Volumes with Kubernetes\n\n\nNode Selectors, Affinity",
            "title": "Lab K103 - Kubernetes LaunchPods"
        },
        {
            "location": "/5-vote-deploying_pods/#lab-k103-launching-pods-with-kubernetes",
            "text": "In this lab, you would learn how to launch applications using  the basic deployment unit of kubernetes i.e.  pods . This time, you are going to do it by writing declarative configs with  yaml  syntax.  Resource Configs  Each entity created with kubernetes is a resource including pod, service, deployments, replication controller etc. Resources can be defined as YAML or JSON.  Here is the syntax to create a YAML specification.  AKMS  => Resource Configs Specs  apiVersion: v1\nkind:\nmetadata:\nspec:  Use this  Kubernetes API Reference Document  while writing the API specs. This is the most important reference  while working with kubernetes, so its highly advisible that you bookmark it for the version of kubernetes that you use.  To find the versino of kubernetes use the following command,  kubectl version -o yaml  To list API objects, use the following commands,  kubectl api-resources  Writing Pod Spec  Lets now create the  Pod config by adding the kind and specs to schma given in the file vote-pod.yaml as follows.  Filename: k8s-code/pods/vote-pod.yaml  apiVersion:\nkind: Pod\nmetadata:\nspec:  Following are the specs to launch the vote application,   pod:  name: vote  labels:  app: python  role: vote  version: v1      container  name: app  image: schoolofdevops/vote:v1     Refer to the  Pod Spec  find out the relevant properties and add it to the  vote-pod.yaml  provided in the supporting code repo.  Filename: k8s-code/pods/vote-pod.yaml  apiVersion: v1\nkind: Pod\nmetadata:\n  name: vote\n  labels:\n    app: python\n    role: vote\n    version: v1\nspec:\n  containers:\n    - name: app\n      image: schoolofdevops/vote:v1  Use this example link to refer to pod spec  Launching and operating Pods  To launch a monitoring screen to see whats being launched, use the following command in a new terminal window where kubectl is configured.  watch -n 1  kubectl get pods,deploy,rs,svc  kubectl Syntax:  kubectl\nkubectl apply --help\nkubectl apply -f FILE  To  launch  pod using configs above,  kubectl apply -f vote-pod.yaml  To  view  pods  kubectl get pods\n\nkubectl get po -o wide\n\nkubectl get pods vote  To get detailed info  kubectl describe pods vote  Commands to   operate  the pod  \nkubectl logs vote\n\nkubectl exec -it vote  sh  Run the following commands inside the container in a pod after running exec command as above  ifconfig\ncat /etc/issue\nhostname\ncat /proc/cpuinfo\nps aux  use ^d or exit to log out",
            "title": "Lab K103: Launching Pods with Kubernetes"
        },
        {
            "location": "/5-vote-deploying_pods/#adding-a-volume-for-data-persistence",
            "text": "Lets create a pod for database and attach a volume to it. To achieve this we will need to   create a  volumes  definition  attach volume to container using  VolumeMounts  property   Local host volumes are of two types:     emptyDir    hostPath     We will pick hostPath.  Refer to this doc to read more about hostPath.  File: db-pod.yaml  apiVersion: v1\nkind: Pod\nmetadata:\n  name: db\n  labels:\n    app: postgres\n    role: database\n    tier: back\nspec:\n  containers:\n    - name: db\n      image: postgres:9.4\n      ports:\n        - containerPort: 5432\n      volumeMounts:\n      - name: db-data\n        mountPath: /var/lib/postgresql/data\n  volumes:\n  - name: db-data\n    hostPath:\n      path: /var/lib/pgdata\n      type: DirectoryOrCreate  To create this pod,  kubectl apply -f db-pod.yaml\n\nkubectl describe pod db\n\nkubectl get events  Exercise  : Examine  /var/lib/pgdata  on the systems to check if the directory is been created and if the data is present.",
            "title": "Adding a Volume for data persistence"
        },
        {
            "location": "/5-vote-deploying_pods/#creating-multi-container-pods",
            "text": "file: multi_container_pod.yml  apiVersion: v1\nkind: Pod\nmetadata:\n  name: web\n  labels:\n    tier: front\n    app: nginx\n    role: ui\nspec:\n  containers:\n    - name: nginx\n      image: nginx:stable-alpine\n      ports:\n        - containerPort: 80\n          protocol: TCP\n      volumeMounts:\n        - name: data\n          mountPath: /var/www/html-sample-app\n\n    - name: sync\n      image: schoolofdevops/sync:v2\n      volumeMounts:\n        - name: data\n          mountPath: /var/www/app\n\n  volumes:\n    - name: data\n      emptyDir: {}  To create this pod  kubectl apply -f multi_container_pod.yml  Check Status  root@kube-01:~# kubectl get pods\nNAME      READY     STATUS              RESTARTS   AGE\nnginx     0/2       ContainerCreating   0          7s\nvote      1/1       Running             0          3m  Checking logs, logging in  kubectl logs  web  -c sync\nkubectl logs  web  -c nginx\n\nkubectl exec -it web  sh  -c nginx\nkubectl exec -it web  sh  -c sync  Observe whats common and whats isolated in two containers running inside  the same pod using the following commands,  shared  hostname\nifconfig  isolated  cat /etc/issue\nps aux\ndf -h",
            "title": "Creating Multi Container Pods"
        },
        {
            "location": "/5-vote-deploying_pods/#adding-resource-requests-and-limits",
            "text": "We can control the amount of resource requested and also put a limit on the maximum a container in a pod could take up.  This can be done by adding to the existing pod spec as below. Refer to  official document on resource management  here.  Filename: vote-pod.yaml  apiVersion: v1\nkind: Pod\nmetadata:\n  name: vote\n  labels:\n    app: python\n    role: vote\n    version: v1\nspec:\n  containers:\n    - name: app\n      image: schoolofdevops/vote:v1\n      resources:\n        requests:\n          memory: \"64Mi\"\n          cpu: \"50m\"\n        limits:\n          memory: \"128Mi\"\n          cpu: \"250m\"  Lets apply the changes now  kubectl apply -f vote-pod.yaml  If you already have  vote  pod running, you may see an output similar to below,  [sample output]  The Pod \"vote\" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)\n{\"Volumes\":[{\"Name\":\"default-token-snbj4\",\"HostPath\":null,\"EmptyDir\":null,\"GCEPersistentDisk\":null,\"AWSElasticBlockStore\":null,\"GitRepo\":null,\"Secret\":{\"SecretName\":\"default-token-snbj4\",\"Items\":null,\"DefaultMode\":420,\"Optional\":null},\"NFS\":null,\"ISCSI\":null,\"Glusterfs\":null,\"PersistentVolumeClaim\":null,\"RBD\":null,\"Quobyte\":null,\"FlexVolume\":null,\"Cinder\":null,\"CephFS\":null,\"Flocker\":null,\"DownwardAPI\":null,\"FC\":null,\"AzureFile\":null,\"ConfigMap\":null,\"VsphereVolume\":null,\"AzureDisk\":null,\"PhotonPersistentDisk\":null,\"Projected\":null,\"PortworxVolume\":null,\"ScaleIO\":null,\"StorageOS\":null}],\"InitContainers\":null,\"Containers\":[{\"Name\":\"app\",\"Image\":\"schoolofdevops/vote:v1\",\"Command\":null,\"Args\":null,\"WorkingDir\":\"\",\"Ports\":null,\"EnvFrom\":null,\"Env\":null,\"Resources\":{\"Limits\":\n....\n...  From the above output, its clear that not all the fields are mutable(except for a few e.g labels). Container based deployments primarily follow concept of  immutable deployments . So to bring your change into effect, you need to re create the pod as,  kubectl delete pod vote\n\nkubectl apply -f vote-pod.yaml\n\nkubectl describe pod vote  From the output of the describe command above, you could confirm the resource constraints you added are in place.  Exercise  * Define the value of **cpu.request** > **cpu.limit** Try to apply and observe.\n* Define the values for **memory.request** and **memory.limit** higher than the total system memory. Apply and observe the deployment and pods.  Deleting Pods  Now that you  are done experimenting with pod,  delete  it with the following command,  kubectl delete pod vote web db\n\nkubectl get pods  Reading List   PodSpec:  Managing Volumes with Kubernetes  Node Selectors, Affinity",
            "title": "Adding Resource requests and limits"
        },
        {
            "location": "/replication/",
            "text": "Lab K104 - Adding HA and Scalability with ReplicaSets\n\n\nIf you are not running a monitoring screen, start it in a new terminal with the following command.\n\n\nwatch -n 1 kubectl get  pod,deploy,rs,svc\n\n\n\n\nCreating a Namespace and switching to it\n\n\nCheck current config\n\n\n\nkubectl config view\n\n\n\n\nYou could also examine the current configs in file \ncat ~/.kube/config\n\n\nCreating a namespace\n\n\nNamespaces offers separation of resources running on the same physical infrastructure into virtual clusters. It is typically useful in mid to large scale environments with multiple projects, teams and need separate scopes. It could also be useful to map to your workflow stages e.g. dev, stage, prod.   \n\n\nLets create a namespace called \ninstavote\n  \n\n\nkubectl get ns\n\nkubectl create namespace instavote\n\nkubectl get ns\n\n\n\n\n\nAnd switch to it\n\n\n\nkubectl config --help\n\nkubectl config get-contexts\n\nkubectl config current-context\n\nkubectl config set-context --help\n\nkubectl config set-context --current --namespace=instavote\n\nkubectl config get-contexts\n\nkubectl config view\n\n\n\n\n\n\nExercise\n: Go back to the monitoring screen and observe what happens after switching the namespace.\n\n\nTo understand how ReplicaSets works with the selectors  lets launch a pod in the new namespace with existing specs.\n\n\ncd k8s-code/pods\nkubectl apply -f vote-pod.yaml\n\nkubectl get pods\n\n\n\n\nAdding ReplicaSet Configurations\n\n\nLets now write the spec for the Rplica Set. This is going to mainly contain,\n\n\n\n\nreplicas\n\n\nselector\n\n\ntemplate (pod spec )\n\n\nminReadySeconds\n\n\n\n\nFrom here on, we would switch to the project and environment specific path and work from there.\n\n\ncd projects/instavote/dev\n\n\n\n\n\nedit file: vote-rs.yaml\n\n\napiVersion: xxx\nkind: xxx\nmetadata:\n  xxx\nspec:\n  xxx\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v1\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v1\n          resources:\n            requests:\n              memory: \"64Mi\"\n              cpu: \"50m\"\n            limits:\n              memory: \"128Mi\"\n              cpu: \"250m\"\n\n\n\n\nAbove file already containts the spec that you had written for the pod. You would observe its already been added as part of \nspec.template\n for replicaset.\n\n\nLets now add the details specific to replicaset.\n\n\nfile: vote-rs.yaml\n\n\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: vote\nspec:\n  replicas: 4\n  minReadySeconds: 20\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3, v4, v5]}\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v1\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v1\n          resources:\n            requests:\n              memory: \"64Mi\"\n              cpu: \"50m\"\n            limits:\n              memory: \"128Mi\"\n              cpu: \"250m\"\n\n\n\n\nThe complete file will look similar to above. Lets now go ahead and apply it.\n\n\nkubectl apply -f vote-rs.yaml --dry-run\n\nkubectl apply -f vote-rs.yaml\n\nkubectl get rs\n\nkubectl describe rs vote\n\nkubectl get pods\n\nkubectl get pods --show-labels\n\n\n\n\nHigh Availability\n\n\nTry deleting pods created by the replicaset,\n\n\nreplace pod-xxxx and pod-yyyy with actuals\n\n\nkubectl get pods\n\nkubectl delete pods vote-xxxx vote-yyyy\n\n\n\n\nObserve as the pods are automatically created again.\n\n\nLets now delete the pod created independent of replica set.\n\n\nkubectl get pods\nkubectl delete pods  vote\n\n\n\n\nObserve what happens.\n  * Does replica set take any action after deleting the pod created outside of its spec ? Why?\n\n\nExercise: Deploying new version of the application\n\n\nkubectl edit rs/vote\n\n\n\n\nUpdate the version of the image from \nschoolofdevops/vote:v1\n to \nschoolofdevops/vote:v2\n\n\nSave the file.\n\n\nObserve what happens ?\n\n\n\n\nDid application get  updated.\n\n\nDid updating replicaset launched new pods to deploy new version ?\n\n\n\n\nScalability\n\n\nScaling up application is as easy as running,  \n\n\nkubectl scale --replicas=8 rs/vote\n\nkubectl get pods --show-labels\n\n\n\n\nObserve what happens\n\n\n\n\nDid the number of  replicas increase to 8 ?\n\n\nWhich version of the app are the new pods running with ?\n\n\n\n\nSummary\n\n\nWith \nReplicaSets\n your application is now high available as well as scalable. However ReplicaSet by itself does not have the intelligence to trigger a rollout if you update the version. For that, you are going to need a \ndeployment\n which is something you would learn in an upcoming  lesson.",
            "title": "Lab K104 - Adding HA and Scalability with ReplicaSets"
        },
        {
            "location": "/replication/#lab-k104-adding-ha-and-scalability-with-replicasets",
            "text": "If you are not running a monitoring screen, start it in a new terminal with the following command.  watch -n 1 kubectl get  pod,deploy,rs,svc  Creating a Namespace and switching to it  Check current config  \nkubectl config view  You could also examine the current configs in file  cat ~/.kube/config",
            "title": "Lab K104 - Adding HA and Scalability with ReplicaSets"
        },
        {
            "location": "/replication/#creating-a-namespace",
            "text": "Namespaces offers separation of resources running on the same physical infrastructure into virtual clusters. It is typically useful in mid to large scale environments with multiple projects, teams and need separate scopes. It could also be useful to map to your workflow stages e.g. dev, stage, prod.     Lets create a namespace called  instavote     kubectl get ns\n\nkubectl create namespace instavote\n\nkubectl get ns  And switch to it  \nkubectl config --help\n\nkubectl config get-contexts\n\nkubectl config current-context\n\nkubectl config set-context --help\n\nkubectl config set-context --current --namespace=instavote\n\nkubectl config get-contexts\n\nkubectl config view  Exercise : Go back to the monitoring screen and observe what happens after switching the namespace.  To understand how ReplicaSets works with the selectors  lets launch a pod in the new namespace with existing specs.  cd k8s-code/pods\nkubectl apply -f vote-pod.yaml\n\nkubectl get pods",
            "title": "Creating a namespace"
        },
        {
            "location": "/replication/#adding-replicaset-configurations",
            "text": "Lets now write the spec for the Rplica Set. This is going to mainly contain,   replicas  selector  template (pod spec )  minReadySeconds   From here on, we would switch to the project and environment specific path and work from there.  cd projects/instavote/dev  edit file: vote-rs.yaml  apiVersion: xxx\nkind: xxx\nmetadata:\n  xxx\nspec:\n  xxx\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v1\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v1\n          resources:\n            requests:\n              memory: \"64Mi\"\n              cpu: \"50m\"\n            limits:\n              memory: \"128Mi\"\n              cpu: \"250m\"  Above file already containts the spec that you had written for the pod. You would observe its already been added as part of  spec.template  for replicaset.  Lets now add the details specific to replicaset.  file: vote-rs.yaml  apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: vote\nspec:\n  replicas: 4\n  minReadySeconds: 20\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3, v4, v5]}\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v1\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v1\n          resources:\n            requests:\n              memory: \"64Mi\"\n              cpu: \"50m\"\n            limits:\n              memory: \"128Mi\"\n              cpu: \"250m\"  The complete file will look similar to above. Lets now go ahead and apply it.  kubectl apply -f vote-rs.yaml --dry-run\n\nkubectl apply -f vote-rs.yaml\n\nkubectl get rs\n\nkubectl describe rs vote\n\nkubectl get pods\n\nkubectl get pods --show-labels  High Availability  Try deleting pods created by the replicaset,  replace pod-xxxx and pod-yyyy with actuals  kubectl get pods\n\nkubectl delete pods vote-xxxx vote-yyyy  Observe as the pods are automatically created again.  Lets now delete the pod created independent of replica set.  kubectl get pods\nkubectl delete pods  vote  Observe what happens.\n  * Does replica set take any action after deleting the pod created outside of its spec ? Why?  Exercise: Deploying new version of the application  kubectl edit rs/vote  Update the version of the image from  schoolofdevops/vote:v1  to  schoolofdevops/vote:v2  Save the file.  Observe what happens ?   Did application get  updated.  Did updating replicaset launched new pods to deploy new version ?   Scalability  Scaling up application is as easy as running,    kubectl scale --replicas=8 rs/vote\n\nkubectl get pods --show-labels  Observe what happens   Did the number of  replicas increase to 8 ?  Which version of the app are the new pods running with ?   Summary  With  ReplicaSets  your application is now high available as well as scalable. However ReplicaSet by itself does not have the intelligence to trigger a rollout if you update the version. For that, you are going to need a  deployment  which is something you would learn in an upcoming  lesson.",
            "title": "Adding ReplicaSet Configurations"
        },
        {
            "location": "/7-vote-exposing_app_with_service/",
            "text": "LAB K105 - Load Balancing and Service Discovery with Services\n\n\nIn this lab, you would not only publish the application deployed with replicaset earlier, but also learn about the load balancing and service discovery features offered by kubernetes.\n\n\nConcepts related to Kubernetes Services are depicted in the following diagram,\n\n\n\n\nPublishing external facing app with NodePort\n\n\nKubernetes comes with four types of services viz.\n\n\n\n\nClusterIP\n\n\nNodePort\n\n\nLoadBalancer\n\n\nExternalName\n\n\n\n\nLets create a  service of type \nNodePort\n to understand how it works.\n\n\nTo check the status of kubernetes objects,\n\n\nkubectl get pods,rs,svc\n\n\n\n\nYou could also start watching the above output for changes. To do so, open a separate terminal window and run,\n\n\nwatch -n 1 kubectl get  pod,deploy,rs,svc\n\n\n\n\nRefer to \nService Specs\n to understand the properties that you could write.\n\n\nfilename: vote-svc.yaml\n\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vote\n  labels:\n    role: vote\nspec:\n  selector:\n    role: vote\n  ports:\n    - port: 80\n      targetPort: 80\n      nodePort: 30000\n  type: NodePort\n\n\n\n\n\nApply this file to to create a service\n\n\nkubectl apply -f vote-svc.yaml --dry-run\nkubectl apply -f vote-svc.yaml\nkubectl get svc\nkubectl describe service vote\n\n\n\n\n[Sample Output of describe command]\n\n\nName:                     vote\nNamespace:                instavote\nLabels:                   role=svc\n                          tier=front\nAnnotations:              kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"role\":\"svc\",\"tier\":\"front\"},\"name\":\"vote\",\"namespace\":\"instavote\"},\"spec\":{...\nSelector:                 app=vote\nType:                     NodePort\nIP:                       10.108.108.157\nPort:                     <unset>  80/TCP\nTargetPort:               80/TCP\nNodePort:                 <unset>  31429/TCP\nEndpoints:                10.38.0.4:80,10.38.0.5:80,10.38.0.6:80 + 2 more...\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   <none>\n\n\n\n\nObserve the following\n\n\n\n\nSelector\n\n\nTargetPort\n\n\nNodePort\n\n\nEndpoints\n\n\n\n\nGo to browser and check http://HOSTIP:NODEPORT\n\n\nHere the node port is 30000 (as defined by nodePort in service spec).\n\n\nSample output will be:\n\n\n\n\nIf you refresh the page, you should also notice its sending traffic to diffent pod each time, in round robin fashion.\n\n\nExercises\n\n\n\n\nChange the selector criteria to use a non existant label. Use \nkubectl edit svc./vote\n to update and apply the configuration. Observe the output of describe command and check the endpoints. Do you see any ?  How are  selectors and pod labels related ?\n\n\nObserve the number of endpoints. Then change the scale of replicas created by the replicasets. Does it have any effect on the number of endpoints ?    \n\n\n\n\nServices Under the Hood\n\n\nLets traverse the route of the \nnetwork packet\n that comes in on port 30000 on any node in your cluster.\n\n\niptables -nvL -t nat  \niptables -nvL -t nat  | grep 30000\n\n\n\n\nAnything that comes on dpt:3000, gets forwarded to the chain created for that service.\n\n\niptables -nvL -t nat  | grep KUBE-SVC-VIQHAVHDK4QE7NA4  -A 10\n\n\n\n\n\nChain KUBE-SVC-VIQHAVHDK4QE7NA4 (2 references)\n pkts bytes target     prot opt in     out     source               destination\n    0     0 KUBE-SEP-RFJGHFMXUDJXIEW6  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* instavote/vote: */ statistic mode random probability 0.20000000019\n    0     0 KUBE-SEP-GBR5YQCVRYY3BA6U  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* instavote/vote: */ statistic mode random probability 0.25000000000\n    0     0 KUBE-SEP-BAI3HQ7SV7RZ2CI6  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* instavote/vote: */ statistic mode random probability 0.33332999982\n    0     0 KUBE-SEP-2EQSLPEP3WDOTI5J  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* instavote/vote: */ statistic mode random probability 0.50000000000\n    0     0 KUBE-SEP-2CJQISP4W7F2HCRW  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* instavote/vote: */\n\n\n\n\n\nWhere,\n  * counrt of KUBE-SEP-xxx matches number of pods\n  * KUBE-SEP-BAI3HQ7SV7RZ2CI6  is a chain created for one of host. examine that next\n\n\n\n\niptables -nvL -t nat  | grep KUBE-SEP-BAI3HQ7SV7RZ2CI6  -A 3\n\n\n\n\n[output]\n\n\npkts bytes target     prot opt in     out     source               destination\n    0     0 KUBE-MARK-MASQ  all  --  *      *       10.32.0.6            0.0.0.0/0            /* instavote/vote: */\n    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* instavote/vote: */ tcp to:10.32.0.6:80\n--\n\n\n\n\nwhere the packet is being forwarded to 10.32.0.6, which should corraborate with the ip of the pod\n\n\ne.g.\n\n\nkubectl get pods -o wide\n\nNAME         READY     STATUS    RESTARTS   AGE       IP          NODE\nvote-58bpv   1/1       Running   0          1h        10.32.0.6   k-02\nvote-986cl   1/1       Running   0          1h        10.38.0.5   k-03\nvote-9rrfz   1/1       Running   0          1h        10.38.0.4   k-03\nvote-dx8f4   1/1       Running   0          1h        10.32.0.4   k-02\nvote-qxmfl   1/1       Running   0          1h        10.32.0.5   k-02\n\n\n\n\n10.32.0.6 matches ip of vote-58bpv  \n\n\nto check how the packet is routed next use,\n\n\nroute -n\n\n\n\n\n[output]\n\n\nKernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\n0.0.0.0         206.189.144.1   0.0.0.0         UG    0      0        0 eth0\n10.15.0.0       0.0.0.0         255.255.0.0     U     0      0        0 eth0\n10.32.0.0       0.0.0.0         255.240.0.0     U     0      0        0 weave\n172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0\n206.189.144.0   0.0.0.0         255.255.240.0   U     0      0        0 eth0\n\n\n\n\n\nwhere, 10.32.0.0 is going over \nweave\n interface.\n\n\nExposing app with ExternalIP\n\n\nObserve the output of service list, specifically note the \nEXTERNAL-IP\n colum in the output.\n\n\nkubectl  get svc\n\n\n\n\nNow, update the service spec and add external IP configs. Pick IP addresses of any two nodes  (You could add one or more) and it to the spec as,\n\n\nkubectl edit svc vote\n\n\n\n\n[sample file edit]\n\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vote\n  labels:\n    role: vote\nspec:\n  selector:\n    role: vote\n  ports:\n    - port: 80\n      targetPort: 80\n      nodePort: 30000\n  type: NodePort\n  externalIPs:\n    - xx.xx.xx.xx\n    - yy.yy.yy.yy\n\n\n\n\nWhere\n\n\nreplace xx.xx.xx.xx and yy.yy.yy.yy with IP addresses of the nodes on two of the kubernetes hosts.\n\n\napply\n\n\nkubectl  get svc\nkubectl apply -f vote-svc.yaml\nkubectl  get svc\nkubectl describe svc vote\n\n\n\n\n[sample output]\n\n\nNAME      TYPE       CLUSTER-IP      EXTERNAL-IP                    PORT(S)        AGE\nvote      NodePort   10.107.71.204   206.189.150.190,159.65.8.227   80:30000/TCP   11m\n\n\n\n\nwhere,\n\n\nEXTERNAL-IP column shows which IPs the application is been exposed on. You could go to http://\n:\n to access this application.  e.g. http://206.189.150.190:80 where you should replace 206.189.150.190 with the actual IP address of the node that you exposed this on.\n\n\nInternal Service Discovery\n\n\nKubernetes not only allows you to publish external facing apps with the services, but also allows you to discover other components of your application stack with the clusterIP and DNS attached to it.\n\n\nBefore you begin adding service discovery,\n\n\n\n\nVisit the vote app from browser\n\n\nAttempt to vote by clicking on one of the options\n\n\n\n\nobserve what happens. Does it go through?  \n\n\nDebugging,\n\n\nkubectl get pod\nkubectl exec vote-xxxx nslookup redis\n\n\n\n\n\n[replace xxxx with the actual pod id of one of the vote pods ]\n\n\nkeep the above command on a watch. You should create a new terminal to run the watch command.\n\n\ne.g.\n\n\nkubectl exec -it vote-xxxx sh\nwatch  kubectl exec vote-xxxx ping redis\n\n\n\n\nwhere, vote-xxxx is one of the vote pods that I am running. Replace this with the actual pod id.\n\n\nNow create \nredis\n service\n\n\nkubectl apply -f redis-svc.yaml\n\nkubectl get svc\n\nkubectl describe svc redis\n\n\n\n\nWatch the nslookup screen  and observe if its able to resolve \nredis\n by hostname and its pointing to an IP address.\n\n\ne.g.\n\n\nName:      redis\nAddress 1: 10.104.111.173 redis.instavote.svc.cluster.local\n\n\n\n\nwhere\n\n\n\n\n10.104.111.173 is the ClusterIP assigned to redis service\n\n\nredis.instavote.svc.cluster.local is the dns attached to the ClusterIP above\n\n\n\n\nWhat happened here?\n\n\n\n\nService \nredis\n was created with a ClusterIP e.g. 10.102.77.6\n\n\nA DNS entry was created for this service. The fqdn of the service is \nredis.instavote.svc.cluster.local\n and it takes the form of\n  my-svc.my-namespace.svc.cluster.local\n\n\nEach pod points to  internal  DNS server running in the cluster. You could see the details of this by running the following commands\n\n\n\n\nkubectl exec vote-xxxx cat /etc/resolv.conf\n\n\n\n\n[replace vote-xxxx with actual pod id]\n\n\n[sample output]\n\n\nnameserver 10.96.0.10\nsearch instavote.svc.cluster.local svc.cluster.local cluster.local\noptions ndots:5\n\n\n\n\nwhere \n10.96.0.10\n is the ClusterIP assigned to the DNS service. You could co relate that with,\n\n\nkubectl get svc -n kube-system\n\n\nNAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE\nkube-dns               ClusterIP   10.96.0.10     <none>        53/UDP,53/TCP   1h\nkubernetes-dashboard   NodePort    10.104.42.73   <none>        80:31000/TCP    23m\n\n\n\n\n\nwhere, \n10.96.0.10\n is the ClusterIP assigned to \nkube-dns\n and matches the configuration in \n/etc/resolv.conf\n above.\n\n\nCreating Endpoints for Redis\n\n\nService is been created, but you still need to launch the actual pods running \nredis\n application.\n\n\nCreate the endpoints now,\n\n\nkubectl apply -f redis-deploy.yaml\nkubectl describe svc redis\n\n\n\n\n\n[sample output]\n\n\nName:              redis\nNamespace:         instavote\nLabels:            role=redis\n                   tier=back\nAnnotations:       kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"role\":\"redis\",\"tier\":\"back\"},\"name\":\"redis\",\"namespace\":\"instavote\"},\"spec\"...\nSelector:          app=redis\nType:              ClusterIP\nIP:                10.102.77.6\nPort:              <unset>  6379/TCP\nTargetPort:        6379/TCP\nEndpoints:         10.32.0.6:6379,10.46.0.6:6379\nSession Affinity:  None\nEvents:            <none>\n\n\n\n\nAgain, visit the vote app from browser, attempt to register your vote.  observe what happens. This time the vote should be registered successfully.\n\n\nSummary\n\n\nIn this lab, you have published a front facing application, learnt how services are implemented under the hood as well as added service discovery to provide  connection strings automatically.\n\n\nReading\n\n\n\n\nDebugging Services\n\n\nKubernetes Services Documentation\n\n\nService API Specs for Kubernetes Version 1.10",
            "title": "Lab K105 - Load Balancing and Service Discovery with Services"
        },
        {
            "location": "/7-vote-exposing_app_with_service/#lab-k105-load-balancing-and-service-discovery-with-services",
            "text": "In this lab, you would not only publish the application deployed with replicaset earlier, but also learn about the load balancing and service discovery features offered by kubernetes.  Concepts related to Kubernetes Services are depicted in the following diagram,   Publishing external facing app with NodePort  Kubernetes comes with four types of services viz.   ClusterIP  NodePort  LoadBalancer  ExternalName   Lets create a  service of type  NodePort  to understand how it works.  To check the status of kubernetes objects,  kubectl get pods,rs,svc  You could also start watching the above output for changes. To do so, open a separate terminal window and run,  watch -n 1 kubectl get  pod,deploy,rs,svc  Refer to  Service Specs  to understand the properties that you could write.  filename: vote-svc.yaml  ---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vote\n  labels:\n    role: vote\nspec:\n  selector:\n    role: vote\n  ports:\n    - port: 80\n      targetPort: 80\n      nodePort: 30000\n  type: NodePort  Apply this file to to create a service  kubectl apply -f vote-svc.yaml --dry-run\nkubectl apply -f vote-svc.yaml\nkubectl get svc\nkubectl describe service vote  [Sample Output of describe command]  Name:                     vote\nNamespace:                instavote\nLabels:                   role=svc\n                          tier=front\nAnnotations:              kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"role\":\"svc\",\"tier\":\"front\"},\"name\":\"vote\",\"namespace\":\"instavote\"},\"spec\":{...\nSelector:                 app=vote\nType:                     NodePort\nIP:                       10.108.108.157\nPort:                     <unset>  80/TCP\nTargetPort:               80/TCP\nNodePort:                 <unset>  31429/TCP\nEndpoints:                10.38.0.4:80,10.38.0.5:80,10.38.0.6:80 + 2 more...\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   <none>  Observe the following   Selector  TargetPort  NodePort  Endpoints   Go to browser and check http://HOSTIP:NODEPORT  Here the node port is 30000 (as defined by nodePort in service spec).  Sample output will be:   If you refresh the page, you should also notice its sending traffic to diffent pod each time, in round robin fashion.  Exercises   Change the selector criteria to use a non existant label. Use  kubectl edit svc./vote  to update and apply the configuration. Observe the output of describe command and check the endpoints. Do you see any ?  How are  selectors and pod labels related ?  Observe the number of endpoints. Then change the scale of replicas created by the replicasets. Does it have any effect on the number of endpoints ?       Services Under the Hood  Lets traverse the route of the  network packet  that comes in on port 30000 on any node in your cluster.  iptables -nvL -t nat  \niptables -nvL -t nat  | grep 30000  Anything that comes on dpt:3000, gets forwarded to the chain created for that service.  iptables -nvL -t nat  | grep KUBE-SVC-VIQHAVHDK4QE7NA4  -A 10  Chain KUBE-SVC-VIQHAVHDK4QE7NA4 (2 references)\n pkts bytes target     prot opt in     out     source               destination\n    0     0 KUBE-SEP-RFJGHFMXUDJXIEW6  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* instavote/vote: */ statistic mode random probability 0.20000000019\n    0     0 KUBE-SEP-GBR5YQCVRYY3BA6U  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* instavote/vote: */ statistic mode random probability 0.25000000000\n    0     0 KUBE-SEP-BAI3HQ7SV7RZ2CI6  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* instavote/vote: */ statistic mode random probability 0.33332999982\n    0     0 KUBE-SEP-2EQSLPEP3WDOTI5J  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* instavote/vote: */ statistic mode random probability 0.50000000000\n    0     0 KUBE-SEP-2CJQISP4W7F2HCRW  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* instavote/vote: */  Where,\n  * counrt of KUBE-SEP-xxx matches number of pods\n  * KUBE-SEP-BAI3HQ7SV7RZ2CI6  is a chain created for one of host. examine that next  \n\niptables -nvL -t nat  | grep KUBE-SEP-BAI3HQ7SV7RZ2CI6  -A 3  [output]  pkts bytes target     prot opt in     out     source               destination\n    0     0 KUBE-MARK-MASQ  all  --  *      *       10.32.0.6            0.0.0.0/0            /* instavote/vote: */\n    0     0 DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* instavote/vote: */ tcp to:10.32.0.6:80\n--  where the packet is being forwarded to 10.32.0.6, which should corraborate with the ip of the pod  e.g.  kubectl get pods -o wide\n\nNAME         READY     STATUS    RESTARTS   AGE       IP          NODE\nvote-58bpv   1/1       Running   0          1h        10.32.0.6   k-02\nvote-986cl   1/1       Running   0          1h        10.38.0.5   k-03\nvote-9rrfz   1/1       Running   0          1h        10.38.0.4   k-03\nvote-dx8f4   1/1       Running   0          1h        10.32.0.4   k-02\nvote-qxmfl   1/1       Running   0          1h        10.32.0.5   k-02  10.32.0.6 matches ip of vote-58bpv    to check how the packet is routed next use,  route -n  [output]  Kernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\n0.0.0.0         206.189.144.1   0.0.0.0         UG    0      0        0 eth0\n10.15.0.0       0.0.0.0         255.255.0.0     U     0      0        0 eth0\n10.32.0.0       0.0.0.0         255.240.0.0     U     0      0        0 weave\n172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0\n206.189.144.0   0.0.0.0         255.255.240.0   U     0      0        0 eth0  where, 10.32.0.0 is going over  weave  interface.  Exposing app with ExternalIP  Observe the output of service list, specifically note the  EXTERNAL-IP  colum in the output.  kubectl  get svc  Now, update the service spec and add external IP configs. Pick IP addresses of any two nodes  (You could add one or more) and it to the spec as,  kubectl edit svc vote  [sample file edit]  ---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vote\n  labels:\n    role: vote\nspec:\n  selector:\n    role: vote\n  ports:\n    - port: 80\n      targetPort: 80\n      nodePort: 30000\n  type: NodePort\n  externalIPs:\n    - xx.xx.xx.xx\n    - yy.yy.yy.yy  Where  replace xx.xx.xx.xx and yy.yy.yy.yy with IP addresses of the nodes on two of the kubernetes hosts.  apply  kubectl  get svc\nkubectl apply -f vote-svc.yaml\nkubectl  get svc\nkubectl describe svc vote  [sample output]  NAME      TYPE       CLUSTER-IP      EXTERNAL-IP                    PORT(S)        AGE\nvote      NodePort   10.107.71.204   206.189.150.190,159.65.8.227   80:30000/TCP   11m  where,  EXTERNAL-IP column shows which IPs the application is been exposed on. You could go to http:// :  to access this application.  e.g. http://206.189.150.190:80 where you should replace 206.189.150.190 with the actual IP address of the node that you exposed this on.",
            "title": "LAB K105 - Load Balancing and Service Discovery with Services"
        },
        {
            "location": "/7-vote-exposing_app_with_service/#internal-service-discovery",
            "text": "Kubernetes not only allows you to publish external facing apps with the services, but also allows you to discover other components of your application stack with the clusterIP and DNS attached to it.  Before you begin adding service discovery,   Visit the vote app from browser  Attempt to vote by clicking on one of the options   observe what happens. Does it go through?    Debugging,  kubectl get pod\nkubectl exec vote-xxxx nslookup redis  [replace xxxx with the actual pod id of one of the vote pods ]  keep the above command on a watch. You should create a new terminal to run the watch command.  e.g.  kubectl exec -it vote-xxxx sh\nwatch  kubectl exec vote-xxxx ping redis  where, vote-xxxx is one of the vote pods that I am running. Replace this with the actual pod id.  Now create  redis  service  kubectl apply -f redis-svc.yaml\n\nkubectl get svc\n\nkubectl describe svc redis  Watch the nslookup screen  and observe if its able to resolve  redis  by hostname and its pointing to an IP address.  e.g.  Name:      redis\nAddress 1: 10.104.111.173 redis.instavote.svc.cluster.local  where   10.104.111.173 is the ClusterIP assigned to redis service  redis.instavote.svc.cluster.local is the dns attached to the ClusterIP above   What happened here?   Service  redis  was created with a ClusterIP e.g. 10.102.77.6  A DNS entry was created for this service. The fqdn of the service is  redis.instavote.svc.cluster.local  and it takes the form of\n  my-svc.my-namespace.svc.cluster.local  Each pod points to  internal  DNS server running in the cluster. You could see the details of this by running the following commands   kubectl exec vote-xxxx cat /etc/resolv.conf  [replace vote-xxxx with actual pod id]  [sample output]  nameserver 10.96.0.10\nsearch instavote.svc.cluster.local svc.cluster.local cluster.local\noptions ndots:5  where  10.96.0.10  is the ClusterIP assigned to the DNS service. You could co relate that with,  kubectl get svc -n kube-system\n\n\nNAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE\nkube-dns               ClusterIP   10.96.0.10     <none>        53/UDP,53/TCP   1h\nkubernetes-dashboard   NodePort    10.104.42.73   <none>        80:31000/TCP    23m  where,  10.96.0.10  is the ClusterIP assigned to  kube-dns  and matches the configuration in  /etc/resolv.conf  above.  Creating Endpoints for Redis  Service is been created, but you still need to launch the actual pods running  redis  application.  Create the endpoints now,  kubectl apply -f redis-deploy.yaml\nkubectl describe svc redis  [sample output]  Name:              redis\nNamespace:         instavote\nLabels:            role=redis\n                   tier=back\nAnnotations:       kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"role\":\"redis\",\"tier\":\"back\"},\"name\":\"redis\",\"namespace\":\"instavote\"},\"spec\"...\nSelector:          app=redis\nType:              ClusterIP\nIP:                10.102.77.6\nPort:              <unset>  6379/TCP\nTargetPort:        6379/TCP\nEndpoints:         10.32.0.6:6379,10.46.0.6:6379\nSession Affinity:  None\nEvents:            <none>  Again, visit the vote app from browser, attempt to register your vote.  observe what happens. This time the vote should be registered successfully.  Summary  In this lab, you have published a front facing application, learnt how services are implemented under the hood as well as added service discovery to provide  connection strings automatically.  Reading   Debugging Services  Kubernetes Services Documentation  Service API Specs for Kubernetes Version 1.10",
            "title": "Internal Service Discovery"
        },
        {
            "location": "/6-vote-kubernetes_deployment/",
            "text": "LAB K106 - Defining Release Strategy with  Deployment\n\n\nA Deployment is a higher level abstraction which sits on top of replica sets and allows you to manage the way applications are deployed, rolled back at a controlled rate.\n\n\nDeployment has mainly two responsibilities,\n\n\n\n\nProvide Fault Tolerance: Maintain the number of replicas for a type of service/app. Schedule/delete pods to meet the desired count.\n\n\nUpdate Strategy: Define a release strategy and update the pods accordingly.\n\n\n\n\n/k8s-code/projects/instavote/dev/\ncp vote-rs.yaml vote-deploy.yaml\n\n\n\n\nDeployment spec (deployment.spec) contains everything that replica set has + strategy. Lets add it as follows,\n\n\nfile: vote-deploy.yaml\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote\nspec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2\n      maxUnavailable: 1\n  revisionHistoryLimit: 4\n  replicas: 12\n  minReadySeconds: 20\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3, v4, v5]}\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v1\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v1\n          resources:\n            requests:\n              memory: \"64Mi\"\n              cpu: \"50m\"\n            limits:\n              memory: \"128Mi\"\n              cpu: \"250m\"\n\n\n\n\nThis time, start monitoring with --show-labels options added.\n\n\nwatch -n 1 kubectl get  pod,deploy,rs,svc --show-labels\n\n\n\n\nLets  create the Deployment. Do monitor the labels of the pod while applying this.\n\n\nkubectl apply -f vote-deploy.yaml\n\n\n\n\nObserve the chances to pod labels, specifically the \npod-template-hash\n.\n\n\nNow that the deployment is created. To validate,\n\n\nkubectl get deployment\nkubectl get rs --show-labels\nkubectl get deploy,pods,rs\nkubectl rollout status deployment/vote\nkubectl get pods --show-labels\n\n\n\n\nSample Output\n\n\nkubectl get deployments\nNAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nvote   3         3         3            1           3m\n\n\n\n\nScaling a deployment\n\n\nTo scale a deployment in Kubernetes:\n\n\nkubectl scale deployment/vote --replicas=15\n\nkubectl rollout status deployment/vote\n\n\n\n\n\nSample output:\n\n\n\nWaiting for rollout to finish: 5 of 15 updated replicas are available...\nWaiting for rollout to finish: 6 of 15 updated replicas are available...\ndeployment \"vote\" successfully rolled out\n\n\n\n\nYou could also update the deployment by editing it.\n\n\nkubectl edit deploy/vote\n\n\n\n\n[change replicas to 8 from the editor, save and observe]\n\n\nRolling Updates in Action\n\n\nNow, update the deployment spec to apply\n\n\nfile: vote-deploy.yaml\n\n\n\n...\ntemplate:\n  metadata:\n    labels:\n      version: v2   \n  spec:\n    containers:\n      - name: app\n        image: schoolofdevops/vote:v2\n\n\n\n\n\napply\n\n\nkubectl apply -f vote-deploy.yaml\n\nkubectl rollout status deployment/vote\n\n\n\n\nObserve rollout status and monitoring screen.\n\n\n\nkubectl rollout history deploy/vote\n\nkubectl rollout history deploy/vote --revision=1\n\n\n\n\n\nTry updating the version of the image from v2 to v3,v4,v5. Repeat a few times to observe how it rolls out a new version.  \n\n\nUndo and Rollback\n\n\nfile: vote-deploy.yaml\n\n\nspec:\n  containers:\n    - name: app\n      image: schoolofdevops/vote:rgjerdf\n\n\n\n\n\napply\n\n\nkubectl apply -f vote-deploy.yaml\n\nkubectl rollout status\n\nkubectl rollout history deploy/vote\n\nkubectl rollout history deploy/vote --revision=xx\n\n\n\n\nwhere replace xxx with revisions\n\n\nFind out the previous revision with sane configs.\n\n\nTo undo to a sane version (for example revision 3)\n\n\nkubectl rollout undo deploy/vote --to-revision=2",
            "title": "Lab K106 - Defining Release Strategy with Deployments"
        },
        {
            "location": "/6-vote-kubernetes_deployment/#lab-k106-defining-release-strategy-with-deployment",
            "text": "A Deployment is a higher level abstraction which sits on top of replica sets and allows you to manage the way applications are deployed, rolled back at a controlled rate.  Deployment has mainly two responsibilities,   Provide Fault Tolerance: Maintain the number of replicas for a type of service/app. Schedule/delete pods to meet the desired count.  Update Strategy: Define a release strategy and update the pods accordingly.   /k8s-code/projects/instavote/dev/\ncp vote-rs.yaml vote-deploy.yaml  Deployment spec (deployment.spec) contains everything that replica set has + strategy. Lets add it as follows,  file: vote-deploy.yaml  apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote\nspec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2\n      maxUnavailable: 1\n  revisionHistoryLimit: 4\n  replicas: 12\n  minReadySeconds: 20\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3, v4, v5]}\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v1\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v1\n          resources:\n            requests:\n              memory: \"64Mi\"\n              cpu: \"50m\"\n            limits:\n              memory: \"128Mi\"\n              cpu: \"250m\"  This time, start monitoring with --show-labels options added.  watch -n 1 kubectl get  pod,deploy,rs,svc --show-labels  Lets  create the Deployment. Do monitor the labels of the pod while applying this.  kubectl apply -f vote-deploy.yaml  Observe the chances to pod labels, specifically the  pod-template-hash .  Now that the deployment is created. To validate,  kubectl get deployment\nkubectl get rs --show-labels\nkubectl get deploy,pods,rs\nkubectl rollout status deployment/vote\nkubectl get pods --show-labels  Sample Output  kubectl get deployments\nNAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nvote   3         3         3            1           3m",
            "title": "LAB K106 - Defining Release Strategy with  Deployment"
        },
        {
            "location": "/6-vote-kubernetes_deployment/#scaling-a-deployment",
            "text": "To scale a deployment in Kubernetes:  kubectl scale deployment/vote --replicas=15\n\nkubectl rollout status deployment/vote  Sample output:  \nWaiting for rollout to finish: 5 of 15 updated replicas are available...\nWaiting for rollout to finish: 6 of 15 updated replicas are available...\ndeployment \"vote\" successfully rolled out  You could also update the deployment by editing it.  kubectl edit deploy/vote  [change replicas to 8 from the editor, save and observe]",
            "title": "Scaling a deployment"
        },
        {
            "location": "/6-vote-kubernetes_deployment/#rolling-updates-in-action",
            "text": "Now, update the deployment spec to apply  file: vote-deploy.yaml  \n...\ntemplate:\n  metadata:\n    labels:\n      version: v2   \n  spec:\n    containers:\n      - name: app\n        image: schoolofdevops/vote:v2  apply  kubectl apply -f vote-deploy.yaml\n\nkubectl rollout status deployment/vote  Observe rollout status and monitoring screen.  \nkubectl rollout history deploy/vote\n\nkubectl rollout history deploy/vote --revision=1  Try updating the version of the image from v2 to v3,v4,v5. Repeat a few times to observe how it rolls out a new version.",
            "title": "Rolling Updates in Action"
        },
        {
            "location": "/6-vote-kubernetes_deployment/#undo-and-rollback",
            "text": "file: vote-deploy.yaml  spec:\n  containers:\n    - name: app\n      image: schoolofdevops/vote:rgjerdf  apply  kubectl apply -f vote-deploy.yaml\n\nkubectl rollout status\n\nkubectl rollout history deploy/vote\n\nkubectl rollout history deploy/vote --revision=xx  where replace xxx with revisions  Find out the previous revision with sane configs.  To undo to a sane version (for example revision 3)  kubectl rollout undo deploy/vote --to-revision=2",
            "title": "Undo and Rollback"
        },
        {
            "location": "/11_deploying_sample_app/",
            "text": "Mini Project: Deploying Multi Tier Application Stack\n\n\nIn this project , you would write definitions for deploying the vote application stack with all components/tiers which include,\n\n\n\n\nvote ui\n\n\nredis\n\n\nworker\n\n\ndb\n\n\nresults ui\n\n\n\n\nTasks\n\n\n\n\nCreate deployments for all applications\n\n\nDefine services for each tier applicable\n\n\nLaunch/apply the definitions\n\n\n\n\nFollowing table depicts the state of readiness of the above services.\n\n\n\n\n\n\n\n\nApp\n\n\nDeployment\n\n\nService\n\n\n\n\n\n\n\n\n\n\nvote\n\n\nready\n\n\nready\n\n\n\n\n\n\nredis\n\n\nready\n\n\nready\n\n\n\n\n\n\nworker\n\n\nTODO\n\n\nn/a\n\n\n\n\n\n\ndb\n\n\nready\n\n\nready\n\n\n\n\n\n\nresults\n\n\nTODO\n\n\nTODO\n\n\n\n\n\n\n\n\nSpecs:\n\n\n\n\nworker\n\n\nimage: schoolofdevops/worker:latest\n\n\n\n\n\n\nresults\n\n\nimage: schoolofdevops/vote-result\n\n\nport: 80\n\n\nservice type: NodePort\n\n\n\n\n\n\n\n\nTo Validate:\n\n\nkubectl get svc -n instavote\n\n\n\n\nSample Output is:\n\n\nkubectl get service vote\nNAME         CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nvote   10.97.104.243   <pending>     80:30000/TCP   1h\n\n\n\n\nHere the port assigned is 31808, go to the browser and enter\n\n\nmasterip:30000\n\n\n\n\n\n\nThis will load the page where you can vote.\n\n\nTo check the result:\n\n\nkubectl get service result\n\n\n\n\nSample Output is:\n\n\nkubectl get service result\nNAME      CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nresult    10.101.112.16   <pending>     80:30100/TCP   1h\n\n\n\n\nHere the port assigned is 32511, go to the browser and enter\n\n\nmasterip:30100\n\n\n\n\n\n\nThis is the page where you should see the results for the vote application stack.",
            "title": "Mini Project"
        },
        {
            "location": "/11_deploying_sample_app/#mini-project-deploying-multi-tier-application-stack",
            "text": "In this project , you would write definitions for deploying the vote application stack with all components/tiers which include,   vote ui  redis  worker  db  results ui",
            "title": "Mini Project: Deploying Multi Tier Application Stack"
        },
        {
            "location": "/11_deploying_sample_app/#tasks",
            "text": "Create deployments for all applications  Define services for each tier applicable  Launch/apply the definitions   Following table depicts the state of readiness of the above services.     App  Deployment  Service      vote  ready  ready    redis  ready  ready    worker  TODO  n/a    db  ready  ready    results  TODO  TODO     Specs:   worker  image: schoolofdevops/worker:latest    results  image: schoolofdevops/vote-result  port: 80  service type: NodePort     To Validate:  kubectl get svc -n instavote  Sample Output is:  kubectl get service vote\nNAME         CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nvote   10.97.104.243   <pending>     80:30000/TCP   1h  Here the port assigned is 31808, go to the browser and enter  masterip:30000   This will load the page where you can vote.  To check the result:  kubectl get service result  Sample Output is:  kubectl get service result\nNAME      CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nresult    10.101.112.16   <pending>     80:30100/TCP   1h  Here the port assigned is 32511, go to the browser and enter  masterip:30100   This is the page where you should see the results for the vote application stack.",
            "title": "Tasks"
        },
        {
            "location": "/9-vote-configmaps_and_secrets/",
            "text": "Configurations Management with ConfigMaps\n\n\nConfigmap is one of the ways to provide configurations to your application.\n\n\nInjecting env variables with configmaps\n\n\nCreate our configmap for vote app\n\n\nfile:  projects/instavote/dev/vote-cm.yaml\n\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vote\n  namespace: instavote\ndata:\n  OPTION_A: Visa\n  OPTION_B: Mastercard\n\n\n\n\nIn the above given configmap, we define two environment variables,\n\n\n\n\nOPTION_A=EMACS\n\n\nOPTION_B=VI\n\n\n\n\nLets create the configmap object\n\n\nkubectl get cm\nkubectl apply -f vote-cm.yaml\nkubectl get cm\nkubectl describe cm vote\n\n\n\n\n\n\nIn order to use this configmap in the deployment, we need to reference it from the deployment file.\n\n\nCheck the deployment file for vote add for the following block.\n\n\nfile: \nvote-deploy.yaml\n\n\n...\n    spec:\n      containers:\n      - image: schoolofdevops/vote\n        imagePullPolicy: Always\n        name: vote\n        envFrom:\n          - configMapRef:\n              name: vote\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        restartPolicy: Always\n\n\n\n\nSo when you create your deployment, these configurations will be made available to your application. In this example, the values defined in the configmap (Visa and Mastercard) will override the default values(CATS and DOGS) present in your source code.\n\n\nkubectl apply -f vote-deploy.yaml\n\n\n\n\nWatch the monitoring screen for deployment in progress.\n\n\nkubectl get deploy --show-labels\nkubectl get rs --show-labels\nkubectl  rollout status deploy/vote\n\n\n\n\n\n\n\nNote: Automatic Updation of deployments on ConfigMap  Updates\n\n\nCurrently, updating configMap does not ensure a new rollout of a deployment. What this means is even after updading configMaps, pods will not immediately reflect the changes.  \n\n\nThere is a feature request for this https://github.com/kubernetes/kubernetes/issues/22368\n\n\nCurrently, this can be done by using immutable configMaps.  \n\n\n\n\nCreate a configMaps and apply it with deployment.\n\n\nTo update, create a new configMaps and do not update the previous one. Treat it as immutable.\n\n\nUpdate deployment spec to use the new version of the configMaps. This will ensure immediate update.",
            "title": "Lab K107 - Configurations Management with ConfigMaps"
        },
        {
            "location": "/9-vote-configmaps_and_secrets/#configurations-management-with-configmaps",
            "text": "Configmap is one of the ways to provide configurations to your application.  Injecting env variables with configmaps  Create our configmap for vote app  file:  projects/instavote/dev/vote-cm.yaml  apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: vote\n  namespace: instavote\ndata:\n  OPTION_A: Visa\n  OPTION_B: Mastercard  In the above given configmap, we define two environment variables,   OPTION_A=EMACS  OPTION_B=VI   Lets create the configmap object  kubectl get cm\nkubectl apply -f vote-cm.yaml\nkubectl get cm\nkubectl describe cm vote  In order to use this configmap in the deployment, we need to reference it from the deployment file.  Check the deployment file for vote add for the following block.  file:  vote-deploy.yaml  ...\n    spec:\n      containers:\n      - image: schoolofdevops/vote\n        imagePullPolicy: Always\n        name: vote\n        envFrom:\n          - configMapRef:\n              name: vote\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        restartPolicy: Always  So when you create your deployment, these configurations will be made available to your application. In this example, the values defined in the configmap (Visa and Mastercard) will override the default values(CATS and DOGS) present in your source code.  kubectl apply -f vote-deploy.yaml  Watch the monitoring screen for deployment in progress.  kubectl get deploy --show-labels\nkubectl get rs --show-labels\nkubectl  rollout status deploy/vote   Note: Automatic Updation of deployments on ConfigMap  Updates  Currently, updating configMap does not ensure a new rollout of a deployment. What this means is even after updading configMaps, pods will not immediately reflect the changes.    There is a feature request for this https://github.com/kubernetes/kubernetes/issues/22368  Currently, this can be done by using immutable configMaps.     Create a configMaps and apply it with deployment.  To update, create a new configMaps and do not update the previous one. Treat it as immutable.  Update deployment spec to use the new version of the configMaps. This will ensure immediate update.",
            "title": "Configurations Management with ConfigMaps"
        },
        {
            "location": "/vote-persistent-volumes/",
            "text": "Steps to set up NFS based Persistent Volumes\n\n\nCreating a Persistent Volume Claim\n\n\nswitch to project directory\n\n\ncd k8s-code/projects/instavote/dev/\n\n\n\n\nCreate the following file with the specs below\n\n\nfile: db-pvc.yaml\n\n\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: db-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 2Gi\n  storageClassName: nfs\n\n\n\n\n\ncreate the Persistent Volume Claim and validate\n\n\nkubectl get pvc\n\n\nkubectl apply -f db-pvc.yaml\n\nkubectl get pvc,pv\n\n\n\n\n\nNow, to use this PVC, db deployment needs to be updated with \nvolume\n and \nvolumeMounts\n configs as given in example below.\n\n\nfile: db-deploy-pvc.yaml\n\n\n...\nspec:\n   containers:\n   - image: postgres:9.4\n     imagePullPolicy: Always\n     name: db\n     ports:\n     - containerPort: 5432\n       protocol: TCP\n     #mount db-vol to postgres data path\n     volumeMounts:\n     - name: db-vol\n       mountPath: /var/lib/postgresql/data\n   #create a volume with pvc\n   volumes:\n   - name: db-vol\n     persistentVolumeClaim:\n       claimName: db-pvc\n\n\n\n\nApply \ndb-deploy-pcv.yaml\n  as\n\n\nkubectl apply -f db-deploy-pvc.yaml\n\nkubectl get pod -o wide --selector='role=db'\n\nkubectl get pvc,pv\n\n\n\n\n\n\nObserve and note which host the pod for \ndb\n is launched.\n\n\nWhat state is it in ? why?\n\n\nHas the persistentVolumeClaim been bound to a persistentVolume ? Why?\n\n\n\n\nSet up NFS Provisioner in kubernetes\n\n\nChange into nfs provisioner installation dir\n\n\ncd k8s-code/storage\n\n\n\n\nDeploy nfs-client provisioner.\n\n\nkubectl apply -f nfs\n\n\n\n\n\nThis will create all the objects required to setup a nfs provisioner. It would be launched with  Statefulsets. \nRead the official documentation on Statefulsets\n to understand how its differnt than deployments.\n\n\nkubectl get storageclass\nkubectl get pods\nkubectl logs -f nfs-provisioner-0\n\n\n\n\n\nNow, observe the output of  the following commands,\n\n\nkubectl get pvc,pv\nkubectl get pods\n\n\n\n\n\n\nDo you see pvc bound to pv ?\n\n\nDo you see the pod for db running ?\n\n\n\n\nObserve the dynamic provisioning, go to the host which is running nfs provisioner and look inside \n/srv\n path to find the provisioned volume.\n\n\nSummary\n\n\nIn this lab, you not only setup dynamic provisioning using NFS, but also learnt about statefulsets as well as rbac policies applied to the nfs provisioner.",
            "title": "Lab K108 - Peristent Volumes"
        },
        {
            "location": "/vote-persistent-volumes/#steps-to-set-up-nfs-based-persistent-volumes",
            "text": "Creating a Persistent Volume Claim  switch to project directory  cd k8s-code/projects/instavote/dev/  Create the following file with the specs below  file: db-pvc.yaml  kind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: db-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  volumeMode: Filesystem\n  resources:\n    requests:\n      storage: 2Gi\n  storageClassName: nfs  create the Persistent Volume Claim and validate  kubectl get pvc\n\n\nkubectl apply -f db-pvc.yaml\n\nkubectl get pvc,pv  Now, to use this PVC, db deployment needs to be updated with  volume  and  volumeMounts  configs as given in example below.  file: db-deploy-pvc.yaml  ...\nspec:\n   containers:\n   - image: postgres:9.4\n     imagePullPolicy: Always\n     name: db\n     ports:\n     - containerPort: 5432\n       protocol: TCP\n     #mount db-vol to postgres data path\n     volumeMounts:\n     - name: db-vol\n       mountPath: /var/lib/postgresql/data\n   #create a volume with pvc\n   volumes:\n   - name: db-vol\n     persistentVolumeClaim:\n       claimName: db-pvc  Apply  db-deploy-pcv.yaml   as  kubectl apply -f db-deploy-pvc.yaml\n\nkubectl get pod -o wide --selector='role=db'\n\nkubectl get pvc,pv   Observe and note which host the pod for  db  is launched.  What state is it in ? why?  Has the persistentVolumeClaim been bound to a persistentVolume ? Why?",
            "title": "Steps to set up NFS based Persistent Volumes"
        },
        {
            "location": "/vote-persistent-volumes/#set-up-nfs-provisioner-in-kubernetes",
            "text": "Change into nfs provisioner installation dir  cd k8s-code/storage  Deploy nfs-client provisioner.  kubectl apply -f nfs  This will create all the objects required to setup a nfs provisioner. It would be launched with  Statefulsets.  Read the official documentation on Statefulsets  to understand how its differnt than deployments.  kubectl get storageclass\nkubectl get pods\nkubectl logs -f nfs-provisioner-0  Now, observe the output of  the following commands,  kubectl get pvc,pv\nkubectl get pods   Do you see pvc bound to pv ?  Do you see the pod for db running ?   Observe the dynamic provisioning, go to the host which is running nfs provisioner and look inside  /srv  path to find the provisioned volume.  Summary  In this lab, you not only setup dynamic provisioning using NFS, but also learnt about statefulsets as well as rbac policies applied to the nfs provisioner.",
            "title": "Set up NFS Provisioner in kubernetes"
        },
        {
            "location": "/ingress/",
            "text": "Lab K201 - Application Routing with Ingress Controllers\n\n\nPre Requisites\n\n\n\n\nIngress controller such as Nginx, Trafeik needs to be deployed before creating ingress resources.\n\n\nOn GCE, ingress controller runs on the master. On all other installations, it needs to be deployed, either as a deployment, or a daemonset. In addition, a service needs to be created for ingress.\n\n\nDaemonset will run ingress on each node. Deployment will just create a highly available setup, which can then be exposed on specific nodes using ExternalIPs configuration in the service.\n\n\n\n\nCreate a Ingress Controller\n\n\nAn ingress controller needs to be created in order to serve the ingress requests. Kubernetes comes with support for \nGCE\n and \nnginx\n ingress controllers, however additional softwares are commonly used too.  As part of this implementation you are going to use \nTraefik\n as the ingress controller. Its a fast and lightweight ingress controller and also comes with great documentation and support.  \n\n\n\n\n+----+----+--+            \n| ingress    |            \n| controller |            \n+----+-------+            \n\n\n\n\n\n\nThere are commonly two ways you could deploy an ingress\n\n\n\n\nUsing Deployments with HA setup\n\n\nUsing DaemonSets which run on every node\n\n\n\n\nWe pick \nDaemonSet\n, which will ensure that one instance of \ntraefik\n is run on every node.  Also, we use a specific configuration \nhostNetwork\n so that the pod running \ntraefik\n attaches to the network of underlying host, and not go through \nkube-proxy\n. This would avoid extra network hop and increase performance  a bit.  \n\n\nYou could refer to \nofficial traefik docs\n to understand the installation options and how to use those.\n\n\nDeploy ingress controller with daemonset as\n\n\n\ncd k8s-code/ingress/traefik\nkubectl apply -f traefik-rbac.yaml\nkubectl apply -f traefik-ds.yaml\n\n\n\n\nValidate\n\n\nkubectl get svc,ds,pods -n kube-system  --selector='k8s-app=traefik-ingress-lb'\n\n\n\n\n[output]\n\n\nNAME                              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)           AGE\nservice/traefik-ingress-service   ClusterIP   10.109.182.203   <none>        80/TCP,8080/TCP   11h\n\nNAME                                              DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\ndaemonset.extensions/traefik-ingress-controller   2         2         2         2            2           <none>          11h\n\nNAME                                   READY     STATUS    RESTARTS   AGE\npod/traefik-ingress-controller-bmwn7   1/1       Running   0          11h\npod/traefik-ingress-controller-vl296   1/1       Running   0          11h\n\n\n\n\nYou would notice that the ingress controller is started on all nodes (except managers). Visit any of the nodes 8080 port e.g. http://IPADDRESS:8080 to see  traefik's management UI.\n\n\n\n\nSetting up Named Based Routing for Vote App\n\n\nWe will direct all our request to the ingress controller now, but with differnt hostname e.g. \nvote.example.com\n or \nresults.example.com\n. And it should direct to the correct service based on the host name.\n\n\nIn order to achieve this you, as a user would create a \ningress\n object with a set of rules,\n\n\n\n\n+----+----+--+            \n| ingress    |            \n| controller |            \n+----+-------+            \n     |              +-----+----+\n     +---watch----> | ingress  | <------- user\n                    +----------+\n\n\n\n\n\nfile: vote-ing.yaml\n\n\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: vote\n  annotations:\n    kubernetes.io/ingress.class: traefik\nspec:\n  rules:\n    - host: vote.example.com\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: vote\n              servicePort: 80\n    - host: results.example.com\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: results\n              servicePort: 80\n\n\n\n\nAnd apply\n\n\nkubectl get ing\nkubectl apply -f vote-ing.yaml --dry-run\nkubectl apply -f vote-ing.yaml\n\n\n\n\nSince the ingress controller  is constantly monitoring for the ingress objects, the moment it detects, it connects with traefik and creates a rule as follows.\n\n\n\n                    +----------+\n     +--create----> | traefik  |\n     |              |  rules   |\n     |              +----------+\n+----+----+--+            ^\n| ingress    |            :\n| controller |            :\n+----+-------+            :\n     |              +-----+----+\n     +---watch----> | ingress  | <------- user\n                    +----------+\n\n\n\n\n\nwhere,\n\n\n\n\nA user creates a ingress object with the rules. This could be a named based or a path based routing.\n\n\nAn ingress controller, in this example traefik constantly monitors for ingress objects. The moment it detects one, it creates a rule and adds it to the traefik load balancer. This rule maps to the ingress specs.\n\n\n\n\nYou could now see the rule added to ingress controller,\n\n\n\n\nWhere,\n\n\n\n\n\n\nvote.example.com\n and \nresults.example.com\n are added as frontends. These frontends point to respective services \nvote\n and \nresults\n.\n\n\n\n\n\n\nrespective backends also appear on the right hand side of the screen, mapping to each of the service.\n\n\n\n\n\n\nAdding Local DNS\n\n\nYou have created the ingress rules based on hostnames e.g.  \nvote.example.com\n and \nresults.example.com\n. In order for you to be able to access those, there has to be a dns entry pointing to your nodes, which are running traefik.\n\n\n\n  vote.example.com     -------+                        +----- vote:81\n                              |     +-------------+    |\n                              |     |   ingress   |    |\n                              +===> |   node:80   | ===+\n                              |     +-------------+    |\n                              |                        |\n  results.example.com  -------+                        +----- results:82\n\n\n\n\n\nTo achieve this you need to either,\n\n\n\n\nCreate a DNS entry, provided you own the domain and have access to the dns management console.\n\n\nCreate a local \nhosts\n file entry. On unix systems its in \n/etc/hosts\n file. On windows its at \nC:\\Windows\\System32\\drivers\\etc\\hosts\n. You need admin access to edit this file.\n\n\n\n\nFor example, on a linux or osx, you could edit it as,\n\n\nsudo vim /etc/hosts\n\n\n\n\nAnd add an entry such as ,\n\n\nxxx.xxx.xxx.xxx vote.example.com results.example.com\n\n\n\n\nwhere,\n\n\n\n\nxxx.xxx.xxx.xxx is the actual IP address of one of the nodes running traefik.\n\n\n\n\nAnd then access the app urls using http://vote.example.com or http://results.example.com\n\n\n\n\nAdding HTTP Authentication with Annotations\n\n\nCreating htpasswd spec as Secret\n\n\napt install -yq apache2-utils\nhtpasswd -c auth devops\n\n\n\n\nOr use \nOnline htpasswd generator\n to generate a htpasswd spec. if you use the online generator, copy the contents to a file by name \nauth\n in the current directory.\n\n\nThen generate the secret as,\n\n\nkubectl create secret generic mysecret --from-file auth\n\nkubectl get secret\n\nkubectl describe secret mysecret\n\n\n\n\nAnd then add annotations to the ingress object so that it is read by the ingress controller to update configurations.\n\n\nfile: vote-ing.yaml\n\n\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: vote\n  annotations:\n    kubernetes.io/ingress.class: traefik\n    ingress.kubernetes.io/auth-type: \"basic\"\n    ingress.kubernetes.io/auth-secret: \"mysecret\"\nspec:\n  rules:\n    - host: vote.example.com\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: vote\n              servicePort: 82\n    - host: results.example.com\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: results\n              servicePort: 81\n\n\n\n\n\nwhere,\n\n\n\n\ningress.kubernetes.io/auth-type: \"basic\"\n defines authentication type that needs to be added.\n\n\ningress.kubernetes.io/auth-secret: \"mysecret\"\n refers to the secret created earlier.\n\n\n\n\napply\n\n\nkubectl apply -f vote-ing.yaml\nkubectl get ing/vote -o yaml\n\n\n\n\nObserve the annotations field. No sooner than you apply this spec, ingress controller reads the event and a basic http authentication is set with the secret you added.\n\n\n\n                      +----------+\n       +--update----> | traefik  |\n       |              |  configs |\n       |              +----------+\n  +----+----+--+            ^\n  | ingress    |            :\n  | controller |            :\n  +----+-------+            :\n       |              +-----+-------+\n       +---watch----> | ingress     | <------- user\n                      | annotations |\n                      +-------------+\n\n\n\n\nAnd if you visit traefik's dashboard and go to the details tab, you should see the basic authentication section enabled as in the diagram below.\n\n\n\n\nReading\n\n\n\n\nTrafeik's Guide to Kubernetes Ingress Controller\n\n\nAnnotations\n\n\nDaemonSets\n\n\n\n\nReferences\n\n\n\n\nOnline htpasswd generator\n\n\n\n\nKeywords\n\n\n\n\ntrafeik on kubernetes\n\n\nkubernetes ingress\n\n\nkubernetes annotations\n\n\ndaemonsets",
            "title": "Lab K201 - Application Routing with Ingress Controllers"
        },
        {
            "location": "/ingress/#lab-k201-application-routing-with-ingress-controllers",
            "text": "",
            "title": "Lab K201 - Application Routing with Ingress Controllers"
        },
        {
            "location": "/ingress/#pre-requisites",
            "text": "Ingress controller such as Nginx, Trafeik needs to be deployed before creating ingress resources.  On GCE, ingress controller runs on the master. On all other installations, it needs to be deployed, either as a deployment, or a daemonset. In addition, a service needs to be created for ingress.  Daemonset will run ingress on each node. Deployment will just create a highly available setup, which can then be exposed on specific nodes using ExternalIPs configuration in the service.   Create a Ingress Controller  An ingress controller needs to be created in order to serve the ingress requests. Kubernetes comes with support for  GCE  and  nginx  ingress controllers, however additional softwares are commonly used too.  As part of this implementation you are going to use  Traefik  as the ingress controller. Its a fast and lightweight ingress controller and also comes with great documentation and support.    \n\n+----+----+--+            \n| ingress    |            \n| controller |            \n+----+-------+              There are commonly two ways you could deploy an ingress   Using Deployments with HA setup  Using DaemonSets which run on every node   We pick  DaemonSet , which will ensure that one instance of  traefik  is run on every node.  Also, we use a specific configuration  hostNetwork  so that the pod running  traefik  attaches to the network of underlying host, and not go through  kube-proxy . This would avoid extra network hop and increase performance  a bit.    You could refer to  official traefik docs  to understand the installation options and how to use those.  Deploy ingress controller with daemonset as  \ncd k8s-code/ingress/traefik\nkubectl apply -f traefik-rbac.yaml\nkubectl apply -f traefik-ds.yaml  Validate  kubectl get svc,ds,pods -n kube-system  --selector='k8s-app=traefik-ingress-lb'  [output]  NAME                              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)           AGE\nservice/traefik-ingress-service   ClusterIP   10.109.182.203   <none>        80/TCP,8080/TCP   11h\n\nNAME                                              DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\ndaemonset.extensions/traefik-ingress-controller   2         2         2         2            2           <none>          11h\n\nNAME                                   READY     STATUS    RESTARTS   AGE\npod/traefik-ingress-controller-bmwn7   1/1       Running   0          11h\npod/traefik-ingress-controller-vl296   1/1       Running   0          11h  You would notice that the ingress controller is started on all nodes (except managers). Visit any of the nodes 8080 port e.g. http://IPADDRESS:8080 to see  traefik's management UI.   Setting up Named Based Routing for Vote App  We will direct all our request to the ingress controller now, but with differnt hostname e.g.  vote.example.com  or  results.example.com . And it should direct to the correct service based on the host name.  In order to achieve this you, as a user would create a  ingress  object with a set of rules,  \n\n+----+----+--+            \n| ingress    |            \n| controller |            \n+----+-------+            \n     |              +-----+----+\n     +---watch----> | ingress  | <------- user\n                    +----------+  file: vote-ing.yaml  apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: vote\n  annotations:\n    kubernetes.io/ingress.class: traefik\nspec:\n  rules:\n    - host: vote.example.com\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: vote\n              servicePort: 80\n    - host: results.example.com\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: results\n              servicePort: 80  And apply  kubectl get ing\nkubectl apply -f vote-ing.yaml --dry-run\nkubectl apply -f vote-ing.yaml  Since the ingress controller  is constantly monitoring for the ingress objects, the moment it detects, it connects with traefik and creates a rule as follows.  \n                    +----------+\n     +--create----> | traefik  |\n     |              |  rules   |\n     |              +----------+\n+----+----+--+            ^\n| ingress    |            :\n| controller |            :\n+----+-------+            :\n     |              +-----+----+\n     +---watch----> | ingress  | <------- user\n                    +----------+  where,   A user creates a ingress object with the rules. This could be a named based or a path based routing.  An ingress controller, in this example traefik constantly monitors for ingress objects. The moment it detects one, it creates a rule and adds it to the traefik load balancer. This rule maps to the ingress specs.   You could now see the rule added to ingress controller,   Where,    vote.example.com  and  results.example.com  are added as frontends. These frontends point to respective services  vote  and  results .    respective backends also appear on the right hand side of the screen, mapping to each of the service.    Adding Local DNS  You have created the ingress rules based on hostnames e.g.   vote.example.com  and  results.example.com . In order for you to be able to access those, there has to be a dns entry pointing to your nodes, which are running traefik.  \n  vote.example.com     -------+                        +----- vote:81\n                              |     +-------------+    |\n                              |     |   ingress   |    |\n                              +===> |   node:80   | ===+\n                              |     +-------------+    |\n                              |                        |\n  results.example.com  -------+                        +----- results:82  To achieve this you need to either,   Create a DNS entry, provided you own the domain and have access to the dns management console.  Create a local  hosts  file entry. On unix systems its in  /etc/hosts  file. On windows its at  C:\\Windows\\System32\\drivers\\etc\\hosts . You need admin access to edit this file.   For example, on a linux or osx, you could edit it as,  sudo vim /etc/hosts  And add an entry such as ,  xxx.xxx.xxx.xxx vote.example.com results.example.com  where,   xxx.xxx.xxx.xxx is the actual IP address of one of the nodes running traefik.   And then access the app urls using http://vote.example.com or http://results.example.com   Adding HTTP Authentication with Annotations  Creating htpasswd spec as Secret  apt install -yq apache2-utils\nhtpasswd -c auth devops  Or use  Online htpasswd generator  to generate a htpasswd spec. if you use the online generator, copy the contents to a file by name  auth  in the current directory.  Then generate the secret as,  kubectl create secret generic mysecret --from-file auth\n\nkubectl get secret\n\nkubectl describe secret mysecret  And then add annotations to the ingress object so that it is read by the ingress controller to update configurations.  file: vote-ing.yaml  apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: vote\n  annotations:\n    kubernetes.io/ingress.class: traefik\n    ingress.kubernetes.io/auth-type: \"basic\"\n    ingress.kubernetes.io/auth-secret: \"mysecret\"\nspec:\n  rules:\n    - host: vote.example.com\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: vote\n              servicePort: 82\n    - host: results.example.com\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: results\n              servicePort: 81  where,   ingress.kubernetes.io/auth-type: \"basic\"  defines authentication type that needs to be added.  ingress.kubernetes.io/auth-secret: \"mysecret\"  refers to the secret created earlier.   apply  kubectl apply -f vote-ing.yaml\nkubectl get ing/vote -o yaml  Observe the annotations field. No sooner than you apply this spec, ingress controller reads the event and a basic http authentication is set with the secret you added.  \n                      +----------+\n       +--update----> | traefik  |\n       |              |  configs |\n       |              +----------+\n  +----+----+--+            ^\n  | ingress    |            :\n  | controller |            :\n  +----+-------+            :\n       |              +-----+-------+\n       +---watch----> | ingress     | <------- user\n                      | annotations |\n                      +-------------+  And if you visit traefik's dashboard and go to the details tab, you should see the basic authentication section enabled as in the diagram below.   Reading   Trafeik's Guide to Kubernetes Ingress Controller  Annotations  DaemonSets   References   Online htpasswd generator   Keywords   trafeik on kubernetes  kubernetes ingress  kubernetes annotations  daemonsets",
            "title": "Pre Requisites"
        },
        {
            "location": "/configuring_authentication_and_authorization/",
            "text": "Lab K202 - Kubernetes Access Control:  Authentication and Authorization\n\n\nIn  this lab you are going to,\n\n\n\n\nCreate users and groups and setup certs based authentication\n\n\nCreate service accounts for applications\n\n\nCreate Roles and ClusterRoles to define authorizations\n\n\nMap Roles and ClusterRoles to subjects i.e. users, groups and service accounts using RoleBingings and ClusterRoleBindings.\n\n\n\n\nHow one can access the Kubernetes API?\n\n\nThe Kubernetes API can be accessed by three ways.\n\n\n\n\nKubectl - A command line utility of Kubernetes\n\n\nClient libraries - Go, Python, etc.,\n\n\nREST requests\n\n\n\n\nWho can access the Kubernetes API?\n\n\nKubernetes API can be accessed by,\n\n\n\n\nHuman Users\n\n\nService Accounts  \n\n\n\n\nEach of these topics will be discussed in detail in the later part of this chapter.\n\n\nStages of a Request\n\n\nWhen a request tries to contact the API , it goes through various stages as illustrated in the image given below.\n\n\n\n  \nsource: official kubernetes site\n\n\napi groups and resources\n\n\n\n\n\n\n\n\napiGroup\n\n\nResources\n\n\n\n\n\n\n\n\n\n\napps\n\n\ndaemonsets, deployments, deployments/rollback, deployments/scale, replicasets, replicasets/scale, statefulsets, statefulsets/scale\n\n\n\n\n\n\ncore\n\n\nconfigmaps, endpoints, persistentvolumeclaims, replicationcontrollers, replicationcontrollers/scale, secrets, serviceaccounts, services,services/proxy\n\n\n\n\n\n\nautoscaling\n\n\nhorizontalpodautoscalers\n\n\n\n\n\n\nbatch\n\n\ncronjobs, jobs\n\n\n\n\n\n\npolicy\n\n\npoddisruptionbudgets\n\n\n\n\n\n\nnetworking.k8s.io\n\n\nnetworkpolicies\n\n\n\n\n\n\nauthorization.k8s.io\n\n\nlocalsubjectaccessreviews\n\n\n\n\n\n\nrbac.authorization.k8s.io\n\n\nrolebindings,roles\n\n\n\n\n\n\nextensions\n\n\ndeprecated (read notes)\n\n\n\n\n\n\n\n\nNotes\n\n\nIn addition to the above apiGroups, you may see \nextensions\n being used in some example code snippets. Please note that \nextensions\n was initially created as a experiement and is been deprecated, by moving most of the matured apis to one of the groups mentioned above.  \nYou could read this comment and the thread\n to get clarity on this.  \n\n\nRole Based Access Control (RBAC)\n\n\n\n\n\n\n\n\nGroup\n\n\nUser\n\n\nNamespaces\n\n\nResources\n\n\nAccess Type (verbs)\n\n\n\n\n\n\n\n\n\n\nops\n\n\nmaya\n\n\nall\n\n\nall\n\n\nget, list, watch, update, patch, create, delete, deletecollection\n\n\n\n\n\n\ndev\n\n\nkim\n\n\ninstavote\n\n\ndeployments, statefulsets, services, pods, configmaps, secrets, replicasets, ingresses, endpoints, cronjobs, jobs, persistentvolumeclaims\n\n\nget, list , watch, update, patch, create\n\n\n\n\n\n\ninterns\n\n\nyono\n\n\ninstavote\n\n\nreadonly\n\n\nget, list, watch\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nService Accounts\n\n\nNamespace\n\n\nResources\n\n\nAccess Type (verbs)\n\n\n\n\n\n\n\n\n\n\nmonitoring\n\n\nall\n\n\nall\n\n\nreadonly\n\n\n\n\n\n\n\n\nCreating Kubernetes Users and Groups\n\n\nGenerate the user's private key\n\n\nmkdir -p  ~/.kube/users\ncd ~/.kube/users\n\nopenssl genrsa -out kim.key 2048\n\n\n\n\n\n[sample Output]\n\n\nopenssl genrsa -out kim.key 2048\nGenerating RSA private key, 2048 bit long modulus\n.............................................................+++\n.........................+++\ne is 65537 (0x10001)\n\n\n\n\n\nLets now create a \nCertification Signing Request (CSR)\n for each of the users. When you generate the csr make sure you also provide\n\n\n\n\nCN: This will be set as username\n\n\nO: Org name. This is actually used as a \ngroup\n by kubernetes while authenticating/authorizing users.  You could add as many as you need\n\n\n\n\ne.g.\n\n\nopenssl req -new -key kim.key -out kim.csr -subj \"/CN=kim/O=dev/O=example.org\"\n\n\n\n\n\nIn order to be deemed authentic, these CSRs need to be signed by the \nCertification Authority (CA)\n which in this case is Kubernetes Master.   You need access to the folllwing files on  kubernetes master.\n\n\n\n\nCertificate : ca.crt (kubeadm) or ca.key (kubespray)\n\n\nPricate Key : ca.key (kubeadm) or ca-key.pem  (kubespray)\n\n\n\n\nYou would typically find it  the following paths\n\n\n\n\n/etc/kubernetes/pki\n\n\n\n\nTo verify which one is your cert and which one is key, use the following command,\n\n\n$ file /etc/kubernetes/pki/ca.crt\nca.pem: PEM certificate\n\n\n$ file /etc/kubernetes/pki/ca.key\nca-key.pem: PEM RSA private key\n\n\n\n\nOnce signed, .csr files with added signatures become the certificates that could be used to authenticate.\n\n\nYou could either\n\n\n\n\nmove the crt files to k8s master, sign and download  \n\n\ncopy over the CA certs and keys to your management node and use it to sign. Make sure to keep your CA related files secure.\n\n\n\n\nIn the example here, I have already downloaded \nca.pem\n and \nca-key.pem\n to my management workstation, which are used to sign the CSRs.  \n\n\nAssuming all the files are in the same directory, sign the CSR as,\n\n\nopenssl x509 -req -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -days 730 -in kim.csr -out kim.crt\n\n\n\n\n\nSetting up User configs with kubectl\n\n\nIn order to configure the users that you created above, following steps need to be performed with kubectl\n\n\n\n\nAdd credentials in the configurations\n\n\nSet context to login as a user to a cluster\n\n\nSwitch context in order to assume the user's identity while working with the cluster\n\n\n\n\nto add credentials,\n\n\nkubectl config set-credentials kim --client-certificate=/root/.kube/users/kim.crt --client-key=/root/.kube/users/kim.key\n\n\n\n\nwhere,\n\n\n\n\n/root/.kube/users/kim.crt : Absolute path to the users' certificate\n\n\n/root/.kube/users/kim.key: Absolute path to the users' key\n\n\n\n\nAnd proceed to set/create  contexts (user@cluster). If you are not sure whats the cluster name, use the following command to find,\n\n\nkubectl config get-contexts\n\n\n\n\n\n[sample output]\n\n\nCURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE\n*         kubernetes-admin@kubernetes   kubernetes   kubernetes-admin   instavote\n\n\n\n\nwhere,  \nkubernetes\n is the  cluster name.\n\n\nTo set context for \nkubernetes\n cluster,\n\n\nkubectl config set-context kim-kubernetes --cluster=kubernetes  --user=kim --namespace=instavote\n\n\n\n\n\nWhere,\n\n\n\n\nkim-kubernetes : name of the context  \n\n\nkubernetes  : name of the  cluster you set while creating it  \n\n\nkim : user you created and configured above to connect to the cluster  \n\n\n\n\nYou could verify the configs with\n\n\nkubectl config get-contexts\n\nCURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE\n          kim-kubernetes                kubernetes   kim                instavote\n*         kubernetes-admin@kubernetes   kubernetes   kubernetes-admin   instavote\n\n\n\n\nand\n\n\nkubectl config view\n\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: DATA+OMITTED\n    server: https://178.128.109.8:6443\n  name: kubernetes\ncontexts:\n- context:\n    cluster: kubernetes\n    namespace: instavote\n    user: kim\n  name: kim-kubernetes\n- context:\n    cluster: kubernetes\n    namespace: instavote\n    user: kubernetes-admin\n  name: kubernetes-admin@kubernetes\ncurrent-context: kubernetes-admin@kubernetes\nkind: Config\npreferences: {}\nusers:\n- name: kim\n  user:\n    client-certificate: users/kim.crt\n    client-key: users/kim.key\n- name: kubernetes-admin\n  user:\n    client-certificate-data: REDACTED\n    client-key-data: REDACTED\n\n\n\n\nWhere, you should see the configurations for the new user you created have been added.\n\n\nYou could assume the identity of user \nkim\n and connect  to the \nkubernetes\n cluster as,\n\n\nkubectl config use-context kim-kubernetes\n\nkubectl config get-contexts\n\nCURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE\n*         kim-kubernetes                kubernetes   kim                instavote\n          kubernetes-admin@kubernetes   kubernetes   kubernetes-admin   instavote\n\n\n\n\nThis time * appears on the line which lists context \nkim-kubernetes\n that you just created.\n\n\nAnd then try running any command as,\n\n\nkubectl get pods\n\n\n\n\nAlternately, if you are a admin user, you could impersonate a user and run a command with that literally using  --as option\n\n\nkubectl config use-context admin-prod\nkubectl get pods --as yono\n\n\n\n\n[Sample Output]\n\n\nNo resources found.\nError from server (Forbidden): pods is forbidden: User \"yono\" cannot list pods in the namespace \"instavote\"\n\n\n\n\n\nEither ways, since there are authorization rules set, the user can not make any api calls. Thats when you would create some roles and bind it to the users in the next section.\n\n\nDefine authorisation rules with Roles and ClusterRoles\n\n\nWhats the difference between Roles and ClusterRoles ??\n\n\n\n\nRole is  limited to a namespace (Projects/Orgs/Env)\n\n\nClusterRole is Global\n\n\n\n\nLets say you want to provide read only access to \ninstavote\n, a project specific namespace to all users in the \nexample.org\n\n\nkubectl config use-context kubernetes-admin@kubernetes\ncd projects/instavote/dev\n\n\n\n\nfile: readonly-role.yaml\n\n\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: Role\nmetadata:\n  namespace: instavote\n  name: readonly\nrules:\n- apiGroups: [\"*\"]\n  resources: [\"*\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n\n\n\n\nIn order to map it to all users in \nexample.org\n, create a RoleBinding as\n\n\nfile: readonly-rolebinding.yml\n\n\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: readonly\n  namespace: instavote\nsubjects:\n- kind: Group\n  name: dev\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: readonly\n  apiGroup: rbac.authorization.k8s.io\n\n\n\n\nkubectl apply -f readonly-role.yaml\nkubectl apply -f readonly-rolebinding.yml\n\n\n\n\nTo get information about the objects created above,\n\n\nkubectl get roles,rolebindings -n instavote\n\nkubectl describe role readonly\nkubectl describe rolebinding readonly\n\n\n\n\n\nTo validate the access,\n\n\nkubectl config get-contexts\nkubectl config use-context kim-kubernetes\nkubectl get pods\n\n\n\n\n\nTo switch back to admin,\n\n\nkubectl config use-context kubernetes-admin@kubernetes\n\n\n\n\n\nExercise\n\n\nCreate a Role and Rolebinding for \ndev\n group with the authorizations defined in the table above. Once applied, test it",
            "title": "Lab K202 - Authentication and Authorization (RBAC)"
        },
        {
            "location": "/configuring_authentication_and_authorization/#lab-k202-kubernetes-access-control-authentication-and-authorization",
            "text": "In  this lab you are going to,   Create users and groups and setup certs based authentication  Create service accounts for applications  Create Roles and ClusterRoles to define authorizations  Map Roles and ClusterRoles to subjects i.e. users, groups and service accounts using RoleBingings and ClusterRoleBindings.",
            "title": "Lab K202 - Kubernetes Access Control:  Authentication and Authorization"
        },
        {
            "location": "/configuring_authentication_and_authorization/#how-one-can-access-the-kubernetes-api",
            "text": "The Kubernetes API can be accessed by three ways.   Kubectl - A command line utility of Kubernetes  Client libraries - Go, Python, etc.,  REST requests",
            "title": "How one can access the Kubernetes API?"
        },
        {
            "location": "/configuring_authentication_and_authorization/#who-can-access-the-kubernetes-api",
            "text": "Kubernetes API can be accessed by,   Human Users  Service Accounts     Each of these topics will be discussed in detail in the later part of this chapter.",
            "title": "Who can access the Kubernetes API?"
        },
        {
            "location": "/configuring_authentication_and_authorization/#stages-of-a-request",
            "text": "When a request tries to contact the API , it goes through various stages as illustrated in the image given below.  \n   source: official kubernetes site",
            "title": "Stages of a Request"
        },
        {
            "location": "/configuring_authentication_and_authorization/#api-groups-and-resources",
            "text": "apiGroup  Resources      apps  daemonsets, deployments, deployments/rollback, deployments/scale, replicasets, replicasets/scale, statefulsets, statefulsets/scale    core  configmaps, endpoints, persistentvolumeclaims, replicationcontrollers, replicationcontrollers/scale, secrets, serviceaccounts, services,services/proxy    autoscaling  horizontalpodautoscalers    batch  cronjobs, jobs    policy  poddisruptionbudgets    networking.k8s.io  networkpolicies    authorization.k8s.io  localsubjectaccessreviews    rbac.authorization.k8s.io  rolebindings,roles    extensions  deprecated (read notes)     Notes  In addition to the above apiGroups, you may see  extensions  being used in some example code snippets. Please note that  extensions  was initially created as a experiement and is been deprecated, by moving most of the matured apis to one of the groups mentioned above.   You could read this comment and the thread  to get clarity on this.",
            "title": "api groups and resources"
        },
        {
            "location": "/configuring_authentication_and_authorization/#role-based-access-control-rbac",
            "text": "Group  User  Namespaces  Resources  Access Type (verbs)      ops  maya  all  all  get, list, watch, update, patch, create, delete, deletecollection    dev  kim  instavote  deployments, statefulsets, services, pods, configmaps, secrets, replicasets, ingresses, endpoints, cronjobs, jobs, persistentvolumeclaims  get, list , watch, update, patch, create    interns  yono  instavote  readonly  get, list, watch        Service Accounts  Namespace  Resources  Access Type (verbs)      monitoring  all  all  readonly     Creating Kubernetes Users and Groups  Generate the user's private key  mkdir -p  ~/.kube/users\ncd ~/.kube/users\n\nopenssl genrsa -out kim.key 2048  [sample Output]  openssl genrsa -out kim.key 2048\nGenerating RSA private key, 2048 bit long modulus\n.............................................................+++\n.........................+++\ne is 65537 (0x10001)  Lets now create a  Certification Signing Request (CSR)  for each of the users. When you generate the csr make sure you also provide   CN: This will be set as username  O: Org name. This is actually used as a  group  by kubernetes while authenticating/authorizing users.  You could add as many as you need   e.g.  openssl req -new -key kim.key -out kim.csr -subj \"/CN=kim/O=dev/O=example.org\"  In order to be deemed authentic, these CSRs need to be signed by the  Certification Authority (CA)  which in this case is Kubernetes Master.   You need access to the folllwing files on  kubernetes master.   Certificate : ca.crt (kubeadm) or ca.key (kubespray)  Pricate Key : ca.key (kubeadm) or ca-key.pem  (kubespray)   You would typically find it  the following paths   /etc/kubernetes/pki   To verify which one is your cert and which one is key, use the following command,  $ file /etc/kubernetes/pki/ca.crt\nca.pem: PEM certificate\n\n\n$ file /etc/kubernetes/pki/ca.key\nca-key.pem: PEM RSA private key  Once signed, .csr files with added signatures become the certificates that could be used to authenticate.  You could either   move the crt files to k8s master, sign and download    copy over the CA certs and keys to your management node and use it to sign. Make sure to keep your CA related files secure.   In the example here, I have already downloaded  ca.pem  and  ca-key.pem  to my management workstation, which are used to sign the CSRs.    Assuming all the files are in the same directory, sign the CSR as,  openssl x509 -req -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -days 730 -in kim.csr -out kim.crt  Setting up User configs with kubectl  In order to configure the users that you created above, following steps need to be performed with kubectl   Add credentials in the configurations  Set context to login as a user to a cluster  Switch context in order to assume the user's identity while working with the cluster   to add credentials,  kubectl config set-credentials kim --client-certificate=/root/.kube/users/kim.crt --client-key=/root/.kube/users/kim.key  where,   /root/.kube/users/kim.crt : Absolute path to the users' certificate  /root/.kube/users/kim.key: Absolute path to the users' key   And proceed to set/create  contexts (user@cluster). If you are not sure whats the cluster name, use the following command to find,  kubectl config get-contexts  [sample output]  CURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE\n*         kubernetes-admin@kubernetes   kubernetes   kubernetes-admin   instavote  where,   kubernetes  is the  cluster name.  To set context for  kubernetes  cluster,  kubectl config set-context kim-kubernetes --cluster=kubernetes  --user=kim --namespace=instavote  Where,   kim-kubernetes : name of the context    kubernetes  : name of the  cluster you set while creating it    kim : user you created and configured above to connect to the cluster     You could verify the configs with  kubectl config get-contexts\n\nCURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE\n          kim-kubernetes                kubernetes   kim                instavote\n*         kubernetes-admin@kubernetes   kubernetes   kubernetes-admin   instavote  and  kubectl config view\n\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: DATA+OMITTED\n    server: https://178.128.109.8:6443\n  name: kubernetes\ncontexts:\n- context:\n    cluster: kubernetes\n    namespace: instavote\n    user: kim\n  name: kim-kubernetes\n- context:\n    cluster: kubernetes\n    namespace: instavote\n    user: kubernetes-admin\n  name: kubernetes-admin@kubernetes\ncurrent-context: kubernetes-admin@kubernetes\nkind: Config\npreferences: {}\nusers:\n- name: kim\n  user:\n    client-certificate: users/kim.crt\n    client-key: users/kim.key\n- name: kubernetes-admin\n  user:\n    client-certificate-data: REDACTED\n    client-key-data: REDACTED  Where, you should see the configurations for the new user you created have been added.  You could assume the identity of user  kim  and connect  to the  kubernetes  cluster as,  kubectl config use-context kim-kubernetes\n\nkubectl config get-contexts\n\nCURRENT   NAME                          CLUSTER      AUTHINFO           NAMESPACE\n*         kim-kubernetes                kubernetes   kim                instavote\n          kubernetes-admin@kubernetes   kubernetes   kubernetes-admin   instavote  This time * appears on the line which lists context  kim-kubernetes  that you just created.  And then try running any command as,  kubectl get pods  Alternately, if you are a admin user, you could impersonate a user and run a command with that literally using  --as option  kubectl config use-context admin-prod\nkubectl get pods --as yono  [Sample Output]  No resources found.\nError from server (Forbidden): pods is forbidden: User \"yono\" cannot list pods in the namespace \"instavote\"  Either ways, since there are authorization rules set, the user can not make any api calls. Thats when you would create some roles and bind it to the users in the next section.",
            "title": "Role Based Access Control (RBAC)"
        },
        {
            "location": "/configuring_authentication_and_authorization/#define-authorisation-rules-with-roles-and-clusterroles",
            "text": "Whats the difference between Roles and ClusterRoles ??   Role is  limited to a namespace (Projects/Orgs/Env)  ClusterRole is Global   Lets say you want to provide read only access to  instavote , a project specific namespace to all users in the  example.org  kubectl config use-context kubernetes-admin@kubernetes\ncd projects/instavote/dev  file: readonly-role.yaml  apiVersion: rbac.authorization.k8s.io/v1beta1\nkind: Role\nmetadata:\n  namespace: instavote\n  name: readonly\nrules:\n- apiGroups: [\"*\"]\n  resources: [\"*\"]\n  verbs: [\"get\", \"list\", \"watch\"]  In order to map it to all users in  example.org , create a RoleBinding as  file: readonly-rolebinding.yml  kind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: readonly\n  namespace: instavote\nsubjects:\n- kind: Group\n  name: dev\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: readonly\n  apiGroup: rbac.authorization.k8s.io  kubectl apply -f readonly-role.yaml\nkubectl apply -f readonly-rolebinding.yml  To get information about the objects created above,  kubectl get roles,rolebindings -n instavote\n\nkubectl describe role readonly\nkubectl describe rolebinding readonly  To validate the access,  kubectl config get-contexts\nkubectl config use-context kim-kubernetes\nkubectl get pods  To switch back to admin,  kubectl config use-context kubernetes-admin@kubernetes  Exercise  Create a Role and Rolebinding for  dev  group with the authorizations defined in the table above. Once applied, test it",
            "title": "Define authorisation rules with Roles and ClusterRoles"
        },
        {
            "location": "/advanced_pod_scheduling/",
            "text": "Lab K203 - Advanced Pod Scheduling\n\n\nIn the Kubernetes bootcamp training, we have seen how to create a pod and and some basic pod configurations to go with it. But this chapter explains some advanced topics related to pod scheduling.\n\n\nFrom the \napi document for version 1.11\n following are the pod specs which are relevant from scheduling perspective.\n\n\n\n\nnodeSelector\n\n\nnodeName\n\n\naffinity\n\n\nschedulerName\n\n\ntolerations\n\n\n\n\nLabeling your nodes\n\n\nkubectl get nodes --show-labels\n\nkubectl label nodes <node-name> zone=aaa\n\nkubectl get nodes --show-labels\n\n\n\n\n\ne.g.\n\n\nkubectl label nodes node1 zone=bbb\nkubectl label nodes node2 zone=bbb\nkubectl label nodes node3 zone=aaa\nkubectl get nodes --show-labels\n\n\n\n\nwhere, replace \nnode1-3\n with the actual nodes in your cluster.\n\n\nDefining  affinity and anti-affinity\n\n\nWe have discussed about scheduling a pod on a particular node using \nNodeSelector\n, but using node selector is a hard condition. If the condition is not met, the pod cannot be scheduled. Node/Pod affinity and anti-affinity solves this issue by introducing soft and hard conditions.\n\n\n\n\nrequired\n\n\n\n\npreferred\n\n\n\n\n\n\nDuringScheduling\n\n\n\n\nDuringExecution\n\n\n\n\nOperators\n\n\n\n\nIn\n\n\nNotIn\n\n\nExists\n\n\nDoesNotExist\n\n\nGt\n\n\nLt\n\n\n\n\nAdding Node Affinity\n\n\nExamine  the current pod distribution  \n\n\nkubectl get pods -o wide --selector=\"role=vote\"\n\n\n\n\n\nand node labels\n\n\nkubectl get nodes --show-labels\n\n\n\n\n\nLets create node affinity criteria as\n\n\n\n\nPods for vote app \nmust\n not run on the master nodes\n\n\nPods for vote app \npreferably\n run on a node in zone \nbbb\n\n\n\n\nFirst is a \nhard\n affinity versus second being \nsoft\n affinity.\n\n\nfile: vote-deploy-nodeaffinity.yaml\n\n\n....\n  template:\n....\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v1\n          ports:\n            - containerPort: 80\n              protocol: TCP\n\n              affinity:\n                nodeAffinity:\n                  requiredDuringSchedulingIgnoredDuringExecution:\n                    nodeSelectorTerms:\n                    - matchExpressions:\n                      - key: node-role.kubernetes.io/master\n                        operator: DoesNotExist\n                  preferredDuringSchedulingIgnoredDuringExecution:\n                    - weight: 1\n                      preference:\n                        matchExpressions:\n                        - key: zone\n                          operator: In\n                          values:\n                            - bbb\n\n\n\n\napply\n\n\nkubectl apply -f vote-deploy-nodeaffinity.yaml\n\nkubectl get pods -o wide\n\n\n\n\nConfiguring Pod Affinity\n\n\nLets define pod affinity criteria as,\n\n\n\n\nPods for \nvote\n and \nredis\n should be co located as much as possible (preferred)\n\n\nNo two pods with \nredis\n app should be running on the same node (required)\n\n\n\n\nkubectl get pods -o wide --selector=\"role in (vote,redis)\"\n\n\n\n\n\nfile: vote-deploy-podaffinity.yaml\n\n\n...\n    template:\n...\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v1\n          ports:\n            - containerPort: 80\n              protocol: TCP\n\n      affinity:\n...\n\n        podAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n            - weight: 1\n              podAffinityTerm:\n                labelSelector:\n                  matchExpressions:\n                  - key: role\n                    operator: In\n                    values:\n                    - redis\n                topologyKey: kubernetes.io/hostname\n\n\n\n\nfile: redis-deploy-podaffinity.yaml\n\n\n....\n  template:\n...\n    spec:\n      containers:\n      - image: schoolofdevops/redis:latest\n        imagePullPolicy: Always\n        name: redis\n        ports:\n        - containerPort: 6379\n          protocol: TCP\n      restartPolicy: Always\n\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: role\n                operator: In\n                values:\n                - redis\n            topologyKey: \"kubernetes.io/hostname\"\n\n\n\n\napply\n\n\nkubectl apply -f redis-deploy-podaffinity.yaml\nkubectl apply -f vote-deploy-podaffinity.yaml\n\n\n\n\n\n\ncheck the pods distribution\n\n\nkubectl get pods -o wide --selector=\"role in (vote,redis)\"\n\n\n\n\nObservations from the above output,\n\n\n\n\nSince redis has a hard constraint not to be on the same node, you would observe redis pods being on differnt nodes  (node2 and node4)\n\n\nsince vote app has a soft constraint, you see some of the pods running on node4 (same node running redis), others continue to run on node 3\n\n\n\n\nIf you kill the pods on node3, at the time of  scheduling new ones, scheduler meets all affinity rules\n\n\nNow try scaling up redis instances\n\n\nkubectl scale deploy/redis --replicas=4\nkubectl get pods -o wide\n\n\n\n\n\n\nAre all redis pods runnning ? Why?\n\n\n\n\nAdding Taints and tolerations\n\n\n\n\nAffinity is defined for pods\n\n\nTaints are defined for nodes\n\n\n\n\nYou could add the taints with criteria and effects. Effetcs can be\n\n\nTaint Specs\n:   \n\n\n\n\neffect  \n\n\nNoSchedule  \n\n\nPreferNoSchedule  \n\n\nNoExecute  \n\n\n\n\n\n\nkey  \n\n\nvalue  \n\n\ntimeAdded (only written for NoExecute taints)  \n\n\n\n\nObserve the pods distribution\n\n\nkubectl get pods -o wide\n\n\n\n\n\nLets taint a node.\n\n\nkubectl taint node node2 dedicated=worker:NoExecute\n\nkubectl describe node node2\n\n\n\n\nafter tainting the node\n\n\nkubectl get pods -o wide\n\n\n\n\nAll pods running on node2 just got evicted.\n\n\nAdd toleration in the Deployment for worker.\n\n\nFile: worker-deploy.yml\n\n\napiVersion: apps/v1\n.....\n  template:\n....\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote-worker:latest\n\n      tolerations:\n        - key: \"dedicated\"\n          operator: \"Equal\"\n          value: \"worker\"\n          effect: \"NoExecute\"\n\n\n\n\napply\n\n\nkubectl apply -f worker-deploy.yml\n\n\n\n\n\nObserve the pod distribution now.\n\n\n$ kubectl get pods -o wide\nNAME                      READY     STATUS    RESTARTS   AGE       IP             NODE\ndb-66496667c9-qggzd       1/1       Running   0          4h        10.233.74.74   node4\nredis-5bf748dbcf-ckn65    1/1       Running   0          3m        10.233.71.26   node3\nredis-5bf748dbcf-vxppx    1/1       Running   0          31m       10.233.74.79   node4\nresult-5c7569bcb7-4fptr   1/1       Running   0          4h        10.233.71.18   node3\nresult-5c7569bcb7-s4rdx   1/1       Running   0          4h        10.233.74.75   node4\nvote-56bf599b9c-22lpw     1/1       Running   0          30m       10.233.74.80   node4\nvote-56bf599b9c-4l6bc     1/1       Running   0          12m       10.233.74.83   node4\nvote-56bf599b9c-bqsrq     1/1       Running   0          12m       10.233.74.82   node4\nvote-56bf599b9c-xw7zc     1/1       Running   0          12m       10.233.74.81   node4\nworker-6cc8dbd4f8-6bkfg   1/1       Running   0          1m        10.233.75.15   node2\n\n\n\n\nYou should see worker being scheduled on node2\n\n\nTo remove the taint created above\n\n\nkubectl taint node node2 dedicate:NoExecute-\n\n\n\n\nExercise\n\n\n\n\nMaster node is unschedulable because of a taint. Find the taint on the master node and remove it. See if new pods get scheduled on it after that.",
            "title": "Lab K203 - Advanced Pod Scheduling"
        },
        {
            "location": "/advanced_pod_scheduling/#lab-k203-advanced-pod-scheduling",
            "text": "In the Kubernetes bootcamp training, we have seen how to create a pod and and some basic pod configurations to go with it. But this chapter explains some advanced topics related to pod scheduling.  From the  api document for version 1.11  following are the pod specs which are relevant from scheduling perspective.   nodeSelector  nodeName  affinity  schedulerName  tolerations",
            "title": "Lab K203 - Advanced Pod Scheduling"
        },
        {
            "location": "/advanced_pod_scheduling/#labeling-your-nodes",
            "text": "kubectl get nodes --show-labels\n\nkubectl label nodes <node-name> zone=aaa\n\nkubectl get nodes --show-labels  e.g.  kubectl label nodes node1 zone=bbb\nkubectl label nodes node2 zone=bbb\nkubectl label nodes node3 zone=aaa\nkubectl get nodes --show-labels  where, replace  node1-3  with the actual nodes in your cluster.",
            "title": "Labeling your nodes"
        },
        {
            "location": "/advanced_pod_scheduling/#defining-affinity-and-anti-affinity",
            "text": "We have discussed about scheduling a pod on a particular node using  NodeSelector , but using node selector is a hard condition. If the condition is not met, the pod cannot be scheduled. Node/Pod affinity and anti-affinity solves this issue by introducing soft and hard conditions.   required   preferred    DuringScheduling   DuringExecution   Operators   In  NotIn  Exists  DoesNotExist  Gt  Lt   Adding Node Affinity  Examine  the current pod distribution    kubectl get pods -o wide --selector=\"role=vote\"  and node labels  kubectl get nodes --show-labels  Lets create node affinity criteria as   Pods for vote app  must  not run on the master nodes  Pods for vote app  preferably  run on a node in zone  bbb   First is a  hard  affinity versus second being  soft  affinity.  file: vote-deploy-nodeaffinity.yaml  ....\n  template:\n....\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v1\n          ports:\n            - containerPort: 80\n              protocol: TCP\n\n              affinity:\n                nodeAffinity:\n                  requiredDuringSchedulingIgnoredDuringExecution:\n                    nodeSelectorTerms:\n                    - matchExpressions:\n                      - key: node-role.kubernetes.io/master\n                        operator: DoesNotExist\n                  preferredDuringSchedulingIgnoredDuringExecution:\n                    - weight: 1\n                      preference:\n                        matchExpressions:\n                        - key: zone\n                          operator: In\n                          values:\n                            - bbb  apply  kubectl apply -f vote-deploy-nodeaffinity.yaml\n\nkubectl get pods -o wide  Configuring Pod Affinity  Lets define pod affinity criteria as,   Pods for  vote  and  redis  should be co located as much as possible (preferred)  No two pods with  redis  app should be running on the same node (required)   kubectl get pods -o wide --selector=\"role in (vote,redis)\"  file: vote-deploy-podaffinity.yaml  ...\n    template:\n...\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v1\n          ports:\n            - containerPort: 80\n              protocol: TCP\n\n      affinity:\n...\n\n        podAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n            - weight: 1\n              podAffinityTerm:\n                labelSelector:\n                  matchExpressions:\n                  - key: role\n                    operator: In\n                    values:\n                    - redis\n                topologyKey: kubernetes.io/hostname  file: redis-deploy-podaffinity.yaml  ....\n  template:\n...\n    spec:\n      containers:\n      - image: schoolofdevops/redis:latest\n        imagePullPolicy: Always\n        name: redis\n        ports:\n        - containerPort: 6379\n          protocol: TCP\n      restartPolicy: Always\n\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: role\n                operator: In\n                values:\n                - redis\n            topologyKey: \"kubernetes.io/hostname\"  apply  kubectl apply -f redis-deploy-podaffinity.yaml\nkubectl apply -f vote-deploy-podaffinity.yaml  check the pods distribution  kubectl get pods -o wide --selector=\"role in (vote,redis)\"  Observations from the above output,   Since redis has a hard constraint not to be on the same node, you would observe redis pods being on differnt nodes  (node2 and node4)  since vote app has a soft constraint, you see some of the pods running on node4 (same node running redis), others continue to run on node 3   If you kill the pods on node3, at the time of  scheduling new ones, scheduler meets all affinity rules  Now try scaling up redis instances  kubectl scale deploy/redis --replicas=4\nkubectl get pods -o wide   Are all redis pods runnning ? Why?",
            "title": "Defining  affinity and anti-affinity"
        },
        {
            "location": "/advanced_pod_scheduling/#adding-taints-and-tolerations",
            "text": "Affinity is defined for pods  Taints are defined for nodes   You could add the taints with criteria and effects. Effetcs can be  Taint Specs :      effect    NoSchedule    PreferNoSchedule    NoExecute      key    value    timeAdded (only written for NoExecute taints)     Observe the pods distribution  kubectl get pods -o wide  Lets taint a node.  kubectl taint node node2 dedicated=worker:NoExecute\n\nkubectl describe node node2  after tainting the node  kubectl get pods -o wide  All pods running on node2 just got evicted.  Add toleration in the Deployment for worker.  File: worker-deploy.yml  apiVersion: apps/v1\n.....\n  template:\n....\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote-worker:latest\n\n      tolerations:\n        - key: \"dedicated\"\n          operator: \"Equal\"\n          value: \"worker\"\n          effect: \"NoExecute\"  apply  kubectl apply -f worker-deploy.yml  Observe the pod distribution now.  $ kubectl get pods -o wide\nNAME                      READY     STATUS    RESTARTS   AGE       IP             NODE\ndb-66496667c9-qggzd       1/1       Running   0          4h        10.233.74.74   node4\nredis-5bf748dbcf-ckn65    1/1       Running   0          3m        10.233.71.26   node3\nredis-5bf748dbcf-vxppx    1/1       Running   0          31m       10.233.74.79   node4\nresult-5c7569bcb7-4fptr   1/1       Running   0          4h        10.233.71.18   node3\nresult-5c7569bcb7-s4rdx   1/1       Running   0          4h        10.233.74.75   node4\nvote-56bf599b9c-22lpw     1/1       Running   0          30m       10.233.74.80   node4\nvote-56bf599b9c-4l6bc     1/1       Running   0          12m       10.233.74.83   node4\nvote-56bf599b9c-bqsrq     1/1       Running   0          12m       10.233.74.82   node4\nvote-56bf599b9c-xw7zc     1/1       Running   0          12m       10.233.74.81   node4\nworker-6cc8dbd4f8-6bkfg   1/1       Running   0          1m        10.233.75.15   node2  You should see worker being scheduled on node2  To remove the taint created above  kubectl taint node node2 dedicate:NoExecute-  Exercise   Master node is unschedulable because of a taint. Find the taint on the master node and remove it. See if new pods get scheduled on it after that.",
            "title": "Adding Taints and tolerations"
        },
        {
            "location": "/pods-health-probes/",
            "text": "Lab K204 - Adding health checks with Probes\n\n\nAdding health checks\n\n\nHealth checks in Kubernetes work the same way as traditional health checks of applications. They make sure that our application is ready to receive and process user requests. In Kubernetes we have two types of health checks,\n  * Liveness Probe\n  * Readiness Probe\nProbes are simply a \ndiagnostic action\n performed by the kubelet. There are three types actions a kubelet perfomes on a pod, which are namely,\n\n\n\n\nExecAction\n: Executes a command inside the pod. Assumed successful when the command \nreturns 0\n as exit code.\n\n\nTCPSocketAction\n: Checks for a state of a particular port on the pod. Considered successful when the state of \nthe port is open\n.\n\n\nHTTPGetAction\n: Performs a GET request on pod's IP. Assumed successful when the status code is \ngreater than 200 and less than 400\n\n\n\n\nIn cases of any failure during the diagnostic action, kubelet will report back to the API server. Let us study about how these health checks work in practice.\n\n\nAdding Liveness/Readineess  Probes\n\n\nLiveness probe checks the status of the pod(whether it is running or not). If livenessProbe fails, then the pod is subjected to its restart policy. The default state of livenessProbe is \nSuccess\n.\n\n\nReadiness probe checks whether your application is ready to serve the requests. When the readiness probe fails, the pod's IP is removed from the end point list of the service. The default state of readinessProbe is \nSuccess\n.\n\n\nLet us add liveness/readiness  probes to our \nvote\n deployment.\n\n\nfile: vote-deploy-probes.yaml\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote\nspec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2\n      maxUnavailable: 1\n  revisionHistoryLimit: 4\n  replicas: 12\n  minReadySeconds: 20\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3, v4, v5]}\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v1\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v1\n          resources:\n            requests:\n              memory: \"64Mi\"\n              cpu: \"50m\"\n            limits:\n              memory: \"128Mi\"\n              cpu: \"250m\"\n          livenessProbe:\n            tcpSocket:\n              port: 80\n            initialDelaySeconds: 5\n            periodSeconds: 5\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 80\n            initialDelaySeconds: 5\n            periodSeconds: 3\n\n\n\n\nwhere,\n\n\n\n\nlivenessProbe used a simple tcp check to test whether application is listening on port 80\n\n\nreadinessProbe does httpGet to actually fetch a page using get method and tests for the http response code.\n\n\n\n\nApply this code using,\n\n\nkubectl apply -f vote-deploy-probes.yaml\nkubectl get pods\nkubectl describe svc vote\n\n\n\n\n\nTesting livenessProbe\n\n\nkubectl edit deploy vote\n\n\n\n\nlivenessProbe:\n  failureThreshold: 3\n  initialDelaySeconds: 5\n  periodSeconds: 5\n  successThreshold: 1\n  tcpSocket:\n    port: 8888\n  timeoutSeconds: 1\n\n\n\n\nSince you are using \nedit\n command, as soon as you save the file, deployment is modified.\n\n\nkubectl get pods\nkubectl describe pod vote-xxxx\n\n\n\n\nwhere, vote-xxxx is one of the new pods created.\n\n\nEvents:\n  Type     Reason     Age                From               Message\n  ----     ------     ----               ----               -------\n  Normal   Scheduled  38s                default-scheduler  Successfully assigned instavote/vote-668579766d-p65xb to k-02\n  Normal   Pulled     18s (x2 over 36s)  kubelet, k-02      Container image \"schoolofdevops/vote:v1\" already present on machine\n  Normal   Created    18s (x2 over 36s)  kubelet, k-02      Created container\n  Normal   Started    18s (x2 over 36s)  kubelet, k-02      Started container\n  Normal   Killing    18s                kubelet, k-02      Killing container with id docker://app:Container failed liveness probe.. Container will be killed and recreated.\n  Warning  Unhealthy  4s (x5 over 29s)   kubelet, k-02      Liveness probe failed: dial tcp 10.32.0.12:8888: connect: connection refused\n\n\n\n\nWhat just happened ?\n\n\n\n\nSince livenessProbe is failing it will keep killing and recreating containers. Thats what you see in the description above.\n\n\nWhen you list pods, you should see it in crashloopbackoff state with number of restarts incrementing with time.\n\n\n\n\ne.g.\n\n\nvote-668579766d-p65xb    0/1     CrashLoopBackOff   7          7m38s   10.32.0.12   k-02        <none>           <none>\nvote-668579766d-sclbr    0/1     CrashLoopBackOff   7          7m38s   10.32.0.10   k-02        <none>           <none>\nvote-668579766d-vrcmj    0/1     CrashLoopBackOff   7          7m38s   10.38.0.8    kube03-01   <none>           <none>\n\n\n\n\nTo fix it, revert the livenessProbe configs by editing the deplyment again.\n\n\nReadiness Probe\n\n\nReadiness probe is configured just like liveness probe. But this time we will use \nhttpGet request\n.\n\n\nkubectl edit deploy vote\n\n\n\n\n\nreadinessProbe:\n  failureThreshold: 3\n  httpGet:\n    path: /test.html\n    port: 80\n    scheme: HTTP\n  initialDelaySeconds: 5\n  periodSeconds: 3\n  successThreshold: 1\n\n\n\n\nwhere, readinessProbe.httpGet.path is been changed from \n/\n to \n/test.html\n which is a non existant path.  \n\n\ncheck\n\n\nkubectl get deploy,rs,pods\n\n\n\n\n[output snippet]\n\n\nNAME                          READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.extensions/vote    11/12   3            11          2m12s\n\n\nvote-8cbb7ff89-6xvbc     0/1     Running   0          73s     10.38.0.10   kube03-01   <none>           <none>\nvote-8cbb7ff89-6z5zv     0/1     Running   0          73s     10.38.0.5    kube03-01   <none>           <none>\nvote-8cbb7ff89-hdmxb     0/1     Running   0          73s     10.32.0.12   k-02        <none>           <none>\n\n\n\n\nkubectl describe pod vote-8cbb7ff89-hdmxb\n\n\n\n\nwhere, vote-8cbb7ff89-hdmxb is one of the pods launched after changing readiness probe.\n\n\n[output snippet]\n\n\nEvents:\n  Type     Reason     Age                  From               Message\n  ----     ------     ----                 ----               -------\n  Normal   Scheduled  109s                 default-scheduler  Successfully assigned instavote/vote-8cbb7ff89-hdmxb to k-02\n  Normal   Pulled     108s                 kubelet, k-02      Container image \"schoolofdevops/vote:v1\" already present on machine\n  Normal   Created    108s                 kubelet, k-02      Created container\n  Normal   Started    108s                 kubelet, k-02      Started container\n  Warning  Unhealthy  39s (x22 over 102s)  kubelet, k-02      Readiness probe failed: HTTP probe failed with statuscode: 404\n\n\n\n\nkubectl describe svc vote\n\n\n\n\nwhat happened ?\n\n\n\n\nSince readinessProbe failed, the new launched batch does not show containers running (0/1)\n\n\nDescription of the pod shows it being \nUnhealthy\n due to failed HTTP probe\n\n\nDeployment shows surged pods, with number of ready pods being less than number of desired replicas (e.g. 11/12).\n\n\nService does not send traffic to the pod which are marked as unhealthy/not ready.  \n\n\n\n\nReverting the changes to readiness probe should bring it back to working state.",
            "title": "Lab K204 - Adding health checks with Probes"
        },
        {
            "location": "/pods-health-probes/#lab-k204-adding-health-checks-with-probes",
            "text": "",
            "title": "Lab K204 - Adding health checks with Probes"
        },
        {
            "location": "/pods-health-probes/#adding-health-checks",
            "text": "Health checks in Kubernetes work the same way as traditional health checks of applications. They make sure that our application is ready to receive and process user requests. In Kubernetes we have two types of health checks,\n  * Liveness Probe\n  * Readiness Probe\nProbes are simply a  diagnostic action  performed by the kubelet. There are three types actions a kubelet perfomes on a pod, which are namely,   ExecAction : Executes a command inside the pod. Assumed successful when the command  returns 0  as exit code.  TCPSocketAction : Checks for a state of a particular port on the pod. Considered successful when the state of  the port is open .  HTTPGetAction : Performs a GET request on pod's IP. Assumed successful when the status code is  greater than 200 and less than 400   In cases of any failure during the diagnostic action, kubelet will report back to the API server. Let us study about how these health checks work in practice.  Adding Liveness/Readineess  Probes  Liveness probe checks the status of the pod(whether it is running or not). If livenessProbe fails, then the pod is subjected to its restart policy. The default state of livenessProbe is  Success .  Readiness probe checks whether your application is ready to serve the requests. When the readiness probe fails, the pod's IP is removed from the end point list of the service. The default state of readinessProbe is  Success .  Let us add liveness/readiness  probes to our  vote  deployment.  file: vote-deploy-probes.yaml  apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote\nspec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2\n      maxUnavailable: 1\n  revisionHistoryLimit: 4\n  replicas: 12\n  minReadySeconds: 20\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3, v4, v5]}\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v1\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v1\n          resources:\n            requests:\n              memory: \"64Mi\"\n              cpu: \"50m\"\n            limits:\n              memory: \"128Mi\"\n              cpu: \"250m\"\n          livenessProbe:\n            tcpSocket:\n              port: 80\n            initialDelaySeconds: 5\n            periodSeconds: 5\n          readinessProbe:\n            httpGet:\n              path: /\n              port: 80\n            initialDelaySeconds: 5\n            periodSeconds: 3  where,   livenessProbe used a simple tcp check to test whether application is listening on port 80  readinessProbe does httpGet to actually fetch a page using get method and tests for the http response code.   Apply this code using,  kubectl apply -f vote-deploy-probes.yaml\nkubectl get pods\nkubectl describe svc vote  Testing livenessProbe  kubectl edit deploy vote  livenessProbe:\n  failureThreshold: 3\n  initialDelaySeconds: 5\n  periodSeconds: 5\n  successThreshold: 1\n  tcpSocket:\n    port: 8888\n  timeoutSeconds: 1  Since you are using  edit  command, as soon as you save the file, deployment is modified.  kubectl get pods\nkubectl describe pod vote-xxxx  where, vote-xxxx is one of the new pods created.  Events:\n  Type     Reason     Age                From               Message\n  ----     ------     ----               ----               -------\n  Normal   Scheduled  38s                default-scheduler  Successfully assigned instavote/vote-668579766d-p65xb to k-02\n  Normal   Pulled     18s (x2 over 36s)  kubelet, k-02      Container image \"schoolofdevops/vote:v1\" already present on machine\n  Normal   Created    18s (x2 over 36s)  kubelet, k-02      Created container\n  Normal   Started    18s (x2 over 36s)  kubelet, k-02      Started container\n  Normal   Killing    18s                kubelet, k-02      Killing container with id docker://app:Container failed liveness probe.. Container will be killed and recreated.\n  Warning  Unhealthy  4s (x5 over 29s)   kubelet, k-02      Liveness probe failed: dial tcp 10.32.0.12:8888: connect: connection refused  What just happened ?   Since livenessProbe is failing it will keep killing and recreating containers. Thats what you see in the description above.  When you list pods, you should see it in crashloopbackoff state with number of restarts incrementing with time.   e.g.  vote-668579766d-p65xb    0/1     CrashLoopBackOff   7          7m38s   10.32.0.12   k-02        <none>           <none>\nvote-668579766d-sclbr    0/1     CrashLoopBackOff   7          7m38s   10.32.0.10   k-02        <none>           <none>\nvote-668579766d-vrcmj    0/1     CrashLoopBackOff   7          7m38s   10.38.0.8    kube03-01   <none>           <none>  To fix it, revert the livenessProbe configs by editing the deplyment again.  Readiness Probe  Readiness probe is configured just like liveness probe. But this time we will use  httpGet request .  kubectl edit deploy vote  readinessProbe:\n  failureThreshold: 3\n  httpGet:\n    path: /test.html\n    port: 80\n    scheme: HTTP\n  initialDelaySeconds: 5\n  periodSeconds: 3\n  successThreshold: 1  where, readinessProbe.httpGet.path is been changed from  /  to  /test.html  which is a non existant path.    check  kubectl get deploy,rs,pods  [output snippet]  NAME                          READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.extensions/vote    11/12   3            11          2m12s\n\n\nvote-8cbb7ff89-6xvbc     0/1     Running   0          73s     10.38.0.10   kube03-01   <none>           <none>\nvote-8cbb7ff89-6z5zv     0/1     Running   0          73s     10.38.0.5    kube03-01   <none>           <none>\nvote-8cbb7ff89-hdmxb     0/1     Running   0          73s     10.32.0.12   k-02        <none>           <none>  kubectl describe pod vote-8cbb7ff89-hdmxb  where, vote-8cbb7ff89-hdmxb is one of the pods launched after changing readiness probe.  [output snippet]  Events:\n  Type     Reason     Age                  From               Message\n  ----     ------     ----                 ----               -------\n  Normal   Scheduled  109s                 default-scheduler  Successfully assigned instavote/vote-8cbb7ff89-hdmxb to k-02\n  Normal   Pulled     108s                 kubelet, k-02      Container image \"schoolofdevops/vote:v1\" already present on machine\n  Normal   Created    108s                 kubelet, k-02      Created container\n  Normal   Started    108s                 kubelet, k-02      Started container\n  Warning  Unhealthy  39s (x22 over 102s)  kubelet, k-02      Readiness probe failed: HTTP probe failed with statuscode: 404  kubectl describe svc vote  what happened ?   Since readinessProbe failed, the new launched batch does not show containers running (0/1)  Description of the pod shows it being  Unhealthy  due to failed HTTP probe  Deployment shows surged pods, with number of ready pods being less than number of desired replicas (e.g. 11/12).  Service does not send traffic to the pod which are marked as unhealthy/not ready.     Reverting the changes to readiness probe should bring it back to working state.",
            "title": "Adding health checks"
        },
        {
            "location": "/helm/",
            "text": "Lab K205 - Monitoring setup with HELM\n\n\nIn this lab, you are going to install and configure helm, and in turns, use it to configure a monitoring system for kubernetes using prometheus and grafana stack.\n\n\nInstalling  Helm\n\n\nTo install helm you can follow following instructions.\n\n\ncurl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get > get_helm.sh\nchmod 700 get_helm.sh\n./get_helm.sh\n\n\n\n\nVerify the installtion is successful,\n\n\nhelm --help\n\n\n\n\nLets now setup  RBAC configurations required for Tiller, a component of helm that runs inside the kubernetes cluster.\n\n\nfile: tiller-rbac.yaml\n\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: tiller\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: tiller\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n  - kind: ServiceAccount\n    name: tiller\n    namespace: kube-system\n\n\n\n\nApply the ClusterRole and ClusterRoleBinding.\n\n\nkubectl apply -f tiller-rbac.yaml\n\n\n\n\n\nThis is where we actually initialize Tiller in our Kubernetes cluster.\n\n\nhelm init --service-account tiller\n\n\n\n\nSetting up Monitoring Stack with HELM\n\n\nYou will now setup prometheus and grafana monitoring stacks with helm, with a few customisations.\n\n\nInstall Prometheus with Helm\n\n\nBefore proceeding, you could review the \nOfficial Prometheus Helm Chart\n  from the repository.\n\n\nSearch and download a chart for prometheus\n\n\nhelm search prometheus\nhelm fetch --untar stable/prometheus\ncd prometheus\n\n\n\n\nTo provide custom configurations, copy over the custom values file from \nk8s-code\n repot.\n\n\ncp ../k8s-code/helper/helm/values/prometheus-customvalues.yaml .\n\n\n\n\nReview \nprometheus-customvalues.yaml\n and then launch prometheus stack as,\n\n\nhelm install --name prometheus --values prometheus-customvalues.yaml  . --dry-run\nhelm install --name prometheus --values prometheus-customvalues.yaml  .\n\nhelm list\nhelm status prometheus\n\n\n\n\nYou should be able to access prometheus UI by using either the \nnodePort\n service or a \ningress\n rule.\n\n\nDeploying Grafana with HELM\n\n\nYou could refer to the \nOfficial Grafana Helm Chart repository\n before proceeding.\n\n\nSearch and download a chart for prometheus\n\n\nhelm search grafana\nhelm fetch --untar stable/grafana\ncd grafana\n\n\n\n\nTo provide custom configurations, copy over the custom values file from \nk8s-code\n repot.\n\n\ncp ../k8s-code/helper/helm/values/grafana-customvalues.yaml .\n\n\n\n\nReview \ngrafana-customvalues.yaml\n and then launch grafana as,\n\n\nhelm install --name grafana --values grafana-customvalues.yaml  . --dry-run\nhelm install --name grafana --values grafana-customvalues.yaml  .\n\nhelm list\nhelm status grafana\n\n\n\n\nYou should be able to access grafana UI by using either the \nnodePort\n service or a \ningress\n rule.\n\n\n\n\ncredentials for grafana\n\n\n\n\nuser: admin\n\n\npass: password\n\n\n\n\nYou could update it along with other values in \ngrafana-customvalues.yaml\n or create a separate file to override the values.\n\n\nIf you update values and would like to apply to existing helm release, use a command simiar to following,\n\n\nhelm upgrade -f grafana-customvalues.yaml grafana .\n\n\n\n\nSummary\n\n\nIn this lab, we not only learnt about HELM, a kubernetes package manager, but  also have setup a sophisticated health monitoring system with prometheus and grafana.",
            "title": "Lab K205 - Monitoring setup with HELM"
        },
        {
            "location": "/helm/#lab-k205-monitoring-setup-with-helm",
            "text": "In this lab, you are going to install and configure helm, and in turns, use it to configure a monitoring system for kubernetes using prometheus and grafana stack.",
            "title": "Lab K205 - Monitoring setup with HELM"
        },
        {
            "location": "/helm/#installing-helm",
            "text": "To install helm you can follow following instructions.  curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get > get_helm.sh\nchmod 700 get_helm.sh\n./get_helm.sh  Verify the installtion is successful,  helm --help  Lets now setup  RBAC configurations required for Tiller, a component of helm that runs inside the kubernetes cluster.  file: tiller-rbac.yaml  apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: tiller\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: tiller\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n  - kind: ServiceAccount\n    name: tiller\n    namespace: kube-system  Apply the ClusterRole and ClusterRoleBinding.  kubectl apply -f tiller-rbac.yaml  This is where we actually initialize Tiller in our Kubernetes cluster.  helm init --service-account tiller",
            "title": "Installing  Helm"
        },
        {
            "location": "/helm/#setting-up-monitoring-stack-with-helm",
            "text": "You will now setup prometheus and grafana monitoring stacks with helm, with a few customisations.  Install Prometheus with Helm  Before proceeding, you could review the  Official Prometheus Helm Chart   from the repository.  Search and download a chart for prometheus  helm search prometheus\nhelm fetch --untar stable/prometheus\ncd prometheus  To provide custom configurations, copy over the custom values file from  k8s-code  repot.  cp ../k8s-code/helper/helm/values/prometheus-customvalues.yaml .  Review  prometheus-customvalues.yaml  and then launch prometheus stack as,  helm install --name prometheus --values prometheus-customvalues.yaml  . --dry-run\nhelm install --name prometheus --values prometheus-customvalues.yaml  .\n\nhelm list\nhelm status prometheus  You should be able to access prometheus UI by using either the  nodePort  service or a  ingress  rule.  Deploying Grafana with HELM  You could refer to the  Official Grafana Helm Chart repository  before proceeding.  Search and download a chart for prometheus  helm search grafana\nhelm fetch --untar stable/grafana\ncd grafana  To provide custom configurations, copy over the custom values file from  k8s-code  repot.  cp ../k8s-code/helper/helm/values/grafana-customvalues.yaml .  Review  grafana-customvalues.yaml  and then launch grafana as,  helm install --name grafana --values grafana-customvalues.yaml  . --dry-run\nhelm install --name grafana --values grafana-customvalues.yaml  .\n\nhelm list\nhelm status grafana  You should be able to access grafana UI by using either the  nodePort  service or a  ingress  rule.   credentials for grafana   user: admin  pass: password   You could update it along with other values in  grafana-customvalues.yaml  or create a separate file to override the values.  If you update values and would like to apply to existing helm release, use a command simiar to following,  helm upgrade -f grafana-customvalues.yaml grafana .  Summary  In this lab, we not only learnt about HELM, a kubernetes package manager, but  also have setup a sophisticated health monitoring system with prometheus and grafana.",
            "title": "Setting up Monitoring Stack with HELM"
        },
        {
            "location": "/10_kubernetes_autoscaling/",
            "text": "Kubernetes Horizonntal Pod Autoscaling\n\n\nWith Horizontal Pod Autoscaling, Kubernetes automatically scales the number of pods in a replication controller, deployment or replica set based on observed CPU utilization (or, with alpha support, on some other, application-provided metrics).\n\n\nThe Horizontal Pod Autoscaler is implemented as a Kubernetes API resource and a controller. The resource determines the behavior of the controller. The controller periodically adjusts the number of replicas in a replication controller or deployment to match the observed average CPU utilization to the target specified by user\n\n\nPrerequisites\n\n\n\n\nMetrics Server\n.\n  This needs to be setup if you are using kubeadm etc.  and replaces \nheapster\n starting with kubernetes version  1.8.\n\n\nResource Requests and Limits. Defining \nCPU\nas well as \nMemory\n requirements for containers in Pod Spec is a must\n\n\n\n\nDeploying Metrics Server\n\n\nKubernetes Horizontal Pod Autoscaler along with \nkubectl top\n command depends on the core monitoring data such as cpu and memory utilization which is scraped and provided by kubelet, which comes with in built cadvisor component.  Earlier, you would have to install a additional component called \nheapster\n in order to collect this data and feed it to the \nhpa\n controller. With 1.8 version of Kubernetes, this behavior is changed, and now \nmetrics-server\n would provide this data. Metric server  is being included as a essential component for kubernetes cluster, and being incroporated into kubernetes to be included out of box. It stores the core monitoring information using in-memory data store.\n\n\nIf you try to pull monitoring information using the following commands\n\n\nkubectl top pod\n\nkubectl top node\n\n\n\n\nit does not show it, rather gives you a error message similar to\n\n\n[output]\n\n\nError from server (NotFound): the server could not find the requested resource (get services http:heapster:)\n\n\n\n\nEven though the error mentions heapster, its replaced with metrics server by default now.\n\n\nDeploy  metric server with the following commands,\n\n\ncd ~\ngit clone  https://github.com/kubernetes-incubator/metrics-server.git\nkubectl apply -f metrics-server/deploy/1.8+/\n\n\n\n\nValidate\n\n\nkubectl get deploy,pods -n kube-system --selector='k8s-app=metrics-server'\n\n\n\n\nMonitoring has been setup.\n\n\nFixing issues with Metrics deployment\n\n\nThere is a known issue as off Dec 2018 with Metrics Server where is fails to work event after deploying it using above commands. This can be fixed with a patch using steps below.\n\n\nTo apply a patch to metrics server,\n\n\nwget -c https://gist.githubusercontent.com/initcron/1a2bd25353e1faa22a0ad41ad1c01b62/raw/008e23f9fbf4d7e2cf79df1dd008de2f1db62a10/k8s-metrics-server.patch.yaml\n\nkubectl patch deploy metrics-server -p \"$(cat k8s-metrics-server.patch.yaml)\" -n kube-system\n\n\n\n\nNow validate with\n\n\nkubectl top node\nkubectl top pod\n\n\n\n\nwhere expected output shoudl be similar to,\n\n\nkubectl top node\n\nNAME     CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%\nvis-01   145m         7%     2215Mi          57%\nvis-13   36m          1%     1001Mi          26%\nvis-14   71m          3%     1047Mi          27%\n\n\n\n\nCreate a HPA\n\n\nTo demonstrate Horizontal Pod Autoscaler we will use a custom docker image based on the php-apache image\n\n\nfile: vote-hpa.yaml\n\n\napiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: vote\nspec:\n  minReplicas: 4\n  maxReplicas: 15\n  targetCPUUtilizationPercentage: 40\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: vote\n\n\n\n\napply\n\n\nkubectl apply -f vote-hpa.yaml\n\n\n\n\nValidate\n\n\nkubectl get hpa\n\nkubectl describe hpa vote\n\nkubectl get pod,deploy\n\n\n\n\n\n\nIf you have a monitoring system such as grafana, you could also view the graphs for \nvote\n deployment.\n\n\n\n\nLoad Test\n\n\nfile: loadtest-job.yaml\n\n\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: loadtest\nspec:\n  template:\n    spec:\n      containers:\n      - name: siege\n        image: schoolofdevops/loadtest:v1\n        command: [\"siege\",  \"--concurrent=5\", \"--benchmark\", \"--time=10m\", \"http://vote\"]\n      restartPolicy: Never\n  backoffLimit: 4\n\n\n\n\nAnd launch the loadtest\n\n\nkubectl apply -f loadtest-job.yaml\n\n\n\n\nTo monitor while the load test is running ,\n\n\nwatch kubectl top pods\n\n\n\n\n\nTo get information about the job\n\n\nkubectl get jobs\nkubectl describe  job loadtest\n\n\n\n\n\nTo check the load test output\n\n\nkubectl logs  -f loadtest-xxxx\n\n\n\n\n[replace \nloadtest-xxxx\n with the actual pod id.]\n\n\n[Sample Output]\n\n\n** SIEGE 3.0.8\n** Preparing 15 concurrent users for battle.\nroot@kube-01:~# kubectl logs vote-loadtest-tv6r2 -f\n** SIEGE 3.0.8\n** Preparing 15 concurrent users for battle.\n\n.....\n\n\nLifting the server siege...      done.\n\nTransactions:              41618 hits\nAvailability:              99.98 %\nElapsed time:             299.13 secs\nData transferred:         127.05 MB\nResponse time:              0.11 secs\nTransaction rate:         139.13 trans/sec\nThroughput:             0.42 MB/sec\nConcurrency:               14.98\nSuccessful transactions:       41618\nFailed transactions:               8\nLongest transaction:            3.70\nShortest transaction:           0.00\n\nFILE: /var/log/siege.log\nYou can disable this annoying message by editing\nthe .siegerc file in your home directory; change\nthe directive 'show-logfile' to false.\n\n\n\n\nNow check the job status again,\n\n\nkubectl get jobs\nNAME            DESIRED   SUCCESSFUL   AGE\nvote-loadtest   1         1            10m\n\n\n\n\n\n\n\nKeep monitoring for the load on the pod as the job progresses.\n\n\nKeep a watch from grafana as well to see the resource utilisation for vote deployment.\n\n\nYou should see hpa in action as it scales out/in the  vote deployment with the increasing/decreasing load.\n\n\n\n\n\n\nSummary\n\n\nIn this lab, you have successfull configured and demonstrated dynamic scaling ability of kubernetes using horizontalpodautoscalers. You have also learnt about a new \njobs\n controller type for running one off or batch jobs.\n\n\nReading List\n\n\n\n\nKubernetes Monitoring Architecture\n\n\nCore Metrics Pipeline\n\n\nMetrics Server\n\n\nAssigning Resources to Containers and Pods\n\n\nHorizontal Pod Autoscaler",
            "title": "Lab K206 - Auto Scaling Capacity with HPA"
        },
        {
            "location": "/10_kubernetes_autoscaling/#kubernetes-horizonntal-pod-autoscaling",
            "text": "With Horizontal Pod Autoscaling, Kubernetes automatically scales the number of pods in a replication controller, deployment or replica set based on observed CPU utilization (or, with alpha support, on some other, application-provided metrics).  The Horizontal Pod Autoscaler is implemented as a Kubernetes API resource and a controller. The resource determines the behavior of the controller. The controller periodically adjusts the number of replicas in a replication controller or deployment to match the observed average CPU utilization to the target specified by user  Prerequisites   Metrics Server .\n  This needs to be setup if you are using kubeadm etc.  and replaces  heapster  starting with kubernetes version  1.8.  Resource Requests and Limits. Defining  CPU as well as  Memory  requirements for containers in Pod Spec is a must   Deploying Metrics Server  Kubernetes Horizontal Pod Autoscaler along with  kubectl top  command depends on the core monitoring data such as cpu and memory utilization which is scraped and provided by kubelet, which comes with in built cadvisor component.  Earlier, you would have to install a additional component called  heapster  in order to collect this data and feed it to the  hpa  controller. With 1.8 version of Kubernetes, this behavior is changed, and now  metrics-server  would provide this data. Metric server  is being included as a essential component for kubernetes cluster, and being incroporated into kubernetes to be included out of box. It stores the core monitoring information using in-memory data store.  If you try to pull monitoring information using the following commands  kubectl top pod\n\nkubectl top node  it does not show it, rather gives you a error message similar to  [output]  Error from server (NotFound): the server could not find the requested resource (get services http:heapster:)  Even though the error mentions heapster, its replaced with metrics server by default now.  Deploy  metric server with the following commands,  cd ~\ngit clone  https://github.com/kubernetes-incubator/metrics-server.git\nkubectl apply -f metrics-server/deploy/1.8+/  Validate  kubectl get deploy,pods -n kube-system --selector='k8s-app=metrics-server'  Monitoring has been setup.  Fixing issues with Metrics deployment  There is a known issue as off Dec 2018 with Metrics Server where is fails to work event after deploying it using above commands. This can be fixed with a patch using steps below.  To apply a patch to metrics server,  wget -c https://gist.githubusercontent.com/initcron/1a2bd25353e1faa22a0ad41ad1c01b62/raw/008e23f9fbf4d7e2cf79df1dd008de2f1db62a10/k8s-metrics-server.patch.yaml\n\nkubectl patch deploy metrics-server -p \"$(cat k8s-metrics-server.patch.yaml)\" -n kube-system  Now validate with  kubectl top node\nkubectl top pod  where expected output shoudl be similar to,  kubectl top node\n\nNAME     CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%\nvis-01   145m         7%     2215Mi          57%\nvis-13   36m          1%     1001Mi          26%\nvis-14   71m          3%     1047Mi          27%  Create a HPA  To demonstrate Horizontal Pod Autoscaler we will use a custom docker image based on the php-apache image  file: vote-hpa.yaml  apiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: vote\nspec:\n  minReplicas: 4\n  maxReplicas: 15\n  targetCPUUtilizationPercentage: 40\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: vote  apply  kubectl apply -f vote-hpa.yaml  Validate  kubectl get hpa\n\nkubectl describe hpa vote\n\nkubectl get pod,deploy  If you have a monitoring system such as grafana, you could also view the graphs for  vote  deployment.   Load Test  file: loadtest-job.yaml  apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: loadtest\nspec:\n  template:\n    spec:\n      containers:\n      - name: siege\n        image: schoolofdevops/loadtest:v1\n        command: [\"siege\",  \"--concurrent=5\", \"--benchmark\", \"--time=10m\", \"http://vote\"]\n      restartPolicy: Never\n  backoffLimit: 4  And launch the loadtest  kubectl apply -f loadtest-job.yaml  To monitor while the load test is running ,  watch kubectl top pods  To get information about the job  kubectl get jobs\nkubectl describe  job loadtest  To check the load test output  kubectl logs  -f loadtest-xxxx  [replace  loadtest-xxxx  with the actual pod id.]  [Sample Output]  ** SIEGE 3.0.8\n** Preparing 15 concurrent users for battle.\nroot@kube-01:~# kubectl logs vote-loadtest-tv6r2 -f\n** SIEGE 3.0.8\n** Preparing 15 concurrent users for battle.\n\n.....\n\n\nLifting the server siege...      done.\n\nTransactions:              41618 hits\nAvailability:              99.98 %\nElapsed time:             299.13 secs\nData transferred:         127.05 MB\nResponse time:              0.11 secs\nTransaction rate:         139.13 trans/sec\nThroughput:             0.42 MB/sec\nConcurrency:               14.98\nSuccessful transactions:       41618\nFailed transactions:               8\nLongest transaction:            3.70\nShortest transaction:           0.00\n\nFILE: /var/log/siege.log\nYou can disable this annoying message by editing\nthe .siegerc file in your home directory; change\nthe directive 'show-logfile' to false.  Now check the job status again,  kubectl get jobs\nNAME            DESIRED   SUCCESSFUL   AGE\nvote-loadtest   1         1            10m   Keep monitoring for the load on the pod as the job progresses.  Keep a watch from grafana as well to see the resource utilisation for vote deployment.  You should see hpa in action as it scales out/in the  vote deployment with the increasing/decreasing load.    Summary  In this lab, you have successfull configured and demonstrated dynamic scaling ability of kubernetes using horizontalpodautoscalers. You have also learnt about a new  jobs  controller type for running one off or batch jobs.  Reading List   Kubernetes Monitoring Architecture  Core Metrics Pipeline  Metrics Server  Assigning Resources to Containers and Pods  Horizontal Pod Autoscaler",
            "title": "Kubernetes Horizonntal Pod Autoscaling"
        },
        {
            "location": "/network_policies/",
            "text": "Setting up a firewall with Network Policies\n\n\nWhile setting up the network policy, you may need to refer to the namespace created earlier. In order to being abel to referred to, namespace should have a label. Lets  update the namespace with a label.\n\n\nfile: instavote-ns.yaml\n\n\nkind: Namespace\napiVersion: v1\nmetadata:\n  name: instavote\n  labels:\n    project: instavote\n\n\n\n\n\napply\n\n\nkubectl get namespace --show-labels\nkubectl apply -f instavote-ns.yaml\nkubectl get namespace --show-labels\n\n\n\n\n\nNow, define a restrictive network policy which would,\n\n\n\n\nBlock all incoming connections from any source except for pods from the same namespace  \n\n\nBlock all outgoing connections\n\n\n\n\n\n  +-----------------------------------------------------------+\n  |                                                           |\n  |    +----------+          +-----------+                    |\nx |    | results  |          | db        |                    |\n  |    |          |          |           |                    |\n  |    +----------+          +-----------+                    |\n  |                                                           |\n  |                                                           |\n  |                                        +----+----+--+     |           \n  |                                        |   worker   |     |            \n  |                                        |            |     |           \n  |                                        +----+-------+     |           \n  |                                                           |\n  |                                                           |\n  |    +----------+          +-----------+                    |\n  |    | vote     |          | redis     |                    |\nx |    |          |          |           |                    |\n  |    +----------+          +-----------+                    |\n  |                                                           |\n  +-----------------------------------------------------------+\n\n\n\n\n\nfile: \ninstavote-netpol.yaml\n\n\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n\n\n\n\napply\n\n\nkubectl get netpol\n\nkubectl apply -f instavote-netpol.yaml\n\nkubectl get netpol\n\nkubectl describe netpol/default-deny\n\n\n\n\n\nTry accessing the vote and results ui. Can you access it ?\n\n\nSetting up ingress rules for outward facing applications\n\n\n\n  +-----------------------------------------------------------+\n  |                                                           |\n  |    +----------+          +-----------+                    |\n=====> | results  |          | db        |                    |\n  |    |          |          |           |                    |\n  |    +----------+          +-----------+                    |\n  |                                                           |\n  |                                                           |\n  |                                        +----+----+--+     |           \n  |                                        |   worker   |     |            \n  |                                        |            |     |           \n  |                                        +----+-------+     |           \n  |                                                           |\n  |                                                           |\n  |    +----------+          +-----------+                    |\n  |    | vote     |          | redis     |                    |\n=====> |          |          |           |                    |\n  |    +----------+          +-----------+                    |\n  |                                                           |\n  +-----------------------------------------------------------+\n\n\n\n\n\nTo the same file, add a new network policy object.\n\n\nfile: instavote-netpol.yaml\n\n\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: public-ingress\n  namespace: instavote\nspec:\n  podSelector:\n    matchExpressions:\n      - {key: role, operator: In, values: [vote, results]}\n  policyTypes:\n  - Ingress\n  ingress:\n    - {}\n\n\n\n\nwhere,\n\n\ninstavote-ingress\n is a new network policy  which,\n\n\n\n\ndefines policy for pods with \nvote\n and \nresults\n role\n\n\nand allows them incoming access from anywhere\n\n\n\n\napply\n\n\nkubectl apply -f instavote-netpol.yaml\n\n\n\n\nExercise\n\n\n\n\nTry accessing the ui now and check if you are able to.\n\n\nTry to vote, see if that works? Why ?\n\n\n\n\nSetting up egress rules to allow communication between services from same project\n\n\nWhen you tried to vote, you might have observed that it does not work. Thats because the default network policy we created earlier blocks all outgoing traffic. Which is good for securing the  environment, however you still need to provide inter connection between services from the same project.  Specifically \nvote\n, \nworker\n and \nresults\n apps need outgoing connection to \nredis\n and \ndb\n. Lets allow that with a egress policy.\n\n\n\n  +-----------------------------------------------------------+\n  |                                                           |\n  |    +------------+        +-----------+                    |\n=====> | results    | ------>| db        |                    |\n  |    |            |        |           | <-------+          |\n  |    +------------+        +-----------+         |          |\n  |                                                |          |\n  |                                                |          |\n  |                                        +----+----+---+    |           \n  |                                        |   worker    |    |            \n  |                                        |             |    |           \n  |                                        +----+--------+    |           \n  |                                                |          |\n  |                                                |          |\n  |    +----------+          +-----------+         |          |\n  |    | vote     |          | redis     | <-------+          |\n=====> |          |  ------> |           |                    |\n  |    +----------+          +-----------+                    |\n  |                                                           |\n  +-----------------------------------------------------------+\n\n\n\n\n\nEdit the same policy file  and add the following snippet,\n\n\nfile: instavote-netpol.yaml\n\n\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          project: instavote\n  egress:\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          project: instavote\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: public-ingress\n  namespace: instavote\nspec:\n  podSelector:\n    matchExpressions:\n      - {key: role, operator: In, values: [vote, results]}\n  policyTypes:\n  - Ingress\n  ingress:\n    - {}\n\n\n\n\nwhere,\n\n\ninstavote-egress\n is a new network policy  which,\n\n\n\n\ndefines policy for pods with \nvote\n, \nworker\n and \nresults\n role\n\n\nand allows them outgoing  access to  any pods in the same namespace, and that includes \nredis\n and \ndb\n\n\n\n\nProject\n\n\nThe above network policies are a good start. However you could even further restrict access by creating a granular network policy for each application.  \n\n\nCreate network policies with following specs,\n\n\nvote\n\n\n\n\nallow incoming connections from anywhere, only on port 80\n\n\nallow outgoing connections to \nredis\n\n\nblock everything else, incoming and outgoing  \n\n\n\n\nredis\n\n\n\n\nallow incoming connections from \nvote\n and \nworker\n, only on port 6379\n\n\nblock everything else, incoming and outgoing  \n\n\n\n\nworker\n\n\n\n\nallow outgoing connections to \nredis\n and \ndb\n\n\nblock everything else, incoming and outgoing  \n\n\n\n\ndb\n\n\n\n\nallow incoming connections from \nworker\n and \nresults\n, only on port 5342\n\n\nblock everything else, incoming and outgoing  \n\n\n\n\nresults\n\n\n\n\nallow incoming connections from anywhere, only on port 80\n\n\nallow outgoing connections to \ndb\n\n\nblock everything else, incoming and outgoing",
            "title": "Lab K207 - Access Control with Network Policies"
        },
        {
            "location": "/network_policies/#setting-up-a-firewall-with-network-policies",
            "text": "While setting up the network policy, you may need to refer to the namespace created earlier. In order to being abel to referred to, namespace should have a label. Lets  update the namespace with a label.  file: instavote-ns.yaml  kind: Namespace\napiVersion: v1\nmetadata:\n  name: instavote\n  labels:\n    project: instavote  apply  kubectl get namespace --show-labels\nkubectl apply -f instavote-ns.yaml\nkubectl get namespace --show-labels  Now, define a restrictive network policy which would,   Block all incoming connections from any source except for pods from the same namespace    Block all outgoing connections   \n  +-----------------------------------------------------------+\n  |                                                           |\n  |    +----------+          +-----------+                    |\nx |    | results  |          | db        |                    |\n  |    |          |          |           |                    |\n  |    +----------+          +-----------+                    |\n  |                                                           |\n  |                                                           |\n  |                                        +----+----+--+     |           \n  |                                        |   worker   |     |            \n  |                                        |            |     |           \n  |                                        +----+-------+     |           \n  |                                                           |\n  |                                                           |\n  |    +----------+          +-----------+                    |\n  |    | vote     |          | redis     |                    |\nx |    |          |          |           |                    |\n  |    +----------+          +-----------+                    |\n  |                                                           |\n  +-----------------------------------------------------------+  file:  instavote-netpol.yaml  apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress  apply  kubectl get netpol\n\nkubectl apply -f instavote-netpol.yaml\n\nkubectl get netpol\n\nkubectl describe netpol/default-deny  Try accessing the vote and results ui. Can you access it ?  Setting up ingress rules for outward facing applications  \n  +-----------------------------------------------------------+\n  |                                                           |\n  |    +----------+          +-----------+                    |\n=====> | results  |          | db        |                    |\n  |    |          |          |           |                    |\n  |    +----------+          +-----------+                    |\n  |                                                           |\n  |                                                           |\n  |                                        +----+----+--+     |           \n  |                                        |   worker   |     |            \n  |                                        |            |     |           \n  |                                        +----+-------+     |           \n  |                                                           |\n  |                                                           |\n  |    +----------+          +-----------+                    |\n  |    | vote     |          | redis     |                    |\n=====> |          |          |           |                    |\n  |    +----------+          +-----------+                    |\n  |                                                           |\n  +-----------------------------------------------------------+  To the same file, add a new network policy object.  file: instavote-netpol.yaml  apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: public-ingress\n  namespace: instavote\nspec:\n  podSelector:\n    matchExpressions:\n      - {key: role, operator: In, values: [vote, results]}\n  policyTypes:\n  - Ingress\n  ingress:\n    - {}  where,  instavote-ingress  is a new network policy  which,   defines policy for pods with  vote  and  results  role  and allows them incoming access from anywhere   apply  kubectl apply -f instavote-netpol.yaml  Exercise   Try accessing the ui now and check if you are able to.  Try to vote, see if that works? Why ?   Setting up egress rules to allow communication between services from same project  When you tried to vote, you might have observed that it does not work. Thats because the default network policy we created earlier blocks all outgoing traffic. Which is good for securing the  environment, however you still need to provide inter connection between services from the same project.  Specifically  vote ,  worker  and  results  apps need outgoing connection to  redis  and  db . Lets allow that with a egress policy.  \n  +-----------------------------------------------------------+\n  |                                                           |\n  |    +------------+        +-----------+                    |\n=====> | results    | ------>| db        |                    |\n  |    |            |        |           | <-------+          |\n  |    +------------+        +-----------+         |          |\n  |                                                |          |\n  |                                                |          |\n  |                                        +----+----+---+    |           \n  |                                        |   worker    |    |            \n  |                                        |             |    |           \n  |                                        +----+--------+    |           \n  |                                                |          |\n  |                                                |          |\n  |    +----------+          +-----------+         |          |\n  |    | vote     |          | redis     | <-------+          |\n=====> |          |  ------> |           |                    |\n  |    +----------+          +-----------+                    |\n  |                                                           |\n  +-----------------------------------------------------------+  Edit the same policy file  and add the following snippet,  file: instavote-netpol.yaml  apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          project: instavote\n  egress:\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          project: instavote\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: public-ingress\n  namespace: instavote\nspec:\n  podSelector:\n    matchExpressions:\n      - {key: role, operator: In, values: [vote, results]}\n  policyTypes:\n  - Ingress\n  ingress:\n    - {}  where,  instavote-egress  is a new network policy  which,   defines policy for pods with  vote ,  worker  and  results  role  and allows them outgoing  access to  any pods in the same namespace, and that includes  redis  and  db   Project  The above network policies are a good start. However you could even further restrict access by creating a granular network policy for each application.    Create network policies with following specs,  vote   allow incoming connections from anywhere, only on port 80  allow outgoing connections to  redis  block everything else, incoming and outgoing     redis   allow incoming connections from  vote  and  worker , only on port 6379  block everything else, incoming and outgoing     worker   allow outgoing connections to  redis  and  db  block everything else, incoming and outgoing     db   allow incoming connections from  worker  and  results , only on port 5342  block everything else, incoming and outgoing     results   allow incoming connections from anywhere, only on port 80  allow outgoing connections to  db  block everything else, incoming and outgoing",
            "title": "Setting up a firewall with Network Policies"
        },
        {
            "location": "/cluster-administration/",
            "text": "Lab K208 - Kubernetes Cluster Administration\n\n\nDefining Quotas\n\n\nCreate and switch to a new \nstaging\n namespace.\n\n\nconfig get-contexts\n\nkubectl create namespace staging\n\nkubectl config set-context --current --namespace=staging\n\nconfig get-contexts\n\n\n\n\nDefine quota\n\n\nfile: staging-quota.yaml\n\n\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: staging\n  namespace: staging\nspec:\n  hard:\n    requests.cpu: \"0.5\"\n    requests.memory: 500Mi\n    limits.cpu: \"2\"\n    limits.memory: 2Gi\n    count/deployments.apps: 1\n\n\n\n\nkubectl get quota -n staging\nkubectl apply -f staging-quota.yaml\nkubectl get quota -n staging\nkubectl describe  quota\n\n\n\n\nfile: nginx-deploy.yaml\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: staging\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: nginx\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        resources:\n          limits:\n            memory: \"500Mi\"\n            cpu: \"500m\"\n          requests:\n            memory: \"200Mi\"\n            cpu: \"200m\"\n\n\n\n\nkubectl apply -f nginx-deploy.yaml\nkubectl describe  quota -n staging\n\n\n\n\n\n\nLets now try to scale up the deployment and observe.\n\n\nkubectl scale deploy nginx --replicas=4\n\nkubectl get deploy\n\nNAME    READY   UP-TO-DATE   AVAILABLE   AGE\nnginx   2/4     2            2           2m55s\n\n\n\n\nWhat happened ?\n\n\n\n\nEven though deployment updated the number of desired replicas, only  2 are available\n\n\nDeployment calls replicaset to launch new replicas. If you describe the replicaset it throws an error related to quota being exceeded.\n\n\n\n\ne.g.\n\n\n# kubectl get rs\nNAME               DESIRED   CURRENT   READY   AGE\nnginx-56c479cd4f   4         2         2       5m4s\n\n# kubectl describe rs nginx-56c479cd4f\n\n  Warning  FailedCreate      34s (x5 over 73s)  replicaset-controller  (combined from similar events): Error creating: pods \"nginx-56c479cd4f-kwf9h\" is forbidden: exceeded quota: staging, requested: requests.cpu=200m,requests.memory=200Mi, used: requests.cpu=400m,requests.memory=400Mi, limited: requests.cpu=500m,requests.memory=500Mi\n\n\n\n\nYou just configured  resource quota based on a  namespace. Now, switch back your namespace to \ninstavote\n or the the one you were using before the beginning of this lab.  \n\n\nkubectl config set-context --current --namespace=instavote\nkubectl config get-contexts\n\n\n\n\nNodes  Maintenance\n\n\nYou could isolate a problematic node for further troubleshooting by \ncordonning\n it off. You could also \ndrain\n it while preparing for maintenance.\n\n\nCordon a Node\n\n\nkubectl get pods -o wide\n\nNAME                      READY     STATUS    RESTARTS   AGE       IP             NODE\ndb-66496667c9-qggzd       1/1       Running   0          5h        10.233.74.74   node4\nredis-5bf748dbcf-ckn65    1/1       Running   0          42m       10.233.71.26   node3\nredis-5bf748dbcf-vxppx    1/1       Running   0          1h        10.233.74.79   node4\nresult-5c7569bcb7-4fptr   1/1       Running   0          5h        10.233.71.18   node3\nresult-5c7569bcb7-s4rdx   1/1       Running   0          5h        10.233.74.75   node4\nvote-56bf599b9c-22lpw     1/1       Running   0          1h        10.233.74.80   node4\nvote-56bf599b9c-4l6bc     1/1       Running   0          50m       10.233.74.83   node4\nvote-56bf599b9c-bqsrq     1/1       Running   0          50m       10.233.74.82   node4\nvote-56bf599b9c-xw7zc     1/1       Running   0          50m       10.233.74.81   node4\nworker-6cc8dbd4f8-6bkfg   1/1       Running   0          39m       10.233.75.15   node2\n\n\n\n\nLets cordon one of the nodes and observe.\n\n\nkubectl cordon node4\nnode/node4 cordoned\n\n\n\n\nObserve the changes\n\n\n\n$ kubectl get pods -o wide\nNAME                      READY     STATUS    RESTARTS   AGE       IP             NODE\ndb-66496667c9-qggzd       1/1       Running   0          5h        10.233.74.74   node4\nredis-5bf748dbcf-ckn65    1/1       Running   0          43m       10.233.71.26   node3\nredis-5bf748dbcf-vxppx    1/1       Running   0          1h        10.233.74.79   node4\nresult-5c7569bcb7-4fptr   1/1       Running   0          5h        10.233.71.18   node3\nresult-5c7569bcb7-s4rdx   1/1       Running   0          5h        10.233.74.75   node4\nvote-56bf599b9c-22lpw     1/1       Running   0          1h        10.233.74.80   node4\nvote-56bf599b9c-4l6bc     1/1       Running   0          51m       10.233.74.83   node4\nvote-56bf599b9c-bqsrq     1/1       Running   0          51m       10.233.74.82   node4\nvote-56bf599b9c-xw7zc     1/1       Running   0          51m       10.233.74.81   node4\nworker-6cc8dbd4f8-6bkfg   1/1       Running   0          40m       10.233.75.15   node2\n\n\n\n$ kubectl get nodes -o wide\n\n\nNAME      STATUS                     ROLES         AGE       VERSION   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nnode1     Ready                      master,node   1d        v1.10.4   <none>        Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode2     Ready                      master,node   1d        v1.10.4   <none>        Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2\nnode3     Ready                      node          1d        v1.10.4   <none>        Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode4     Ready,SchedulingDisabled   node          1d        v1.10.4   <none>        Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2\n\n\n\n\n\n\nNow launch a new deployment and scale it.\n\n\nkubectl create deployment cordontest --image=busybox --replicas=5\nkubectl scale deploy cordontest --replicas=5\n\nkubectl get pods -o wide\n\n\n\n\nwhat happened ?\n\n\n\n\nNew pods scheduled due the deployment above, do not get launched on the node which is been cordoned off.\n\n\n\n\n$ kubectl uncordon node4\nnode/node4 uncordoned\n\n\n$ kubectl get nodes -o wide\nNAME      STATUS    ROLES         AGE       VERSION   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nnode1     Ready     master,node   1d        v1.10.4   <none>        Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode2     Ready     master,node   1d        v1.10.4   <none>        Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2\nnode3     Ready     node          1d        v1.10.4   <none>        Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode4     Ready     node          1d        v1.10.4   <none>        Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2\n\n\n\n\ndelete the test  deployment\n\n\nkubectl delete deploy cordontest\n\n\n\n\nDrain a Node\n\n\nDraining a node will not only mark it unschedulable but also will evict existing pods running on it. Use it with care.\n\n\n$ kubectl drain node3\nnode/node3 cordoned\nerror: unable to drain node \"node3\", aborting command...\n\nThere are pending nodes to be drained:\n node3\nerror: pods with local storage (use --delete-local-data to override): kubernetes-dashboard-55fdfd74b4-jdgch; DaemonSet-managed pods (use --ignore-daemonsets to ignore): calico-node-4f8xc\n\n\n\n\n\nDrain with options\n\n\nkubectl drain node3 --delete-local-data --ignore-daemonsets\n\n\n\n\nObserve the effect,\n\n\nkubectl get pods -o wide\nkubectl get nodes -o wide\n\n\n\n\n\nTo add the node back to the available schedulable node pool,\n\n\nkubectl uncordon node4\nnode/node4 uncordoned\n\n\n\n\n\nSummary\n\n\nIn this lab, we learnt about limiting resource by defining per namespace quota, as well as learnt how to prepare nodes for maintenance by cordoning and draining it.",
            "title": "Lab K208 - Cluster Administration"
        },
        {
            "location": "/cluster-administration/#lab-k208-kubernetes-cluster-administration",
            "text": "",
            "title": "Lab K208 - Kubernetes Cluster Administration"
        },
        {
            "location": "/cluster-administration/#defining-quotas",
            "text": "Create and switch to a new  staging  namespace.  config get-contexts\n\nkubectl create namespace staging\n\nkubectl config set-context --current --namespace=staging\n\nconfig get-contexts  Define quota  file: staging-quota.yaml  apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: staging\n  namespace: staging\nspec:\n  hard:\n    requests.cpu: \"0.5\"\n    requests.memory: 500Mi\n    limits.cpu: \"2\"\n    limits.memory: 2Gi\n    count/deployments.apps: 1  kubectl get quota -n staging\nkubectl apply -f staging-quota.yaml\nkubectl get quota -n staging\nkubectl describe  quota  file: nginx-deploy.yaml  apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: staging\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      name: nginx\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        resources:\n          limits:\n            memory: \"500Mi\"\n            cpu: \"500m\"\n          requests:\n            memory: \"200Mi\"\n            cpu: \"200m\"  kubectl apply -f nginx-deploy.yaml\nkubectl describe  quota -n staging  Lets now try to scale up the deployment and observe.  kubectl scale deploy nginx --replicas=4\n\nkubectl get deploy\n\nNAME    READY   UP-TO-DATE   AVAILABLE   AGE\nnginx   2/4     2            2           2m55s  What happened ?   Even though deployment updated the number of desired replicas, only  2 are available  Deployment calls replicaset to launch new replicas. If you describe the replicaset it throws an error related to quota being exceeded.   e.g.  # kubectl get rs\nNAME               DESIRED   CURRENT   READY   AGE\nnginx-56c479cd4f   4         2         2       5m4s\n\n# kubectl describe rs nginx-56c479cd4f\n\n  Warning  FailedCreate      34s (x5 over 73s)  replicaset-controller  (combined from similar events): Error creating: pods \"nginx-56c479cd4f-kwf9h\" is forbidden: exceeded quota: staging, requested: requests.cpu=200m,requests.memory=200Mi, used: requests.cpu=400m,requests.memory=400Mi, limited: requests.cpu=500m,requests.memory=500Mi  You just configured  resource quota based on a  namespace. Now, switch back your namespace to  instavote  or the the one you were using before the beginning of this lab.    kubectl config set-context --current --namespace=instavote\nkubectl config get-contexts",
            "title": "Defining Quotas"
        },
        {
            "location": "/cluster-administration/#nodes-maintenance",
            "text": "You could isolate a problematic node for further troubleshooting by  cordonning  it off. You could also  drain  it while preparing for maintenance.  Cordon a Node  kubectl get pods -o wide\n\nNAME                      READY     STATUS    RESTARTS   AGE       IP             NODE\ndb-66496667c9-qggzd       1/1       Running   0          5h        10.233.74.74   node4\nredis-5bf748dbcf-ckn65    1/1       Running   0          42m       10.233.71.26   node3\nredis-5bf748dbcf-vxppx    1/1       Running   0          1h        10.233.74.79   node4\nresult-5c7569bcb7-4fptr   1/1       Running   0          5h        10.233.71.18   node3\nresult-5c7569bcb7-s4rdx   1/1       Running   0          5h        10.233.74.75   node4\nvote-56bf599b9c-22lpw     1/1       Running   0          1h        10.233.74.80   node4\nvote-56bf599b9c-4l6bc     1/1       Running   0          50m       10.233.74.83   node4\nvote-56bf599b9c-bqsrq     1/1       Running   0          50m       10.233.74.82   node4\nvote-56bf599b9c-xw7zc     1/1       Running   0          50m       10.233.74.81   node4\nworker-6cc8dbd4f8-6bkfg   1/1       Running   0          39m       10.233.75.15   node2  Lets cordon one of the nodes and observe.  kubectl cordon node4\nnode/node4 cordoned  Observe the changes  \n$ kubectl get pods -o wide\nNAME                      READY     STATUS    RESTARTS   AGE       IP             NODE\ndb-66496667c9-qggzd       1/1       Running   0          5h        10.233.74.74   node4\nredis-5bf748dbcf-ckn65    1/1       Running   0          43m       10.233.71.26   node3\nredis-5bf748dbcf-vxppx    1/1       Running   0          1h        10.233.74.79   node4\nresult-5c7569bcb7-4fptr   1/1       Running   0          5h        10.233.71.18   node3\nresult-5c7569bcb7-s4rdx   1/1       Running   0          5h        10.233.74.75   node4\nvote-56bf599b9c-22lpw     1/1       Running   0          1h        10.233.74.80   node4\nvote-56bf599b9c-4l6bc     1/1       Running   0          51m       10.233.74.83   node4\nvote-56bf599b9c-bqsrq     1/1       Running   0          51m       10.233.74.82   node4\nvote-56bf599b9c-xw7zc     1/1       Running   0          51m       10.233.74.81   node4\nworker-6cc8dbd4f8-6bkfg   1/1       Running   0          40m       10.233.75.15   node2\n\n\n\n$ kubectl get nodes -o wide\n\n\nNAME      STATUS                     ROLES         AGE       VERSION   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nnode1     Ready                      master,node   1d        v1.10.4   <none>        Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode2     Ready                      master,node   1d        v1.10.4   <none>        Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2\nnode3     Ready                      node          1d        v1.10.4   <none>        Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode4     Ready,SchedulingDisabled   node          1d        v1.10.4   <none>        Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2  Now launch a new deployment and scale it.  kubectl create deployment cordontest --image=busybox --replicas=5\nkubectl scale deploy cordontest --replicas=5\n\nkubectl get pods -o wide  what happened ?   New pods scheduled due the deployment above, do not get launched on the node which is been cordoned off.   $ kubectl uncordon node4\nnode/node4 uncordoned\n\n\n$ kubectl get nodes -o wide\nNAME      STATUS    ROLES         AGE       VERSION   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nnode1     Ready     master,node   1d        v1.10.4   <none>        Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode2     Ready     master,node   1d        v1.10.4   <none>        Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2\nnode3     Ready     node          1d        v1.10.4   <none>        Ubuntu 16.04.4 LTS   4.4.0-130-generic   docker://17.3.2\nnode4     Ready     node          1d        v1.10.4   <none>        Ubuntu 16.04.4 LTS   4.4.0-124-generic   docker://17.3.2  delete the test  deployment  kubectl delete deploy cordontest  Drain a Node  Draining a node will not only mark it unschedulable but also will evict existing pods running on it. Use it with care.  $ kubectl drain node3\nnode/node3 cordoned\nerror: unable to drain node \"node3\", aborting command...\n\nThere are pending nodes to be drained:\n node3\nerror: pods with local storage (use --delete-local-data to override): kubernetes-dashboard-55fdfd74b4-jdgch; DaemonSet-managed pods (use --ignore-daemonsets to ignore): calico-node-4f8xc  Drain with options  kubectl drain node3 --delete-local-data --ignore-daemonsets  Observe the effect,  kubectl get pods -o wide\nkubectl get nodes -o wide  To add the node back to the available schedulable node pool,  kubectl uncordon node4\nnode/node4 uncordoned  Summary  In this lab, we learnt about limiting resource by defining per namespace quota, as well as learnt how to prepare nodes for maintenance by cordoning and draining it.",
            "title": "Nodes  Maintenance"
        },
        {
            "location": "/vault/",
            "text": "Vault and Consul Integration with Kubernetes\n\n\nSetup Consul\n\n\nSwitch to default namespace and run helm to install consul \n\n\nkubectl config set-context --current --namespace=default \nhelm  install --name consul stable/consul\n\n\n\n\n\nSetup vault\n\n\nDownload and run the following script to setup vault client and server. Run this on the host which is running \nkubectl\n. \n\n\n\nwget -c https://gist.githubusercontent.com/initcron/92664e60bef002379ae22c912970cd4d/raw/36121a01780da8a72975969ab2ef7b308b753539/setup_vault_ubuntu_1804.sh\n\nbash setup_vault_ubuntu_1804.sh\n\n\n\n\n\nNote down the vault unseal and root keys. \n\n\nUnseal vault by running the following command thrice, and providing with three different unseal keys. \n\n\nVAULT_ADDR=http://127.0.0.1:8200\n\nvault operator unseal\nvault operator unseal\nvault operator unseal\n\n\n\n\nThis completes vault setup. \n\n\nIntegrate vault with kubernetes\n\n\nDownload vault  demo repo \n\n\ngit clone https://github.com/hashicorp/vault-guides.git\ncd vault-guides/identity/vault-agent-k8s-demo\n\n\n\n\nEdit \nsetup-k8s-auth.sh\n\n\n\n\nuncomment line 27  : vault secrets enable -path=secret kv\n\n\nUpdate Line 48: K8S_HOST=IPADDRESS_MASTER\n\n\nUpdate Line 54: PORT/6443\n\n\n\n\nSetup kubernetes auth method for vault \n\n\nlogin to vault first \n\n\nvault login <ROOT_TOKEN>\n\n\n\n\nreplace <ROOT_TOKEN> with actual token..\n\n\nRun setup script \n\n\n./setup-k8s-auth.sh\n\n\n\n\n\nFetching secrets from the  vault\n\n\nEDIT \nexample-k8s-spec.yml\n\n\n\n\nUpdate line 43 and 74 : IPADDRESS of VAULT \n\n\n\n\nLaunch example pod with \n\n\nkubectl create configmap example-vault-agent-config --from-file=./configs-k8s/\n\nkubectl get configmap example-vault-agent-config -o yaml\n\nkubectl apply -f example-k8s-spec.yml\n\n\n\n\nFind out the IP address of the pod, and test it by running curl \n\n\nkubectl get pods -o wide \n\ncurl IPADDRESS",
            "title": "Integrating Consul and Vault"
        },
        {
            "location": "/vault/#vault-and-consul-integration-with-kubernetes",
            "text": "Setup Consul  Switch to default namespace and run helm to install consul   kubectl config set-context --current --namespace=default \nhelm  install --name consul stable/consul  Setup vault  Download and run the following script to setup vault client and server. Run this on the host which is running  kubectl .   \nwget -c https://gist.githubusercontent.com/initcron/92664e60bef002379ae22c912970cd4d/raw/36121a01780da8a72975969ab2ef7b308b753539/setup_vault_ubuntu_1804.sh\n\nbash setup_vault_ubuntu_1804.sh  Note down the vault unseal and root keys.   Unseal vault by running the following command thrice, and providing with three different unseal keys.   VAULT_ADDR=http://127.0.0.1:8200\n\nvault operator unseal\nvault operator unseal\nvault operator unseal  This completes vault setup.   Integrate vault with kubernetes  Download vault  demo repo   git clone https://github.com/hashicorp/vault-guides.git\ncd vault-guides/identity/vault-agent-k8s-demo  Edit  setup-k8s-auth.sh   uncomment line 27  : vault secrets enable -path=secret kv  Update Line 48: K8S_HOST=IPADDRESS_MASTER  Update Line 54: PORT/6443   Setup kubernetes auth method for vault   login to vault first   vault login <ROOT_TOKEN>  replace <ROOT_TOKEN> with actual token..  Run setup script   ./setup-k8s-auth.sh  Fetching secrets from the  vault  EDIT  example-k8s-spec.yml   Update line 43 and 74 : IPADDRESS of VAULT    Launch example pod with   kubectl create configmap example-vault-agent-config --from-file=./configs-k8s/\n\nkubectl get configmap example-vault-agent-config -o yaml\n\nkubectl apply -f example-k8s-spec.yml  Find out the IP address of the pod, and test it by running curl   kubectl get pods -o wide \n\ncurl IPADDRESS",
            "title": "Vault and Consul Integration with Kubernetes"
        },
        {
            "location": "/13_redis_statefulset/",
            "text": "Deploying Redis Cluster with StatefulSets\n\n\nWhat will you learn  \n\n\n\n\nStatefulsets  \n\n\ninitContainers\n\n\n\n\nRedis Service\n\n\nWe will use Redis as Statefulsets for our Vote application.\nIt is similar to Deployment, but Statefulsets requires a \nService Name\n.\nSo we will create a \nheadless service\n (service without endpoints) first.\n\n\nfile: redis-svc.yml\n\n\nkind: Service\nmetadata:\n  name: redis\n  labels:\n    app: redis\nspec:\n  type: ClusterIP\n  ports:\n  - name: redis\n    port: 6379\n    targetPort: redis\n  clusterIP: None\n  selector:\n    app: redis\n    role: master\n\n\n\n\napply\n\n\nkubectl apply -f redis-svc.yml\n\n\n\n\nNote: clusterIP's value is set to None\n.\n\n\nRedis ConfigMap\n\n\nRedis ConfigMap has two sections.\n  * master.conf - for Redis master\n  * slave.conf - for Redis slave\n\n\nfile: redis-cm.yml\n\n\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: redis\ndata:\n  master.conf: |\n    bind 0.0.0.0\n    protected-mode yes\n    port 6379\n    tcp-backlog 511\n    timeout 0\n    tcp-keepalive 300\n    daemonize no\n    supervised no\n    pidfile /var/run/redis_6379.pid\n    loglevel notice\n    logfile \"\"\n  slave.conf: |\n    slaveof redis-0.redis 6379\n\n\n\n\napply\n\n\nkubectl apply -f redis-svc.yml\n\n\n\n\nRedis initContainers\n\n\nWe have to deploy redis master/slave set up from one statefulset cluster. This requires two different redis cofigurations , which needs to be described in one Pod template. This complexity can be resolved by using init containers. These init containers copy the appropriate redis configuration by analysing the hostname of the pod. If the Pod's (host)name has \n0\n as \nOrdinal number\n, then it is choosen as the master and master.conf is copied to /etc/ directory. Other Pods will get slave.conf as configuration.\n\n\nfile: redis-sts.yml\n\n\n[...]\n      initContainers:\n      - name: init-redis\n        image: redis:4.0.9\n        command:\n        - bash\n        - \"-c\"\n        - |\n          set -ex\n          # Generate mysql server-id from pod ordinal index.\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n          ordinal=${BASH_REMATCH[1]}\n          # Copy appropriate conf.d files from config-map to emptyDir.\n          if [[ $ordinal -eq 0 ]]; then\n            cp /mnt/config-map/master.conf /etc/redis.conf\n          else\n            cp /mnt/config-map/slave.conf /etc/redis.conf\n          fi\n        volumeMounts:\n        - name: conf\n          mountPath: /etc\n          subPath: redis.conf\n        - name: config-map\n          mountPath: /mnt/config-map\n\n\n\n\nRedis Statefulsets\n\n\nThese redis containers are started after initContainers are succefully ran. One thing to note here, these containers mount the same volume, \nconf\n, from the initContainers which has the proper Redis configuration.\n\n\nfile: redis-sts.yaml\n\n\n[...]\n      containers:\n      - name: redis\n        image: redis:4.0.9\n        command: [\"redis-server\"]\n        args: [\"/etc/redis.conf\"]\n        env:\n        - name: ALLOW_EMPTY_PASSWORD\n          value: \"yes\"\n        ports:\n        - name: redis\n          containerPort: 6379\n        volumeMounts:\n        - name: redis-data\n          mountPath: /data\n        - name: conf\n          mountPath: /etc/\n          subPath: redis.conf\n\n\n\n\nTo apply\n\n\nkubectl apply -f redis-sts.yml\n\n\n\n\nReading List\n\n\n\n\nRedis Replication\n\n\nRun Replicated Statefulsets Applications\n\n\nInit Containers\n\n\n\n\nSearch Keywords\n\n\n\n\ninit containers\n\n\nkubernetes statefulsets\n\n\nredis replication",
            "title": "Creating Replicated Redis Cluster with Statefulsets"
        },
        {
            "location": "/13_redis_statefulset/#deploying-redis-cluster-with-statefulsets",
            "text": "What will you learn     Statefulsets    initContainers   Redis Service  We will use Redis as Statefulsets for our Vote application.\nIt is similar to Deployment, but Statefulsets requires a  Service Name .\nSo we will create a  headless service  (service without endpoints) first.  file: redis-svc.yml  kind: Service\nmetadata:\n  name: redis\n  labels:\n    app: redis\nspec:\n  type: ClusterIP\n  ports:\n  - name: redis\n    port: 6379\n    targetPort: redis\n  clusterIP: None\n  selector:\n    app: redis\n    role: master  apply  kubectl apply -f redis-svc.yml  Note: clusterIP's value is set to None .  Redis ConfigMap  Redis ConfigMap has two sections.\n  * master.conf - for Redis master\n  * slave.conf - for Redis slave  file: redis-cm.yml  apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: redis\ndata:\n  master.conf: |\n    bind 0.0.0.0\n    protected-mode yes\n    port 6379\n    tcp-backlog 511\n    timeout 0\n    tcp-keepalive 300\n    daemonize no\n    supervised no\n    pidfile /var/run/redis_6379.pid\n    loglevel notice\n    logfile \"\"\n  slave.conf: |\n    slaveof redis-0.redis 6379  apply  kubectl apply -f redis-svc.yml  Redis initContainers  We have to deploy redis master/slave set up from one statefulset cluster. This requires two different redis cofigurations , which needs to be described in one Pod template. This complexity can be resolved by using init containers. These init containers copy the appropriate redis configuration by analysing the hostname of the pod. If the Pod's (host)name has  0  as  Ordinal number , then it is choosen as the master and master.conf is copied to /etc/ directory. Other Pods will get slave.conf as configuration.  file: redis-sts.yml  [...]\n      initContainers:\n      - name: init-redis\n        image: redis:4.0.9\n        command:\n        - bash\n        - \"-c\"\n        - |\n          set -ex\n          # Generate mysql server-id from pod ordinal index.\n          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n          ordinal=${BASH_REMATCH[1]}\n          # Copy appropriate conf.d files from config-map to emptyDir.\n          if [[ $ordinal -eq 0 ]]; then\n            cp /mnt/config-map/master.conf /etc/redis.conf\n          else\n            cp /mnt/config-map/slave.conf /etc/redis.conf\n          fi\n        volumeMounts:\n        - name: conf\n          mountPath: /etc\n          subPath: redis.conf\n        - name: config-map\n          mountPath: /mnt/config-map  Redis Statefulsets  These redis containers are started after initContainers are succefully ran. One thing to note here, these containers mount the same volume,  conf , from the initContainers which has the proper Redis configuration.  file: redis-sts.yaml  [...]\n      containers:\n      - name: redis\n        image: redis:4.0.9\n        command: [\"redis-server\"]\n        args: [\"/etc/redis.conf\"]\n        env:\n        - name: ALLOW_EMPTY_PASSWORD\n          value: \"yes\"\n        ports:\n        - name: redis\n          containerPort: 6379\n        volumeMounts:\n        - name: redis-data\n          mountPath: /data\n        - name: conf\n          mountPath: /etc/\n          subPath: redis.conf  To apply  kubectl apply -f redis-sts.yml  Reading List   Redis Replication  Run Replicated Statefulsets Applications  Init Containers   Search Keywords   init containers  kubernetes statefulsets  redis replication",
            "title": "Deploying Redis Cluster with StatefulSets"
        },
        {
            "location": "/vote-deployement_strategies/",
            "text": "Release Strategies\n\n\nReleases with downtime using Recreate Strategy\n\n\nWhen the \nRecreate\n deployment strategy is used,\n  * The old pods will be deleted\n  * Then the new pods will be created.\n\n\nThis will create some downtime in our stack.\n\n\nLet us change the deployment strategy to \nrecreate\n and image tag to \nv4\n.\n\n\ncp vote-deploy.yaml vote-deploy-recreate.yaml\n\n\n\n\nAnd edit the specs with following changes\n\n\n\n\nUpdate strategy to \nRecreate\n\n\nRemove rolling update specs\n\n\n\n\nfile: vote-deploy-recreate.yaml\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote\nspec:\n  strategy:\n    type: Recreate\n  revisionHistoryLimit: 4\n  paused: false\n  replicas: 15\n  minReadySeconds: 20\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3, v4]}\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v4\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v4\n          ports:\n            - containerPort: 80\n              protocol: TCP\n\n\n\n\nand apply\n\n\nkubectl get pods,rs,deploy,svc\n\nkubectl apply -f vote-deploy-recreate.yaml\n\nkubectl rollout status deplloyment/vote\n\n\n\n\n\nWhile the deployment happens, use the monitoring/visualiser and observe the manner in which the deployment gets updated.\n\n\nYou would observe that\n\n\n\n\nAll pods wit the current version are deleted first\n\n\nOnly after all the existing pods are deleted, pods with new version are launched\n\n\n\n\nCanary  Releases\n\n\ncd k8s-code/projets/instavote/dev\nmkdir canary\ncp vote-deploy.yaml canary/vote-canary-deploy.yaml\n\n\n\n\n\nchange the following fields in \nvote-canary-deploy.yaml\n\n\n\n\nmetadata.name: vote-canary\n\n\nspec.replicas: 3\n\n\nspec.selector.matchExpressions: - {key: version, operator: In, values: [v1, v2, v3, v4]}\n\n\ntemplate.metadata.labels.version: v4\n\n\ntemplate.spec.containers.image: schoolofdevops/vote:v4\n\n\n\n\nFile: canary/frontend-canary-deploy.yml\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote-canary\nspec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2\n      maxUnavailable: 1\n  revisionHistoryLimit: 4\n  paused: false\n  replicas: 3\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3, v4, v5]}\n  minReadySeconds: 40\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v4\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v4\n          ports:\n            - containerPort: 80\n              protocol: TCP\n\n\n\n\nBefore creating this deployment, find out how many endpoints the service has,\n\n\nkubectl describe service/vote\n\n\n\n\n[sample output ]\n\n\nEndpoints:                10.32.0.10:80,10.32.0.11:80,10.32.0.4:80 + 12 more...\n\n\n\n\nIn this example current endpoints are \n15\n\n\nNow create the  deployment for canary release\n\n\n\nkubectl apply -f canary/frontend-canary-deploy.yml\n\n\n\n\n\nAnd validate,\n\n\nkubectl get rs,deploy,svc\n\nkubectl describe service/vote\n\n\n\n\nWhen you describe vote service, observe the number of endpoints\n\n\n[sample output]\n\n\nEndpoints:                10.32.0.10:80,10.32.0.11:80,10.32.0.16:80 + 15 more...\n\n\n\n\nNow its \n18\n, which is 3 more than the previous number. Those are the pods created by the canary deployment. And the above output proves that its actually sending traffic to both versions.\n\n\nDelete Canary\n\n\nOnce validated, you could clean up canary release using\n\n\nkubectl delete -f canary/vote-canary-deploy.yaml\n\n\n\n\nBlue/Green  Releases\n\n\nBefore proceeding, lets clean up the existing deployment.\n\n\nkubectl delete deploy/vote\nkubectl delete svc/vote\nkubectl get pods,deploy,rs,svc\n\n\n\n\n\nAnd create the work directory for blue-green release definitions.\n\n\ncd k8s-code/projets/instavote/dev\nmkdir blue-green\n\n\n\n\n\nfile: blue-green/vote-blue-deploy.yaml\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote-blue\nspec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2\n      maxUnavailable: 1\n  revisionHistoryLimit: 4\n  paused: false\n  replicas: 15\n  minReadySeconds: 20\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3]}\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v3\n        release: bluegreen\n        code: blue\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v3\n          ports:\n            - containerPort: 80\n              protocol: TCP\n\n\n\n\nfile: blue-green/vote-green-deploy.yaml\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote-green\nspec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2\n      maxUnavailable: 1\n  revisionHistoryLimit: 4\n  paused: false\n  replicas: 15\n  minReadySeconds: 20\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3, v4]}\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v3\n        release: bluegreen\n        code: green\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v4\n          ports:\n            - containerPort: 80\n              protocol: TCP\n\n\n\n\n\nfile: blue-green/vote-bg-svc.yaml\n\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vote-bg\n  labels:\n    role: vote\n    release: bluegreen\nspec:\n  selector:\n    role: vote\n    release: bluegreen\n    code: green\n  ports:\n    - port: 80\n      targetPort: 80\n      nodePort: 30001\n  type: NodePort\n\n\n\n\n\nfile: vote-svc.yaml\n\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vote\n  labels:\n    role: vote\nspec:\n  selector:\n    role: vote\n    release: bluegreen\n    code: blue\n  ports:\n    - port: 80\n      targetPort: 80\n      nodePort: 30000\n  type: NodePort\n\n\n\n\nCreating  blue deployment\n\n\nNow create \nvote\n service and observe the endpoints\n\n\nkubectl apply -f vote-svc.yaml\nkubectl get svc\nkubectl describe svc/vote\n\n\n\n\n[sample output]\n\n\nName:                     vote\nNamespace:                instavote\nLabels:                   role=vote\nAnnotations:              kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"role\":\"vote\"},\"name\":\"vote\",\"namespace\":\"instavote\"},\"spec\":{\"externalIPs\":...\nSelector:                 code=blue,release=bluegreen,role=vote\nType:                     NodePort\nIP:                       10.111.93.227\nExternal IPs:             206.189.150.190,159.65.8.227\nPort:                     <unset>  80/TCP\nTargetPort:               80/TCP\nNodePort:                 <unset>  30000/TCP\nEndpoints:                <none>\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   <none>\n\n\n\n\nwhere,\n  * \nendpoints\n are \nNone\n\n  * its selecting pods with \ncode=blue\n\n\nNow lets create the deployment for \nblue\n release\n\n\nkubectl get pods,rs,deploy\nkubectl apply -f blue-green/vote-blue-deploy.yaml\nkubectl get pods,rs,deploy\nkubectl rollout status deploy/vote-blue\n\n\n\n\n[sample output]\n\n\nWaiting for rollout to finish: 2 of 15 updated replicas are available...\ndeployment \"vote-blue\" successfully rolled out\n\n\n\n\nNow if you check the service, it should have the pods launched with blue set as endpoints\n\n\nkubectl describe svc/vote\n\nName:                     vote\nNamespace:                instavote\nLabels:                   role=vote\nAnnotations:              kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"role\":\"vote\"},\"name\":\"vote\",\"namespace\":\"instavote\"},\"spec\":{\"externalIPs\":...\nSelector:                 code=blue,release=bluegreen,role=vote\nType:                     NodePort\nIP:                       10.111.93.227\nExternal IPs:             206.189.150.190,159.65.8.227\nPort:                     <unset>  80/TCP\nTargetPort:               80/TCP\nNodePort:                 <unset>  30000/TCP\nEndpoints:                10.32.0.10:80,10.32.0.11:80,10.32.0.4:80 + 12 more...\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   <none>\n\n\n\n\n\nYou could observe the \nEndpoints\n created and added to the service.  Browse to http://IPADDRESS:NODEPORT to see the application deployed.\n\n\n\n\nDeploying new version with green release\n\n\nWhile deploying a new version with blue-green strategy, we would\n\n\n\n\nCreate a new deployment in parallel\n\n\nTest it by creating another service\n\n\nCut over to new release by updating selector in the main service\n\n\n\n\nLets create the deployment with new version and a service to test it. Lets call it the \ngreen\n deployment  \n\n\nkubectl apply -f blue-green/vote-bg-svc.yaml\nkubectl apply -f blue-green/vote-bg-svc.yaml\nkubectl apply -f blue-green/vote-green-deploy.yaml\nkubectl rollout status deploy/vote-green\n\n\n\n\n\n[sample output]\n\n\nWaiting for rollout to finish: 0 of 15 updated replicas are available...\nWaiting for rollout to finish: 0 of 15 updated replicas are available...\nWaiting for rollout to finish: 0 of 15 updated replicas are available...\nWaiting for rollout to finish: 0 of 15 updated replicas are available...\nWaiting for rollout to finish: 7 of 15 updated replicas are available...\ndeployment \"vote-green\" successfully rolled out\n\n\n\n\nValidate\n\n\nkubectl get pods,rs,deploy,svc\n\n\n\n\nYou could also test it by going to the http://host:nodeport for service \nvote-bg\n\n\nSwitching to new version\n\n\nNow that you  have the new version running in parallel, you could quickly switch to it by updating selector for main \nvote\n service which is live. Please note, while switching there may be a momentory downtime.\n\n\nSteps\n\n\n\n\nvisit http://HOST:NODEPORT for \nvote\n service\n\n\nupdate \nvote\n service to select \ngreen\n release\n\n\napply service definition\n\n\nvisit http://HOST:NODEPORT for \nvote\n service again to validate\n\n\n\n\nfile: vote-svc.yaml\n\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vote\n  labels:\n    role: vote\nspec:\n  selector:\n    role: vote\n    release: bluegreen\n    code: green\n  ports:\n    - port: 80\n      targetPort: 80\n      nodePort: 30000\n  type: NodePort\n\n\n\n\n\nApply it with\n\n\nkubectl apply -f vote-svc.yaml\n\nkubectl describe svc/vote\n\n\n\n\nIf you visit http://HOST:NODEPORT for \nvote\n service, you should see the application version updated\n\n\n\n\nClean up the previous version\n\n\nkubectl delete deploy/vote-blue\n\n\n\n\n\nClean up blue-green configs\n\n\nNow that you are done testing blue green release, lets revert to our previous configurations.\n\n\nkubectl delete deploy/vote-green\nkubectl apply -f vote-deploy.yaml\n\n\n\n\n\nAlso update the service definition and remove following  selectors added for blue green release\n\n\n\n\nrelease: bluegreen\n\n\ncode: blue\n\n\n\n\nfile: vote-svc.yaml\n\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vote\n  labels:\n    role: vote\nspec:\n  selector:\n    role: vote\n  ports:\n    - port: 80\n      targetPort: 80\n      nodePort: 30000\n  type: NodePort\n\n\n\n\nAnd apply\n\n\nkubectl apply -f vote-svc.yaml\n\n\n\n\nPause/Unpause\n\n\nWhen you are in the middle of a new update for your application and you found out that the application is behaving as intended. In those situations,\n  1. we can pause the update,\n  2. fix the issue,\n  3. resume the update.\n\n\nLet us change the image tag to V4 in pod spec.\n\n\nFile: vote-deploy.yaml\n\n\n    spec:\n       containers:\n         - name: app\n           image: schoolofdevops/vote:V4\n           ports:\n             - containerPort: 80\n               protocol: TCP\n\n\n\n\nApply the changes.\n\n\nkubectl apply -f vote-deploy.yaml\n\nkubectl get pods\n\n[Output]\nNAME                         READY     STATUS         RESTARTS   AGE\nvote-6c4f7b49d8-g5dgc   1/1       Running        0          16m\nvote-765554cc7-xsbhs    0/1       ErrImagePull   0          9s\n\n\n\n\nOur deployment is failing. From some debugging, we can conclude that we are using a wrong image tag.\n\n\nNow pause the update\n\n\nkubectl rollout pause deploy/vote\n\n\n\n\nSet the deployment to use \nv4\n version of the image.\n\n\nNow resume the update\n\n\nkubectl rollout resume deployment vote\nkubectl rollout status deployment vote\n\n[Ouput]\ndeployment \"vote\" successfully rolled out\n\n\n\n\nand validate\n\n\nkubectl get pods,rs,deploy\n\n[Output]\nNAME                         READY     STATUS    RESTARTS   AGE\nvote-6875c8df8f-k4hls   1/1       Running   0          1m\n\n\n\n\nWhen you do this, you skip the need of creating a new rolling update altogether.",
            "title": "Building Deployment Strategies"
        },
        {
            "location": "/vote-deployement_strategies/#release-strategies",
            "text": "",
            "title": "Release Strategies"
        },
        {
            "location": "/vote-deployement_strategies/#releases-with-downtime-using-recreate-strategy",
            "text": "When the  Recreate  deployment strategy is used,\n  * The old pods will be deleted\n  * Then the new pods will be created.  This will create some downtime in our stack.  Let us change the deployment strategy to  recreate  and image tag to  v4 .  cp vote-deploy.yaml vote-deploy-recreate.yaml  And edit the specs with following changes   Update strategy to  Recreate  Remove rolling update specs   file: vote-deploy-recreate.yaml  apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote\nspec:\n  strategy:\n    type: Recreate\n  revisionHistoryLimit: 4\n  paused: false\n  replicas: 15\n  minReadySeconds: 20\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3, v4]}\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v4\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v4\n          ports:\n            - containerPort: 80\n              protocol: TCP  and apply  kubectl get pods,rs,deploy,svc\n\nkubectl apply -f vote-deploy-recreate.yaml\n\nkubectl rollout status deplloyment/vote  While the deployment happens, use the monitoring/visualiser and observe the manner in which the deployment gets updated.  You would observe that   All pods wit the current version are deleted first  Only after all the existing pods are deleted, pods with new version are launched",
            "title": "Releases with downtime using Recreate Strategy"
        },
        {
            "location": "/vote-deployement_strategies/#canary-releases",
            "text": "cd k8s-code/projets/instavote/dev\nmkdir canary\ncp vote-deploy.yaml canary/vote-canary-deploy.yaml  change the following fields in  vote-canary-deploy.yaml   metadata.name: vote-canary  spec.replicas: 3  spec.selector.matchExpressions: - {key: version, operator: In, values: [v1, v2, v3, v4]}  template.metadata.labels.version: v4  template.spec.containers.image: schoolofdevops/vote:v4   File: canary/frontend-canary-deploy.yml  apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote-canary\nspec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2\n      maxUnavailable: 1\n  revisionHistoryLimit: 4\n  paused: false\n  replicas: 3\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3, v4, v5]}\n  minReadySeconds: 40\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v4\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v4\n          ports:\n            - containerPort: 80\n              protocol: TCP  Before creating this deployment, find out how many endpoints the service has,  kubectl describe service/vote  [sample output ]  Endpoints:                10.32.0.10:80,10.32.0.11:80,10.32.0.4:80 + 12 more...  In this example current endpoints are  15  Now create the  deployment for canary release  \nkubectl apply -f canary/frontend-canary-deploy.yml  And validate,  kubectl get rs,deploy,svc\n\nkubectl describe service/vote  When you describe vote service, observe the number of endpoints  [sample output]  Endpoints:                10.32.0.10:80,10.32.0.11:80,10.32.0.16:80 + 15 more...  Now its  18 , which is 3 more than the previous number. Those are the pods created by the canary deployment. And the above output proves that its actually sending traffic to both versions.  Delete Canary  Once validated, you could clean up canary release using  kubectl delete -f canary/vote-canary-deploy.yaml",
            "title": "Canary  Releases"
        },
        {
            "location": "/vote-deployement_strategies/#bluegreen-releases",
            "text": "Before proceeding, lets clean up the existing deployment.  kubectl delete deploy/vote\nkubectl delete svc/vote\nkubectl get pods,deploy,rs,svc  And create the work directory for blue-green release definitions.  cd k8s-code/projets/instavote/dev\nmkdir blue-green  file: blue-green/vote-blue-deploy.yaml  apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote-blue\nspec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2\n      maxUnavailable: 1\n  revisionHistoryLimit: 4\n  paused: false\n  replicas: 15\n  minReadySeconds: 20\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3]}\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v3\n        release: bluegreen\n        code: blue\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v3\n          ports:\n            - containerPort: 80\n              protocol: TCP  file: blue-green/vote-green-deploy.yaml  apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote-green\nspec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 2\n      maxUnavailable: 1\n  revisionHistoryLimit: 4\n  paused: false\n  replicas: 15\n  minReadySeconds: 20\n  selector:\n    matchLabels:\n      role: vote\n    matchExpressions:\n      - {key: version, operator: In, values: [v1, v2, v3, v4]}\n  template:\n    metadata:\n      name: vote\n      labels:\n        app: python\n        role: vote\n        version: v3\n        release: bluegreen\n        code: green\n    spec:\n      containers:\n        - name: app\n          image: schoolofdevops/vote:v4\n          ports:\n            - containerPort: 80\n              protocol: TCP  file: blue-green/vote-bg-svc.yaml  ---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vote-bg\n  labels:\n    role: vote\n    release: bluegreen\nspec:\n  selector:\n    role: vote\n    release: bluegreen\n    code: green\n  ports:\n    - port: 80\n      targetPort: 80\n      nodePort: 30001\n  type: NodePort  file: vote-svc.yaml  ---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vote\n  labels:\n    role: vote\nspec:\n  selector:\n    role: vote\n    release: bluegreen\n    code: blue\n  ports:\n    - port: 80\n      targetPort: 80\n      nodePort: 30000\n  type: NodePort  Creating  blue deployment  Now create  vote  service and observe the endpoints  kubectl apply -f vote-svc.yaml\nkubectl get svc\nkubectl describe svc/vote  [sample output]  Name:                     vote\nNamespace:                instavote\nLabels:                   role=vote\nAnnotations:              kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"role\":\"vote\"},\"name\":\"vote\",\"namespace\":\"instavote\"},\"spec\":{\"externalIPs\":...\nSelector:                 code=blue,release=bluegreen,role=vote\nType:                     NodePort\nIP:                       10.111.93.227\nExternal IPs:             206.189.150.190,159.65.8.227\nPort:                     <unset>  80/TCP\nTargetPort:               80/TCP\nNodePort:                 <unset>  30000/TCP\nEndpoints:                <none>\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   <none>  where,\n  *  endpoints  are  None \n  * its selecting pods with  code=blue  Now lets create the deployment for  blue  release  kubectl get pods,rs,deploy\nkubectl apply -f blue-green/vote-blue-deploy.yaml\nkubectl get pods,rs,deploy\nkubectl rollout status deploy/vote-blue  [sample output]  Waiting for rollout to finish: 2 of 15 updated replicas are available...\ndeployment \"vote-blue\" successfully rolled out  Now if you check the service, it should have the pods launched with blue set as endpoints  kubectl describe svc/vote\n\nName:                     vote\nNamespace:                instavote\nLabels:                   role=vote\nAnnotations:              kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"labels\":{\"role\":\"vote\"},\"name\":\"vote\",\"namespace\":\"instavote\"},\"spec\":{\"externalIPs\":...\nSelector:                 code=blue,release=bluegreen,role=vote\nType:                     NodePort\nIP:                       10.111.93.227\nExternal IPs:             206.189.150.190,159.65.8.227\nPort:                     <unset>  80/TCP\nTargetPort:               80/TCP\nNodePort:                 <unset>  30000/TCP\nEndpoints:                10.32.0.10:80,10.32.0.11:80,10.32.0.4:80 + 12 more...\nSession Affinity:         None\nExternal Traffic Policy:  Cluster\nEvents:                   <none>  You could observe the  Endpoints  created and added to the service.  Browse to http://IPADDRESS:NODEPORT to see the application deployed.   Deploying new version with green release  While deploying a new version with blue-green strategy, we would   Create a new deployment in parallel  Test it by creating another service  Cut over to new release by updating selector in the main service   Lets create the deployment with new version and a service to test it. Lets call it the  green  deployment    kubectl apply -f blue-green/vote-bg-svc.yaml\nkubectl apply -f blue-green/vote-bg-svc.yaml\nkubectl apply -f blue-green/vote-green-deploy.yaml\nkubectl rollout status deploy/vote-green  [sample output]  Waiting for rollout to finish: 0 of 15 updated replicas are available...\nWaiting for rollout to finish: 0 of 15 updated replicas are available...\nWaiting for rollout to finish: 0 of 15 updated replicas are available...\nWaiting for rollout to finish: 0 of 15 updated replicas are available...\nWaiting for rollout to finish: 7 of 15 updated replicas are available...\ndeployment \"vote-green\" successfully rolled out  Validate  kubectl get pods,rs,deploy,svc  You could also test it by going to the http://host:nodeport for service  vote-bg  Switching to new version  Now that you  have the new version running in parallel, you could quickly switch to it by updating selector for main  vote  service which is live. Please note, while switching there may be a momentory downtime.  Steps   visit http://HOST:NODEPORT for  vote  service  update  vote  service to select  green  release  apply service definition  visit http://HOST:NODEPORT for  vote  service again to validate   file: vote-svc.yaml  ---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vote\n  labels:\n    role: vote\nspec:\n  selector:\n    role: vote\n    release: bluegreen\n    code: green\n  ports:\n    - port: 80\n      targetPort: 80\n      nodePort: 30000\n  type: NodePort  Apply it with  kubectl apply -f vote-svc.yaml\n\nkubectl describe svc/vote  If you visit http://HOST:NODEPORT for  vote  service, you should see the application version updated   Clean up the previous version  kubectl delete deploy/vote-blue  Clean up blue-green configs  Now that you are done testing blue green release, lets revert to our previous configurations.  kubectl delete deploy/vote-green\nkubectl apply -f vote-deploy.yaml  Also update the service definition and remove following  selectors added for blue green release   release: bluegreen  code: blue   file: vote-svc.yaml  ---\napiVersion: v1\nkind: Service\nmetadata:\n  name: vote\n  labels:\n    role: vote\nspec:\n  selector:\n    role: vote\n  ports:\n    - port: 80\n      targetPort: 80\n      nodePort: 30000\n  type: NodePort  And apply  kubectl apply -f vote-svc.yaml  Pause/Unpause  When you are in the middle of a new update for your application and you found out that the application is behaving as intended. In those situations,\n  1. we can pause the update,\n  2. fix the issue,\n  3. resume the update.  Let us change the image tag to V4 in pod spec.  File: vote-deploy.yaml      spec:\n       containers:\n         - name: app\n           image: schoolofdevops/vote:V4\n           ports:\n             - containerPort: 80\n               protocol: TCP  Apply the changes.  kubectl apply -f vote-deploy.yaml\n\nkubectl get pods\n\n[Output]\nNAME                         READY     STATUS         RESTARTS   AGE\nvote-6c4f7b49d8-g5dgc   1/1       Running        0          16m\nvote-765554cc7-xsbhs    0/1       ErrImagePull   0          9s  Our deployment is failing. From some debugging, we can conclude that we are using a wrong image tag.  Now pause the update  kubectl rollout pause deploy/vote  Set the deployment to use  v4  version of the image.  Now resume the update  kubectl rollout resume deployment vote\nkubectl rollout status deployment vote\n\n[Ouput]\ndeployment \"vote\" successfully rolled out  and validate  kubectl get pods,rs,deploy\n\n[Output]\nNAME                         READY     STATUS    RESTARTS   AGE\nvote-6875c8df8f-k4hls   1/1       Running   0          1m  When you do this, you skip the need of creating a new rolling update altogether.",
            "title": "Blue/Green  Releases"
        },
        {
            "location": "/12_troubleshooting/",
            "text": "Troubleshooting the Kubernetes cluster\n\n\nIn this chapter we will learn about how to trouble shoot our Kubernetes cluster at \ncontrol plane\n level and at \napplication level\n.\n\n\nTroubleshooting the control plane\n\n\nListing the nodes in a cluster\n\n\nFirst thing to check if your cluster is working fine or not is to list the nodes associated with your cluster.\n\n\nkubectl get nodes\n\n\n\n\nMake sure that all nodes are in \nReady\n state.\n\n\nList the control plane pods\n\n\nIf your nodes are up and running, next thing to check is the status of Kubernetes components.\nRun,\n\n\nkubectl get pods -n kube-system\n\n\n\n\nIf any of the pod is \nrestarting or crashing\n, look in to the issue.\nThis can be done by getting the pod's description.\nFor example, in my cluster \nkube-dns\n is crashing. In order to fix this first check the deployment for errors.\n\n\nkubectl describe deployment -n kube-system kube-dns\n\n\n\n\nLog files\n\n\nMaster\n\n\nIf your deployment is good, the next thing to look for is log files.\nThe locations of log files are given below...\n\n\n/var/log/kube-apiserver.log - For API Server logs\n/var/log/kube-scheduler.log - For Scheduler logs\n/var/log/kube-controller-manager.log - For Replication Controller logs\n\n\n\n\nIf your Kubernetes components are running as pods, then you can get their logs by following the steps given below,\nKeep in mind that the \nactual pod's name may differ\n from cluster to cluster...\n\n\nkubectl logs -n kube-system -f kube-apiserver-node1\nkubectl logs -n kube-system -f kube-scheduler-node1\nkubectl logs -n kube-system -f kube-controller-manager-node1\n\n\n\n\nWorker Nodes\n\n\nIn your worker, you will need to check for errors in kubelet's log...\n\n\nsudo journalctl -u  kubelet\n\n\n\n\nTroubleshooting the application\n\n\nSometimes your application(pod) may fail to start because of various reasons. Let's see how to troubleshoot.\n\n\nGetting detailed status of an object (pods, deployments)\n\n\nobject.status\n shows a detailed information about whats the status of an object ( e.g. pod) and why its in that condition. This can be very useful to identify the issues.\n\n\nExample\n\n\nkubectl get pod vote -o yaml\n\n\n\n\n\nexample output snippet when a wrong image was used to create a pod. \n\n\nstatus:\n...\ncontainerStatuses:\n....\nstate:\n  waiting:\n    message: 'rpc error: code = Unknown desc = Error response from daemon: manifest\n      for schoolofdevops/vote:latst not found'\n    reason: ErrImagePull\nhostIP: 139.59.232.248\n\n\n\n\nChecking the status of Deployment\n\n\nFor this example I have a sample deployment called nginx.\n\n\nFILE: nginx-deployment.yml\n\n\napiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: ngnix:latest\n          ports:\n            - containerPort: 80\n\n\n\n\nList the deployment to check for the \navailability of pods\n\n\nkubectl get deployment nginx\n\nNAME          DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnginx         1         1         1            0           20h\n\n\n\n\nIt is clear that my pod is unavailable. Lets dig further.\n\n\nCheck the \nevents\n of your deployment.\n\n\nkubectl describe deployment nginx\n\n\n\n\nList the pods to check for any \nregistry related error\n\n\nkubectl get pods\n\nNAME                           READY     STATUS             RESTARTS   AGE\nnginx-57c88d7bb8-c6kpc         0/1       ImagePullBackOff   0          7m\n\n\n\n\nAs we can see, we are not able to pull the image(\nImagePullBackOff\n). Let's investigate further.\n\n\nkubectl describe pod nginx-57c88d7bb8-c6kpc\n\n\n\n\nCheck the events of the pod's description.\n\n\nEvents:\n  Type     Reason                 Age               From                                               Message\n  ----     ------                 ----              ----                                               -------\n  Normal   Scheduled              9m                default-scheduler                                  Successfully assigned nginx-57c88d7bb8-c6kpc to ip-11-0-1-111.us-west-2.compute.internal\n  Normal   SuccessfulMountVolume  9m                kubelet, ip-11-0-1-111.us-west-2.compute.internal  MountVolume.SetUp succeeded for volume \"default-token-8cwn4\"\n  Normal   Pulling                8m (x4 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  pulling image \"ngnix\"\n  Warning  Failed                 8m (x4 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  Failed to pull image \"ngnix\": rpc error: code = Unknown desc = Error response from daemon: repository ngnix not found: does not exist or no pull access\n  Normal   BackOff                7m (x6 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  Back-off pulling image \"ngnix\"\n  Warning  FailedSync             4m (x24 over 9m)  kubelet, ip-11-0-1-111.us-west-2.compute.internal  Error syncing pod\n\n\n\n\nBingo! The name of the image is \nngnix\n instead of \nnginx\n. So \nfix the typo in your deployment file and redo the deployment\n.\n\n\nSometimes, your application(pod) may fail to start because of some configuration issues. For those errors, we can follow the logs of the pod.\n\n\nkubectl logs -f nginx-57c88d7bb8-c6kpc\n\n\n\n\nIf you have any errors it will get populated in your logs.",
            "title": "Troubleshooting Tips"
        },
        {
            "location": "/12_troubleshooting/#troubleshooting-the-kubernetes-cluster",
            "text": "In this chapter we will learn about how to trouble shoot our Kubernetes cluster at  control plane  level and at  application level .  Troubleshooting the control plane  Listing the nodes in a cluster  First thing to check if your cluster is working fine or not is to list the nodes associated with your cluster.  kubectl get nodes  Make sure that all nodes are in  Ready  state.  List the control plane pods  If your nodes are up and running, next thing to check is the status of Kubernetes components.\nRun,  kubectl get pods -n kube-system  If any of the pod is  restarting or crashing , look in to the issue.\nThis can be done by getting the pod's description.\nFor example, in my cluster  kube-dns  is crashing. In order to fix this first check the deployment for errors.  kubectl describe deployment -n kube-system kube-dns  Log files  Master  If your deployment is good, the next thing to look for is log files.\nThe locations of log files are given below...  /var/log/kube-apiserver.log - For API Server logs\n/var/log/kube-scheduler.log - For Scheduler logs\n/var/log/kube-controller-manager.log - For Replication Controller logs  If your Kubernetes components are running as pods, then you can get their logs by following the steps given below,\nKeep in mind that the  actual pod's name may differ  from cluster to cluster...  kubectl logs -n kube-system -f kube-apiserver-node1\nkubectl logs -n kube-system -f kube-scheduler-node1\nkubectl logs -n kube-system -f kube-controller-manager-node1  Worker Nodes  In your worker, you will need to check for errors in kubelet's log...  sudo journalctl -u  kubelet  Troubleshooting the application  Sometimes your application(pod) may fail to start because of various reasons. Let's see how to troubleshoot.  Getting detailed status of an object (pods, deployments)  object.status  shows a detailed information about whats the status of an object ( e.g. pod) and why its in that condition. This can be very useful to identify the issues.  Example  kubectl get pod vote -o yaml  example output snippet when a wrong image was used to create a pod.   status:\n...\ncontainerStatuses:\n....\nstate:\n  waiting:\n    message: 'rpc error: code = Unknown desc = Error response from daemon: manifest\n      for schoolofdevops/vote:latst not found'\n    reason: ErrImagePull\nhostIP: 139.59.232.248  Checking the status of Deployment  For this example I have a sample deployment called nginx.  FILE: nginx-deployment.yml  apiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: ngnix:latest\n          ports:\n            - containerPort: 80  List the deployment to check for the  availability of pods  kubectl get deployment nginx\n\nNAME          DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\nnginx         1         1         1            0           20h  It is clear that my pod is unavailable. Lets dig further.  Check the  events  of your deployment.  kubectl describe deployment nginx  List the pods to check for any  registry related error  kubectl get pods\n\nNAME                           READY     STATUS             RESTARTS   AGE\nnginx-57c88d7bb8-c6kpc         0/1       ImagePullBackOff   0          7m  As we can see, we are not able to pull the image( ImagePullBackOff ). Let's investigate further.  kubectl describe pod nginx-57c88d7bb8-c6kpc  Check the events of the pod's description.  Events:\n  Type     Reason                 Age               From                                               Message\n  ----     ------                 ----              ----                                               -------\n  Normal   Scheduled              9m                default-scheduler                                  Successfully assigned nginx-57c88d7bb8-c6kpc to ip-11-0-1-111.us-west-2.compute.internal\n  Normal   SuccessfulMountVolume  9m                kubelet, ip-11-0-1-111.us-west-2.compute.internal  MountVolume.SetUp succeeded for volume \"default-token-8cwn4\"\n  Normal   Pulling                8m (x4 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  pulling image \"ngnix\"\n  Warning  Failed                 8m (x4 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  Failed to pull image \"ngnix\": rpc error: code = Unknown desc = Error response from daemon: repository ngnix not found: does not exist or no pull access\n  Normal   BackOff                7m (x6 over 9m)   kubelet, ip-11-0-1-111.us-west-2.compute.internal  Back-off pulling image \"ngnix\"\n  Warning  FailedSync             4m (x24 over 9m)  kubelet, ip-11-0-1-111.us-west-2.compute.internal  Error syncing pod  Bingo! The name of the image is  ngnix  instead of  nginx . So  fix the typo in your deployment file and redo the deployment .  Sometimes, your application(pod) may fail to start because of some configuration issues. For those errors, we can follow the logs of the pod.  kubectl logs -f nginx-57c88d7bb8-c6kpc  If you have any errors it will get populated in your logs.",
            "title": "Troubleshooting the Kubernetes cluster"
        },
        {
            "location": "/logging/",
            "text": "Logging\n\n\nReferences\n\n\n\n\nLogging Arcchitecture",
            "title": "Logging"
        },
        {
            "location": "/logging/#logging",
            "text": "References   Logging Arcchitecture",
            "title": "Logging"
        },
        {
            "location": "/wq001-cni/",
            "text": "KWQ001 - CNI Web Quest\n\n\nKubernetes has adapted Container Network Interface(CNI) as a standard to provide networking between pods running across hosts and to assign those with IP addresses.  The purpose of this Web Quest is to compare the CNI plugins available, and make recommendations based on the merit that you find in those.\n\n\nTask\n\n\nYour task is to work in your group, research on CNI plugins, try to find answers to the following questions, and present briefly on it.\n\n\nQuestions\n:\n\n\n\n\nWhy did kubernetes choose \nCNI\n (Container Network Interface) as a networking standard instead of \nCNM\n (Container Network Management ) that  \n\n\nList down some of the most popular CNI plugins. How do they compare, what are the key differences ?\n\n\nWhat are some of the advantages of using \nCalico\n as a CNI plugin ?\n\n\nDo you see any issues with network implementation in your organisation ? If yes, briefly describe.\n\n\nWhich CNI plugin would you choose for your project ? Why ?\n\n\n\n\nProcess\n\n\nFollowing is the process you could loosely follow,\n\n\n\n\nStart researching on web and reading individually about the topic. Reference the links and articles in the resources  section while you do so.\n\n\nReflect on how it could apply to your work environment. Speak with with the relevant teams in your organisation if needed gain insights into current implementation, what are the issues being faced. Think about  how you could you apply the knowledge you have researched on.\n\n\nHave a group discussion with  within  your group, exchange what you have learnt as well as your experience with the topic in your prior/existing  projects.  You may come up with the notes and common points you would like to present.\n\n\nPrepare a short 5/10 minutes presentation to explain the topic and answer the questions given in the \ntask\n section above.\n\n\n\n\nResources\n\n\n\n\nArticle: \nChoosing a CNI Network Provider for Kubernetes\n\n\nArticle: \nKubernetes Network Plugins\n\n\nDoc: \nOfficial CNI Doc\n\n\nVideo: \nHow VXLANs Work\n\n\nVideo: \nLife of a Packet\n\n\nVideo: \nHow Calico Works\n\n\nDoc: \nOfficial Kubernetes Doc on Cluster Networking",
            "title": "KWQ001 - CNI Web Quest"
        },
        {
            "location": "/wq001-cni/#kwq001-cni-web-quest",
            "text": "Kubernetes has adapted Container Network Interface(CNI) as a standard to provide networking between pods running across hosts and to assign those with IP addresses.  The purpose of this Web Quest is to compare the CNI plugins available, and make recommendations based on the merit that you find in those.  Task  Your task is to work in your group, research on CNI plugins, try to find answers to the following questions, and present briefly on it.  Questions :   Why did kubernetes choose  CNI  (Container Network Interface) as a networking standard instead of  CNM  (Container Network Management ) that    List down some of the most popular CNI plugins. How do they compare, what are the key differences ?  What are some of the advantages of using  Calico  as a CNI plugin ?  Do you see any issues with network implementation in your organisation ? If yes, briefly describe.  Which CNI plugin would you choose for your project ? Why ?   Process  Following is the process you could loosely follow,   Start researching on web and reading individually about the topic. Reference the links and articles in the resources  section while you do so.  Reflect on how it could apply to your work environment. Speak with with the relevant teams in your organisation if needed gain insights into current implementation, what are the issues being faced. Think about  how you could you apply the knowledge you have researched on.  Have a group discussion with  within  your group, exchange what you have learnt as well as your experience with the topic in your prior/existing  projects.  You may come up with the notes and common points you would like to present.  Prepare a short 5/10 minutes presentation to explain the topic and answer the questions given in the  task  section above.   Resources   Article:  Choosing a CNI Network Provider for Kubernetes  Article:  Kubernetes Network Plugins  Doc:  Official CNI Doc  Video:  How VXLANs Work  Video:  Life of a Packet  Video:  How Calico Works  Doc:  Official Kubernetes Doc on Cluster Networking",
            "title": "KWQ001 - CNI Web Quest"
        },
        {
            "location": "/wq002-security/",
            "text": "KWQ002 - Kubernetes Security Web Quest\n\n\nSecurity is a critical aspect while designing, building and maintaining any application infrastructure including kubernetes environemnts. With this exercise you are on a quest to find the measures you could implement to harden your kubernetes environment holistically.  \n\n\nTask\n\n\nYour task is to work in your group, research on security measures at every level, and try to find answers to the following questions. You will present on the topic briefly.\n\n\nQuestions\n:\n\n\n\n\nProvide an example of a major kubernetes vulnerability and how it had an adverse impact  on an organisation.    \n\n\nWhat are the security measures you would take to harden your  kubernetes environment ?\n\n\nIn your opinion, which is the most overlooked aspect of Kubernetes Security ?\n\n\n\n\nProcess\n\n\nFollowing is the process you could loosely follow,\n\n\n\n\nStart researching on web and reading individually about the topic. Reference the links and articles in the resources  section while you do so.\n\n\nReflect on how it could apply to your work environment. Speak with with the relevant teams in your organisation if needed gain insights into current implementation, what are the issues being faced. Think about  how you could you apply the knowledge you have researched on.\n\n\nHave a group discussion with  within  your group, exchange what you have learnt as well as your experience with the topic in your prior/existing  projects.  You may come up with the notes and common points you would like to present.\n\n\nPrepare a short 5/10 minutes presentation to explain the topic and answer the questions given in the \ntask\n section above.\n\n\n\n\nResources\n\n\n\n\nArticle: \n9 Kubernetes Security Best Practices Everyone Must Follow\n\n\neBook: \nKubernetes Deployment and Security Patterns\n\n\nvideo: \nHacking and Hardening Kubernetes Clusters by Example\n\n\nvideo: \nKubernetes Security Best Practices",
            "title": "KWQ001 - Kubernetes Security Web Quest"
        },
        {
            "location": "/wq002-security/#kwq002-kubernetes-security-web-quest",
            "text": "Security is a critical aspect while designing, building and maintaining any application infrastructure including kubernetes environemnts. With this exercise you are on a quest to find the measures you could implement to harden your kubernetes environment holistically.    Task  Your task is to work in your group, research on security measures at every level, and try to find answers to the following questions. You will present on the topic briefly.  Questions :   Provide an example of a major kubernetes vulnerability and how it had an adverse impact  on an organisation.      What are the security measures you would take to harden your  kubernetes environment ?  In your opinion, which is the most overlooked aspect of Kubernetes Security ?   Process  Following is the process you could loosely follow,   Start researching on web and reading individually about the topic. Reference the links and articles in the resources  section while you do so.  Reflect on how it could apply to your work environment. Speak with with the relevant teams in your organisation if needed gain insights into current implementation, what are the issues being faced. Think about  how you could you apply the knowledge you have researched on.  Have a group discussion with  within  your group, exchange what you have learnt as well as your experience with the topic in your prior/existing  projects.  You may come up with the notes and common points you would like to present.  Prepare a short 5/10 minutes presentation to explain the topic and answer the questions given in the  task  section above.   Resources   Article:  9 Kubernetes Security Best Practices Everyone Must Follow  eBook:  Kubernetes Deployment and Security Patterns  video:  Hacking and Hardening Kubernetes Clusters by Example  video:  Kubernetes Security Best Practices",
            "title": "KWQ002 - Kubernetes Security Web Quest"
        },
        {
            "location": "/wq003-ingress/",
            "text": "KWQ003 - Ingress Web Quest\n\n\nIngress controllers adds layer7 load balancing, reverse proxy and intelligent http routing capabilities to kubernetes cluster. The purpose of this Web Quest is to compare the ingress controllers available, and make recommendations based on the merit that you find in those.\n\n\nTask\n\n\nYour task is to work in your group, research on ingress controllers, and try to find answers to the following questions. You will present on the topic briefly.\n\n\nQuestions\n:\n\n\n\n\nIf you have to choose between \nLoadBalancer\n as a service type and a \nIngress Controller\n which one would you choose? Why?\n\n\nList down some of the most popular ingress controllers ? How do they compare, what are the key differences ?   \n\n\nHow do you make ingress controllers pick up configurations when a new service/deployment is created.  \n\n\nWhich ingress controller would you use ? Why?\n\n\n\n\nProcess\n\n\nFollowing is the process you could loosely follow,\n\n\n\n\nStart researching on web and reading individually about the topic. Reference the links and articles in the resources  section while you do so.\n\n\nReflect on how it could apply to your work environment. Speak with with the relevant teams in your organisation if needed gain insights into current implementation, what are the issues being faced. Think about  how you could you apply the knowledge you have researched on.\n\n\nHave a group discussion with  within  your group, exchange what you have learnt as well as your experience with the topic in your prior/existing  projects.  You may come up with the notes and common points you would like to present.\n\n\nPrepare a short 5/10 minutes presentation to explain the topic and answer the questions given in the \ntask\n section above.\n\n\n\n\nResources\n\n\n\n\nArticle: \nIngress comparison by kubedex.com\n\n\neBook: \nManaging Ingress Controllers on Kubernetes - a Guide by Giant Swarm\n\n\nDoc: \nTraefik Ingress Controller",
            "title": "KWQ003 - Ingress Web Quest"
        },
        {
            "location": "/wq003-ingress/#kwq003-ingress-web-quest",
            "text": "Ingress controllers adds layer7 load balancing, reverse proxy and intelligent http routing capabilities to kubernetes cluster. The purpose of this Web Quest is to compare the ingress controllers available, and make recommendations based on the merit that you find in those.  Task  Your task is to work in your group, research on ingress controllers, and try to find answers to the following questions. You will present on the topic briefly.  Questions :   If you have to choose between  LoadBalancer  as a service type and a  Ingress Controller  which one would you choose? Why?  List down some of the most popular ingress controllers ? How do they compare, what are the key differences ?     How do you make ingress controllers pick up configurations when a new service/deployment is created.    Which ingress controller would you use ? Why?   Process  Following is the process you could loosely follow,   Start researching on web and reading individually about the topic. Reference the links and articles in the resources  section while you do so.  Reflect on how it could apply to your work environment. Speak with with the relevant teams in your organisation if needed gain insights into current implementation, what are the issues being faced. Think about  how you could you apply the knowledge you have researched on.  Have a group discussion with  within  your group, exchange what you have learnt as well as your experience with the topic in your prior/existing  projects.  You may come up with the notes and common points you would like to present.  Prepare a short 5/10 minutes presentation to explain the topic and answer the questions given in the  task  section above.   Resources   Article:  Ingress comparison by kubedex.com  eBook:  Managing Ingress Controllers on Kubernetes - a Guide by Giant Swarm  Doc:  Traefik Ingress Controller",
            "title": "KWQ003 - Ingress Web Quest"
        },
        {
            "location": "/wq004-monitoring/",
            "text": "KWQ004 - Kubernetes Monitoring  Web Quest\n\n\nMonitoring a kubernetes environment including nodes, services, deployments, as well as application availability  as well as logs is important for a kubernetes administrator. With this web quest, your task is to dive deeper into the kubernetes monitoring infrastructure and make recommendations.\n\n\nTask\n\n\nYour task is to work in your group, research on monitoring architecture as well as tools available, and try to find answers to the following questions. You will present on the topic briefly.\n\n\nQuestions\n:\n\n\n\n\nExplain the Kubernetes Monitoring Architecture briefly   \n\n\nWhar are the monitoring solutions available for kubernetes ?  Which one would you recommend and why ?  \n\n\nWhat are the aspects of kubernetes environment and yoyr infastructure that you would monitor ? Does  introduction to kubernetes have an effect on any of the aspects being monitored ?\n\n\nWhich grafana dashboard did you find most useful ?\n\n\n\n\nProcess\n\n\nFollowing is the process you could loosely follow,\n\n\n\n\nStart researching on web and reading individually about the topic. Reference the links and articles in the resources  section while you do so.\n\n\nReflect on how it could apply to your work environment. Speak with with the relevant teams in your organisation if needed gain insights into current implementation, what are the issues being faced. Think about  how you could you apply the knowledge you have researched on.\n\n\nHave a group discussion with  within  your group, exchange what you have learnt as well as your experience with the topic in your prior/existing  projects.  You may come up with the notes and common points you would like to present.\n\n\nPrepare a short 5/10 minutes presentation to explain the topic and answer the questions given in the \ntask\n section above.\n\n\n\n\nResources\n\n\n\n\nArticle: \n5 Tools for monitoring Kubernetes at Scale in Production\n\n\neBook: \nThe State of Kubernetes Ecosystem\n\n\nVideo: \nYou are Monitoring Kubernetes Wrong\n\n\nVideo: \nMonitoring Kubernetes Clusters with Prometheus\n\n\nDoc : \nKubernetes Monitoring Architecture",
            "title": "KWQ004 - Monitoring Web Quest"
        },
        {
            "location": "/wq004-monitoring/#kwq004-kubernetes-monitoring-web-quest",
            "text": "Monitoring a kubernetes environment including nodes, services, deployments, as well as application availability  as well as logs is important for a kubernetes administrator. With this web quest, your task is to dive deeper into the kubernetes monitoring infrastructure and make recommendations.  Task  Your task is to work in your group, research on monitoring architecture as well as tools available, and try to find answers to the following questions. You will present on the topic briefly.  Questions :   Explain the Kubernetes Monitoring Architecture briefly     Whar are the monitoring solutions available for kubernetes ?  Which one would you recommend and why ?    What are the aspects of kubernetes environment and yoyr infastructure that you would monitor ? Does  introduction to kubernetes have an effect on any of the aspects being monitored ?  Which grafana dashboard did you find most useful ?   Process  Following is the process you could loosely follow,   Start researching on web and reading individually about the topic. Reference the links and articles in the resources  section while you do so.  Reflect on how it could apply to your work environment. Speak with with the relevant teams in your organisation if needed gain insights into current implementation, what are the issues being faced. Think about  how you could you apply the knowledge you have researched on.  Have a group discussion with  within  your group, exchange what you have learnt as well as your experience with the topic in your prior/existing  projects.  You may come up with the notes and common points you would like to present.  Prepare a short 5/10 minutes presentation to explain the topic and answer the questions given in the  task  section above.   Resources   Article:  5 Tools for monitoring Kubernetes at Scale in Production  eBook:  The State of Kubernetes Ecosystem  Video:  You are Monitoring Kubernetes Wrong  Video:  Monitoring Kubernetes Clusters with Prometheus  Doc :  Kubernetes Monitoring Architecture",
            "title": "KWQ004 - Kubernetes Monitoring  Web Quest"
        },
        {
            "location": "/wq005-controllers/",
            "text": "KWQ005 - Controllers Web Quest\n\n\nIf you look beyond deployment, which is great for running application with high availability, kubernetes supports variety of workloads. The purpose of this web quest is understand at a depth, the  gamut of controllers  that kubernetes supports and how each of that behaves.\n\n\nTask\n\n\nYour task is to work in your group, research on the controllers and workloads that the kubernetes supports, the purpose of each and how they behave.  You would  try to find answers to the following questions and  present on the topic briefly.\n\n\nFollowing are the controllers you would research on\n\n\n\n\ndeployment\n\n\nstatefulset\n\n\ndaemonset\n\n\njobs\n\n\ncron\n\n\n\n\nQuestions\n:\n\n\n\n\nDescribe and compare type of controllers available with kubernetes. Explain the purpose, key feature and the type of work load each is suitable for,  \n\n\nWhich controllers would you pick to run the following applications ? Why ?\n\n\na stateless web frontend\n\n\ncassandra db cluster\n\n\nmonitoring agents\n\n\nscheduled jobs\n\n\nad-hoc load tests\n\n\n\n\n\n\n\n\nProcess\n\n\nFollowing is the process you could loosely follow,\n\n\n\n\nStart researching on web and reading individually about the topic. Reference the links and articles in the resources  section while you do so.\n\n\nReflect on how it could apply to your work environment. Speak with with the relevant teams in your organisation if needed gain insights into current implementation, what are the issues being faced. Think about  how you could you apply the knowledge you have researched on.\n\n\nHave a group discussion with  within  your group, exchange what you have learnt as well as your experience with the topic in your prior/existing  projects.  You may come up with the notes and common points you would like to present.\n\n\nPrepare a short 5/10 minutes presentation to explain the topic and answer the questions given in the \ntask\n section above.\n\n\n\n\nResources\n\n\n\n\ndoc: \nStatefulsets\n\n\ndoc: \nDaemonSet\n\n\ndoc: \nJobs\n\n\ndoc: \nCron\n\n\ndoc: \nKubernetes Examples",
            "title": "KWQ005 - Controllers Web Quest"
        },
        {
            "location": "/wq005-controllers/#kwq005-controllers-web-quest",
            "text": "If you look beyond deployment, which is great for running application with high availability, kubernetes supports variety of workloads. The purpose of this web quest is understand at a depth, the  gamut of controllers  that kubernetes supports and how each of that behaves.  Task  Your task is to work in your group, research on the controllers and workloads that the kubernetes supports, the purpose of each and how they behave.  You would  try to find answers to the following questions and  present on the topic briefly.  Following are the controllers you would research on   deployment  statefulset  daemonset  jobs  cron   Questions :   Describe and compare type of controllers available with kubernetes. Explain the purpose, key feature and the type of work load each is suitable for,    Which controllers would you pick to run the following applications ? Why ?  a stateless web frontend  cassandra db cluster  monitoring agents  scheduled jobs  ad-hoc load tests     Process  Following is the process you could loosely follow,   Start researching on web and reading individually about the topic. Reference the links and articles in the resources  section while you do so.  Reflect on how it could apply to your work environment. Speak with with the relevant teams in your organisation if needed gain insights into current implementation, what are the issues being faced. Think about  how you could you apply the knowledge you have researched on.  Have a group discussion with  within  your group, exchange what you have learnt as well as your experience with the topic in your prior/existing  projects.  You may come up with the notes and common points you would like to present.  Prepare a short 5/10 minutes presentation to explain the topic and answer the questions given in the  task  section above.   Resources   doc:  Statefulsets  doc:  DaemonSet  doc:  Jobs  doc:  Cron  doc:  Kubernetes Examples",
            "title": "KWQ005 - Controllers Web Quest"
        },
        {
            "location": "/wq006-persistentvolume/",
            "text": "KWQ001 - Persistent Volume and Storage Web Quest\n\n\nStorage and data persistence is essential in most application environments. With kubernetes, a network storage solution becomes important due to the immutable deployments of pods which can spawn on new nodes every time  with a typical deployment. The purpose of this web quest is to understand the storage options, data peristence and dynamic provisioning of data.\n\n\nTask\n\n\nYour task is to work in your group, research on the storage systems and persistent volumes that kubernetes is compatible with and make recommendations.  You will  try to find answers to the following questions and present on the topic briefly.\n\n\nQuestions\n:\n\n\n\n\nExplain the concept of dynamic provisioning    \n\n\nWhich are the storage classes that  have internal provisioners ?\n\n\nWhich storage solution would you use in your organisation ? Why? How would your storage and provisioning system ? Explain briefly  \n\n\n\n\nProcess\n\n\nFollowing is the process you could loosely follow,\n\n\n\n\nStart researching on web and reading individually about the topic. Reference the links and articles in the resources  section while you do so.\n\n\nReflect on how it could apply to your work environment. Speak with with the relevant teams in your organisation if needed gain insights into current implementation, what are the issues being faced. Think about  how you could you apply the knowledge you have researched on.\n\n\nHave a group discussion with  within  your group, exchange what you have learnt as well as your experience with the topic in your prior/existing  projects.  You may come up with the notes and common points you would like to present.\n\n\nPrepare a short 5/10 minutes presentation to explain the topic and answer the questions given in the \ntask\n section above.\n\n\n\n\nResources\n\n\n\n\ndoc: \nStorage Classes\n\n\neBook: \nState of kubernetes ecosystem\n\n\nArticle: \nThe Kubernetes Storage Ecosystem\n\n\nVideo: \nPersistent Storage with Kubernetes in Production - Which Solution and Why?\n\n\nVideo: \nKubernetes Storage Lingo 101",
            "title": "KWQ006 - Persistent Data Web Quest"
        },
        {
            "location": "/wq006-persistentvolume/#kwq001-persistent-volume-and-storage-web-quest",
            "text": "Storage and data persistence is essential in most application environments. With kubernetes, a network storage solution becomes important due to the immutable deployments of pods which can spawn on new nodes every time  with a typical deployment. The purpose of this web quest is to understand the storage options, data peristence and dynamic provisioning of data.  Task  Your task is to work in your group, research on the storage systems and persistent volumes that kubernetes is compatible with and make recommendations.  You will  try to find answers to the following questions and present on the topic briefly.  Questions :   Explain the concept of dynamic provisioning      Which are the storage classes that  have internal provisioners ?  Which storage solution would you use in your organisation ? Why? How would your storage and provisioning system ? Explain briefly     Process  Following is the process you could loosely follow,   Start researching on web and reading individually about the topic. Reference the links and articles in the resources  section while you do so.  Reflect on how it could apply to your work environment. Speak with with the relevant teams in your organisation if needed gain insights into current implementation, what are the issues being faced. Think about  how you could you apply the knowledge you have researched on.  Have a group discussion with  within  your group, exchange what you have learnt as well as your experience with the topic in your prior/existing  projects.  You may come up with the notes and common points you would like to present.  Prepare a short 5/10 minutes presentation to explain the topic and answer the questions given in the  task  section above.   Resources   doc:  Storage Classes  eBook:  State of kubernetes ecosystem  Article:  The Kubernetes Storage Ecosystem  Video:  Persistent Storage with Kubernetes in Production - Which Solution and Why?  Video:  Kubernetes Storage Lingo 101",
            "title": "KWQ001 - Persistent Volume and Storage Web Quest"
        },
        {
            "location": "/wq007-maintenance/",
            "text": "KWQ001 - Cluster Maintenance Security Web Quest\n\n\nKubernetes administrators need to be well aware of the cluster maintenance and administration tasks. With this web quest, you would research on the  \n\n\nTask\n\n\nYour task is to work in your group, research on cluster maintenance activities in depth, and try to find answers to the following questions. You will present on the topic briefly.\n\n\nQuestions\n:\n\n\n\n\nWhat are the cluster maintenance and administration tasks that you anticipate doing as part of your job ?\n\n\nBriefly explain how you could do the following\n\n\nPrepare nodes for maintenence\n\n\nDefine resource quotas\n\n\nCreate  users and  groups\n\n\nResize a cluster  \n\n\nUpgrade the version of kubernetes on the nodes\n\n\n\n\n\n\n\n\nProcess\n\n\nFollowing is the process you could loosely follow,\n\n\n\n\nStart researching on web and reading individually about the topic. Reference the links and articles in the resources  section while you do so.\n\n\nReflect on how it could apply to your work environment. Speak with with the relevant teams in your organisation if needed gain insights into current implementation, what are the issues being faced. Think about  how you could you apply the knowledge you have researched on.\n\n\nHave a group discussion with  within  your group, exchange what you have learnt as well as your experience with the topic in your prior/existing  projects.  You may come up with the notes and common points you would like to present.\n\n\nPrepare a short 5/10 minutes presentation to explain the topic and answer the questions given in the \ntask\n section above.\n\n\n\n\nResources\n\n\n\n\ndoc: \nCluster Management",
            "title": "KWQ007 - Cluster Maintenance Web Quest"
        },
        {
            "location": "/wq007-maintenance/#kwq001-cluster-maintenance-security-web-quest",
            "text": "Kubernetes administrators need to be well aware of the cluster maintenance and administration tasks. With this web quest, you would research on the    Task  Your task is to work in your group, research on cluster maintenance activities in depth, and try to find answers to the following questions. You will present on the topic briefly.  Questions :   What are the cluster maintenance and administration tasks that you anticipate doing as part of your job ?  Briefly explain how you could do the following  Prepare nodes for maintenence  Define resource quotas  Create  users and  groups  Resize a cluster    Upgrade the version of kubernetes on the nodes     Process  Following is the process you could loosely follow,   Start researching on web and reading individually about the topic. Reference the links and articles in the resources  section while you do so.  Reflect on how it could apply to your work environment. Speak with with the relevant teams in your organisation if needed gain insights into current implementation, what are the issues being faced. Think about  how you could you apply the knowledge you have researched on.  Have a group discussion with  within  your group, exchange what you have learnt as well as your experience with the topic in your prior/existing  projects.  You may come up with the notes and common points you would like to present.  Prepare a short 5/10 minutes presentation to explain the topic and answer the questions given in the  task  section above.   Resources   doc:  Cluster Management",
            "title": "KWQ001 - Cluster Maintenance Security Web Quest"
        },
        {
            "location": "/rbac-resource-group-mapping/",
            "text": "RBAC Reference\n\n\nkubernetes Instances Configuration\n\n\nGCP\n\n\n\n\n\n\n\n\nNUMBER OF NODE-SIZE\n\n\nINSTANCE TYPE\n\n\nCPU\n\n\nMEMORY\n\n\n\n\n\n\n\n\n\n\n\n\n1-5\n\n\nn1-standard-1\n\n\n1\n\n\n\n\n\n\n\n\n6-10\n\n\nn1-standard-2\n\n\n2\n\n\n\n\n\n\n\n\n11-100\n\n\nn1-standard-4\n\n\n4\n\n\n\n\n\n\n\n\n101-250\n\n\nn1-standard-8\n\n\n8\n\n\n\n\n\n\n\n\n251-500\n\n\nn1-standard-16\n\n\n16\n\n\n\n\n\n\n\n\nmore than 500\n\n\nn1-standard-32\n\n\n32\n\n\n\n\n\n\n\n\nAWS\n\n\n\n\n\n\n\n\nNUMBER OF NODE_SIZE\n\n\nINSTANCE TYPE\n\n\nCPU\n\n\nMEMORY\n\n\n\n\n\n\n\n\n\n\n1-5\n\n\nm3.medium\n\n\n1\n\n\n3.75\n\n\n\n\n\n\n6-10\n\n\nm3.large\n\n\n2\n\n\n7.50\n\n\n\n\n\n\n11-100\n\n\nm3.xlarge\n\n\n4\n\n\n15\n\n\n\n\n\n\n101-250\n\n\nm3.2xlarge\n\n\n8\n\n\n30\n\n\n\n\n\n\n251-500\n\n\nc4.4xlarge\n\n\n8\n\n\n30\n\n\n\n\n\n\nmore than 500\n\n\nc4.8xlarge\n\n\n16\n\n\n60\n\n\n\n\n\n\n\n\napi groups and resources\n\n\n\n\n\n\n\n\napiGroup\n\n\nResources\n\n\n\n\n\n\n\n\n\n\napps\n\n\ndaemonsets, deployments, deployments/rollback, deployments/scale, replicasets, replicasets/scale, statefulsets, statefulsets/scale\n\n\n\n\n\n\ncore\n\n\nconfigmaps, endpoints, persistentvolumeclaims, replicationcontrollers, replicationcontrollers/scale, secrets, serviceaccounts, services,services/proxy\n\n\n\n\n\n\nautoscaling\n\n\nhorizontalpodautoscalers\n\n\n\n\n\n\nbatch\n\n\ncronjobs, jobs\n\n\n\n\n\n\npolicy\n\n\npoddisruptionbudgets\n\n\n\n\n\n\nnetworking.k8s.io\n\n\nnetworkpolicies\n\n\n\n\n\n\nauthorization.k8s.io\n\n\nlocalsubjectaccessreviews\n\n\n\n\n\n\nrbac.authorization.k8s.io\n\n\nrolebindings,roles\n\n\n\n\n\n\nextensions\n\n\ndeprecated (read notes)\n\n\n\n\n\n\n\n\nNotes\n\n\nIn addition to the above apiGroups, you may see \nextensions\n being used in some example code snippets. Please note that \nextensions\n was initially created as a experiement and is been deprecated, by moving most of the matured apis to one of the groups mentioned above.  \nYou could read this comment and the thread\n to get clarity on this.",
            "title": "RBAC apiGroups to Resource Mapping"
        },
        {
            "location": "/rbac-resource-group-mapping/#rbac-reference",
            "text": "kubernetes Instances Configuration",
            "title": "RBAC Reference"
        },
        {
            "location": "/rbac-resource-group-mapping/#gcp",
            "text": "NUMBER OF NODE-SIZE  INSTANCE TYPE  CPU  MEMORY       1-5  n1-standard-1  1     6-10  n1-standard-2  2     11-100  n1-standard-4  4     101-250  n1-standard-8  8     251-500  n1-standard-16  16     more than 500  n1-standard-32  32",
            "title": "GCP"
        },
        {
            "location": "/rbac-resource-group-mapping/#aws",
            "text": "NUMBER OF NODE_SIZE  INSTANCE TYPE  CPU  MEMORY      1-5  m3.medium  1  3.75    6-10  m3.large  2  7.50    11-100  m3.xlarge  4  15    101-250  m3.2xlarge  8  30    251-500  c4.4xlarge  8  30    more than 500  c4.8xlarge  16  60",
            "title": "AWS"
        },
        {
            "location": "/rbac-resource-group-mapping/#api-groups-and-resources",
            "text": "apiGroup  Resources      apps  daemonsets, deployments, deployments/rollback, deployments/scale, replicasets, replicasets/scale, statefulsets, statefulsets/scale    core  configmaps, endpoints, persistentvolumeclaims, replicationcontrollers, replicationcontrollers/scale, secrets, serviceaccounts, services,services/proxy    autoscaling  horizontalpodautoscalers    batch  cronjobs, jobs    policy  poddisruptionbudgets    networking.k8s.io  networkpolicies    authorization.k8s.io  localsubjectaccessreviews    rbac.authorization.k8s.io  rolebindings,roles    extensions  deprecated (read notes)     Notes  In addition to the above apiGroups, you may see  extensions  being used in some example code snippets. Please note that  extensions  was initially created as a experiement and is been deprecated, by moving most of the matured apis to one of the groups mentioned above.   You could read this comment and the thread  to get clarity on this.",
            "title": "api groups and resources"
        }
    ]
}